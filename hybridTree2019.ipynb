{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GainRatio is used to select the node splitting attribute and the splitting point of the continuous attribute value in the C4.5 algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A_i is an attribute in the attribute set A to be split, a_1 and a_2 are weight coefficients assigned to the information gain and the gainratio: 0 <= a_1, a_2 <= 1, and a_1 + a_2 = 1.\n",
    "\n",
    "IG_GR(Data, A_i) = (a_1 + a_2 / SplitInformation(D, A_i)) * InfoGain(D, A_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C4.5 \n",
    "https://github.com/barisesmer/C4.5/tree/master/c45\n",
    "\n",
    "https://github.com/Valdecy/C4.5/blob/master/Python-DM-Classification-04-C4.5.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code of Node Class and Tree and modified from \n",
    "\n",
    "https://github.com/loginaway/DecisionTree/blob/master/Node.py\n",
    "\n",
    "https://github.com/loginaway/DecisionTree/blob/master/DecisionTree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    '''\n",
    "    Basic element for decision tree.\n",
    "    Initialization: to initialize a tree node, at least feature_name is needed.\n",
    "    Connection: to connect different nodes, there are two situations.\n",
    "        1, if the parent node is discrete (i.e. node.discrete==True), \n",
    "            then \n",
    "            >> node[feature_value]=childnode \n",
    "            will do the connection.\n",
    "        2, if the parent node is continuous, then \n",
    "            >> node['<=']=childnode \n",
    "            or \n",
    "            >> node['>']=childnode \n",
    "            will assign its children. To be specific, \n",
    "            node['<=']=childnode means if the feature value is bigger than \n",
    "            node.threshold, then the workflow goes left to the childnode, and vice versa.\n",
    "    \n",
    "    Classification: if you have constructed a tree, you may want to classify a \n",
    "        certain group of data. Simply call the root node will do this task, i.e.\n",
    "        >> root(data)\n",
    "        where data should be a dictionary containing the corresponding key and value.\n",
    "        And this method will return the classification result.\n",
    "    Visualization: A naive visualization is implemented here. \n",
    "        For example, you may visualize a subtree rooted at 'node' by\n",
    "        >> print(node)\n",
    "        The tree structure will then be printed on the console.\n",
    "    '''\n",
    "    def __init__(self, feature_name='', discrete=True, threshold=0, isLeaf=False, classification=None):\n",
    "        # for discrete node: the next node can be retrieved by self.map[feature_value]\n",
    "        # for continuous node: by self.map['<='] (when value <= self.threshold)\n",
    "        #                       by self.map['>'] (when value > self.threshold)\n",
    "        self.map=dict()\n",
    "\n",
    "        self.discrete=discrete\n",
    "        self.feature_name=feature_name\n",
    "        self.isLeaf=isLeaf\n",
    "        if isLeaf:\n",
    "            self.classification=classification\n",
    "        if not discrete:\n",
    "            self.threshold=threshold\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.map[key]=value\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.map.get(key)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        '''\n",
    "        This method should be used on the root to predict its classification.\n",
    "        data: a dict with its key being the features and value being \n",
    "        the corresponding value.\n",
    "        '''\n",
    "        if self.isLeaf:\n",
    "            return self.classification\n",
    "        # if the node has a discrete feature, use __getitem__ to find the next node\n",
    "        # then call the next node.\n",
    "        if self.discrete:\n",
    "            return self.map[data[self.feature_name]](data)\n",
    "        else:\n",
    "        # node is not discrete: print the self.feature_name\n",
    "            #print(\"continuous name\")\n",
    "            #print(self.feature_name)\n",
    "            if data[self.feature_name].astype(np.float32)>self.threshold:\n",
    "                return self.map['>'](data)\n",
    "            else:\n",
    "                return self.map['<='](data)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This method is designed for printing the subtree rooted at the current node.\n",
    "        To print the tree, try print(self).\n",
    "        '''\n",
    "        hierarchy={}\n",
    "        stack=[(0, 0, self)]\n",
    "        count=0\n",
    "        while stack:\n",
    "            # BFS\n",
    "            (layer_index, node_index, current)=stack.pop(0)\n",
    "            layer_index+=1\n",
    "\n",
    "            hierarchy[layer_index]=hierarchy.get(layer_index, [])\n",
    "            hierarchy[layer_index].append((node_index, current.feature_name,\\\n",
    "                    [(str(current.classification),)] if current.isLeaf else \\\n",
    "                     [(str(key), '' if current.discrete else str(current.threshold), item.feature_name) for key, item in current.map.items()]))\n",
    "            # print(hierarchy)\n",
    "\n",
    "            if current.isLeaf:\n",
    "                continue\n",
    "            for _, item in current.map.items():\n",
    "                count+=1\n",
    "                stack.append((layer_index, count, item))\n",
    "            # print(item_index_map)\n",
    "\n",
    "        totallist=[]\n",
    "        for layer, layer_content in hierarchy.items():\n",
    "            rowlist=[]\n",
    "            for i in layer_content:\n",
    "                rowlist.append('['+i[1]+']'+': {'+', '.join([' '.join(item) for item in i[2]])+'}')\n",
    "            rowstr=' +++ '.join(rowlist)\n",
    "            totallist.append(rowstr)\n",
    "\n",
    "        return '============Root===========\\n'+'\\n\\n'.join(totallist)+'\\n============Leaf==========='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG\n",
    "#import sys\n",
    "import numpy as np\n",
    "#from Node import Node\n",
    "\n",
    "# set the maximal recursion limits here.\n",
    "#sys.setrecursionlimit(10000)\n",
    "\n",
    "class baseClassDecisionTree(object):\n",
    "\n",
    "    '''\n",
    "    The main class of decision tree.\n",
    "    '''\n",
    "    def __init__(self, feature_discrete=[], treeType='C4.5'):\n",
    "        '''\n",
    "        feature_discrete: a dict with its each key-value pair being (feature_name: True/False),\n",
    "            where True means the feature is discrete and False means the feature is \n",
    "            continuous. \n",
    "        type: ID3/C4.5/CART\n",
    "        pruning: pre/post\n",
    "        '''\n",
    "\n",
    "        self.feature_discrete=feature_discrete\n",
    "        self.treeType=treeType\n",
    "        self.leaf_count=0\n",
    "        self.tmp_classification=''\n",
    "        self._root_node = root\n",
    "        self.tree=None\n",
    "\n",
    "    def Entropy(self, list_of_class):\n",
    "        '''\n",
    "        Compute the entropy for the given list of class.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        'duck': 2/3, 'dolphin': 1/3, so the entropy for this array is 0.918\n",
    "        '''\n",
    "        count={}\n",
    "        for key in list_of_class:\n",
    "            count[key]=count.get(key, 0)+1\n",
    "        frequency=np.array(tuple(count.values()))/len(list_of_class)\n",
    "        return -1*np.vdot(frequency, np.log2(frequency))\n",
    "\n",
    "    def Information_Gain(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Gain.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Gain for this example is 0.2516.\n",
    "        '''\n",
    "        sec2=np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        return self.Entropy(list_of_class)-sec2\n",
    "\n",
    "\n",
    "    def Information_Ratio(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Ratio.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Ratio for this example is 0.2740.\n",
    "        '''\n",
    "        tmp=np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        # Here we assume instrinsic_value is SplitInformation! \n",
    "        intrinsic_value=-1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        return self.Information_Gain(list_of_class, grouped_list_of_class) / intrinsic_value\n",
    "\n",
    "    def IGGR(self, list_of_class, grouped_list_of_class, a1, a2):\n",
    "\n",
    "        sec2 = np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        \n",
    "        infoGain =  self.Entropy(list_of_class)-sec2\n",
    "\n",
    "        #print(\"IGGR grouped_list_of_class\")\n",
    "        #print(\"----------\")\n",
    "        #print(grouped_list_of_class)\n",
    "        #print(\"----------\")\n",
    "\n",
    "        tmp = np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        intrinsic_value = -1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        GainRatio = self.Information_Gain(list_of_class, grouped_list_of_class)/ intrinsic_value\n",
    "\n",
    "        sol1 = a1 * infoGain + a2 * GainRatio\n",
    "        sol2 = (a1 + ( a2 / intrinsic_value) ) * infoGain \n",
    "\n",
    "        return sol2\n",
    "\n",
    "    def orderByGainOrRatio(self, D, A, by='Gain'):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function = self.Information_Gain if by=='Gain' else self.Information_Ratio\n",
    "\n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "\n",
    "                for val in possibleVal:\n",
    "\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "\n",
    "                IC_value=target_function(D[:, -1], grouped_list_of_class)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "\n",
    "                # continuous\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "\n",
    "                for point in split_points:\n",
    "                    smaller_set=D[np.argwhere(D[:, info[0]]<=str(point)), -1].flatten()\n",
    "                    bigger_set=D[np.argwhere(D[:, info[0]]>str(point)), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    IC_tmp=target_function(D[:, -1], (smaller_set, bigger_set))\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def orderByIGGR(self, D, A, a1, a2):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function1 = self.Information_Gain\n",
    "        target_function2 = self.Information_Ratio\n",
    "        target_function3 = self.IGGR\n",
    "        \n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "                for val in possibleVal:\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "                IC_value=target_function3(D[:, -1], grouped_list_of_class, a1, a2)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "                # continuous\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "                for point in split_points:\n",
    "                    smaller_set=D[np.argwhere(D[:, info[0]]<=str(point)), -1].flatten()\n",
    "                    bigger_set=D[np.argwhere(D[:, info[0]]>str(point)), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    # list_of_class, grouped_list_of_class, a1, a2\n",
    "                    IC_tmp=target_function3(D[:, -1], (smaller_set, bigger_set), a1, a2)\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def chooseAttribute(self, D, A):\n",
    "\n",
    "        '''\n",
    "        Choose an attribute from A according to the metrics above.\n",
    "        For the definition of D and A, see method 'fit'.\n",
    "        Different principal for different tree types:\n",
    "        ID3: choose the attribute that maximizes the Information Gain.\n",
    "        C4.5: \n",
    "            1, choose those attributes whose Information Gain are above average.\n",
    "            2, choose the one that maximizes the Gain Ratio from these attributes.\n",
    "        CART: choose the attribute that minimizes the Gini Index.\n",
    "        IG_GR: \n",
    "        '''\n",
    "        if self.treeType=='ID3':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='C4.5':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "\n",
    "            # for C4.5, we choose the attributes whose Gain are above average\n",
    "            # and then order them by Ratio.\n",
    "\n",
    "            sub_A={key: A[key] for key in attr_list}\n",
    "            attr_list=self.orderByGainOrRatio(D, sub_A, by='Ratio')\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='IGGR':\n",
    "\n",
    "            attr_list=self.orderByIGGR(D, A, 0.3, 0.7)\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "    def fit(self, D, A):\n",
    "        '''\n",
    "        Train the tree.\n",
    "        To save the training result:\n",
    "        >> self.tree=self.fit(D, A)\n",
    "        D: the training set, a size [m, n+1] numpy array (with str type elements), \n",
    "            where m is the number of training data and n is the number of attributes.\n",
    "            The last column of D is the classifications (or labels).\n",
    "        A: the attributes set. It is a dict with its structure being like \n",
    "            {attr_name: [index_in_D_columns, possibleVal_or_threshold], ...}\n",
    "            attr_name: name of the attribute\n",
    "            index_in_D_columns: the corresponding index of the attribute in ndarray D (starting from 0)\n",
    "            possibleVal_or_threshold: \n",
    "                ###################################################\n",
    "                ## This value may not always be available in A   ##\n",
    "                ## it is added after 'chooseAttribute' is called ##\n",
    "                ## And it will be updated after each call        ##\n",
    "                ###################################################\n",
    "                1, if the attribute is discrete, then it is a ndarray containing all possible values \n",
    "                    of this attribute.\n",
    "                2, if the attribute is continuous, then possibleVal_or_threshold is the most recent \n",
    "                    threshold.\n",
    "        '''\n",
    "        if len(D)==0:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=self.tmp_classification)\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(np.unique(D[:, -1]))<=1:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=D[0, -1])\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(A)==0 or len(np.unique(D[:, :-1], axis=0))<=1:\n",
    "            count_dict={}\n",
    "            for key in D[:, -1]:\n",
    "                count_dict[key]=count_dict.get(key, 0)+1\n",
    "            most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=most_frequent)\n",
    "            self.leaf_count+=1\n",
    "            return node \n",
    "\n",
    "        count_dict={}\n",
    "        for key in D[:, -1]:\n",
    "            count_dict[key]=count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "        self.tmp_classification=most_frequent\n",
    "\n",
    "        # choose target attribute\n",
    "        target_attr=self.chooseAttribute(D, A)\n",
    "        # print(target_attr)\n",
    "\n",
    "        # generate nodes for each possible values of the target attribute if it's discrete\n",
    "        # generate two nodes for the two classification if it's continuous\n",
    "        # related information is stored in A[target_attr][1] now, \n",
    "        # since we have called chooseAttribute at least once.\n",
    "        info=A[target_attr]\n",
    "\n",
    "        if self.feature_discrete[target_attr]:\n",
    "\n",
    "            node=Node(feature_name=target_attr, discrete=True, isLeaf=False)\n",
    "            # generate nodes for each possible values\n",
    "            for possibleVal in info[1]:\n",
    "                keys=set(A.keys()).difference({target_attr})\n",
    "                # connect node to its child\n",
    "                tmp_D=D[np.argwhere(D[:, info[0]]==possibleVal), :]\n",
    "                tmp_A={key: A[key] for key in keys}\n",
    "\n",
    "                node[possibleVal]=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), tmp_A)\n",
    "        \n",
    "        else:\n",
    "            # continuous\n",
    "            threshold=info[1]\n",
    "            node=Node(feature_name=target_attr, discrete=False, threshold=threshold,isLeaf=False)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]<=str(threshold)), :]\n",
    "            node['<=']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]>str(threshold)), :]\n",
    "            node['>']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "            \n",
    "        \n",
    "        return node\n",
    "\n",
    "    def post_prune(self, training_D, testing_D, A, current=None, parent=None):\n",
    "        '''\n",
    "        self.tree is required.\n",
    "        This method conducts the post-pruning to enhance the model performance.\n",
    "        To make sure this method will work, set \n",
    "        >> current=self.tree\n",
    "        when you call it.\n",
    "        '''\n",
    "        self.current_accuracy=self.evaluate(testing_D, A)\n",
    "        count_dict={}\n",
    "        if len(training_D)==0:\n",
    "            return \n",
    "        # print(training_D)\n",
    "        for key in training_D[:, -1]:\n",
    "            count_dict[key]=count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(training_D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "        leaf_parent=True\n",
    "        for key, node in current.map.items():\n",
    "            if not node.isLeaf:\n",
    "                leaf_parent=False\n",
    "                # Recursion, DFS\n",
    "                if node.discrete:\n",
    "                    tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]==key), :]\n",
    "                else:\n",
    "                    if key=='<=':\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]<=str(node.threshold)), :]\n",
    "                    else:\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]>str(node.threshold)), :]\n",
    "                self.post_prune(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), testing_D, A, parent=current, current=node)\n",
    "        \n",
    "        tmp_node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, classification=most_frequent)\n",
    "        \n",
    "        if parent:\n",
    "            # when current node is not the root\n",
    "            for key, node in parent.map.items():\n",
    "                if node==current:\n",
    "                    parent.map[key]=tmp_node\n",
    "                    saved_key=key\n",
    "                    break\n",
    "            # compare the evaluation, if it is enhanced then prune the tree.\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                parent.map[saved_key]=current\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "        else:\n",
    "            # when current node is the root\n",
    "            saved_tree=self.tree\n",
    "            self.tree=tmp_node\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                self.tree=saved_tree\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "\n",
    "    def predict(self, D, A):\n",
    "        '''\n",
    "        Predict the classification for the data in D.\n",
    "        For the definition of A, see method 'fit'. \n",
    "        There is one critical difference between D and that defined in 'fit':\n",
    "            the last column may or may not be the labels. \n",
    "            This method works as long as the feature index in A matches the corresponding\n",
    "            column in D.\n",
    "        '''\n",
    "        row, _=D.shape\n",
    "        pred=np.empty((row, 1), dtype=str)\n",
    "        tmp_data={key: None for key in A.keys()}\n",
    "        for i in range(len(D)):\n",
    "            for key, info in A.items():\n",
    "                tmp_data[key]=D[i, info[0]]\n",
    "            pred[i]=self.tree(tmp_data)\n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, testing_D, A):\n",
    "        '''\n",
    "        Evaluate the performance of decision tree. (Accuracy)\n",
    "        For definition of testing_D and A, see 'predict'.\n",
    "        However, here the testing_D is required to be labelled, that is, its last column \n",
    "        should be labels of the data.\n",
    "        '''\n",
    "        true_label=testing_D[:, -1]\n",
    "        pred_label=self.predict(testing_D, A)\n",
    "        \n",
    "        success_count=0\n",
    "        for i in range(len(true_label)):\n",
    "            if true_label[i]==pred_label[i]:\n",
    "                success_count+=1\n",
    "\n",
    "        return success_count/len(true_label)\n",
    "\n",
    "        \n",
    "\n",
    "class Tree_DPDT(baseClassDecisionTree):\n",
    "    def __init__(self, conf):\n",
    "\n",
    "        super().__init__(conf['feature_discrete'], conf['treeType'])\n",
    "        self.conf=conf\n",
    "\n",
    "    # try rename 'train' as 'fit' and run again, what is wrong?\n",
    "    def train(self, D):\n",
    "        self.tree=super().fit(D, self.conf['A'])\n",
    "        \n",
    "    def prune(self, training_D, testing_D):\n",
    "        super().post_prune(training_D, testing_D, self.conf['A'], current=self.tree)\n",
    "\n",
    "    def pred(self, D):\n",
    "        return super().predict(D, self.conf['A'])\n",
    "    \n",
    "    def eval(self, D):\n",
    "        return super().evaluate(D, self.conf['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import numpy as np\n",
    "#from Node import Node\n",
    "\n",
    "# set the maximal recursion limits here.\n",
    "#sys.setrecursionlimit(10000)\n",
    "\n",
    "class Tree_DPDT(baseClassDecisionTree):\n",
    "    def __init__(self, conf):\n",
    "\n",
    "        super().__init__(conf['feature_discrete'], conf['treeType'])\n",
    "        self.conf=conf\n",
    "\n",
    "    # try rename 'train' as 'fit' and run again, what is wrong?\n",
    "    def train(self, D):\n",
    "        self.tree=super().fit(D, self.conf['A'])\n",
    "        \n",
    "    def prune(self, training_D, testing_D):\n",
    "        super().post_prune(training_D, testing_D, self.conf['A'], current=self.tree)\n",
    "\n",
    "    def pred(self, D):\n",
    "        return super().predict(D, self.conf['A'])\n",
    "    \n",
    "    def eval(self, D):\n",
    "        return super().evaluate(D, self.conf['A'])\n",
    "\n",
    "class baseClassDecisionTree(object):\n",
    "\n",
    "    '''\n",
    "    The main class of decision tree.\n",
    "    '''\n",
    "    def __init__(self, feature_discrete=[], treeType='C4.5'):\n",
    "        '''\n",
    "        feature_discrete: a dict with its each key-value pair being (feature_name: True/False),\n",
    "            where True means the feature is discrete and False means the feature is \n",
    "            continuous. \n",
    "        type: ID3/C4.5/CART\n",
    "        pruning: pre/post\n",
    "        '''\n",
    "\n",
    "        self.feature_discrete=feature_discrete\n",
    "        self.treeType=treeType\n",
    "        self.leaf_count=0\n",
    "        self.tmp_classification=''\n",
    "        self._root_node = root\n",
    "        self.tree=None\n",
    "\n",
    "    def Entropy(self, list_of_class):\n",
    "        '''\n",
    "        Compute the entropy for the given list of class.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        'duck': 2/3, 'dolphin': 1/3, so the entropy for this array is 0.918\n",
    "        '''\n",
    "        count={}\n",
    "        for key in list_of_class:\n",
    "            count[key]=count.get(key, 0)+1\n",
    "        frequency=np.array(tuple(count.values()))/len(list_of_class)\n",
    "        return -1*np.vdot(frequency, np.log2(frequency))\n",
    "\n",
    "    def Information_Gain(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Gain.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Gain for this example is 0.2516.\n",
    "        '''\n",
    "        sec2=np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        return self.Entropy(list_of_class)-sec2\n",
    "\n",
    "\n",
    "    def Information_Ratio(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Ratio.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Ratio for this example is 0.2740.\n",
    "        '''\n",
    "        tmp=np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        # Here we assume instrinsic_value is SplitInformation! \n",
    "        intrinsic_value=-1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        return self.Information_Gain(list_of_class, grouped_list_of_class) / intrinsic_value\n",
    "\n",
    "    def IGGR(self, list_of_class, grouped_list_of_class, a1, a2):\n",
    "\n",
    "        sec2 = np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        \n",
    "        infoGain =  self.Entropy(list_of_class)-sec2\n",
    "\n",
    "        #print(\"IGGR grouped_list_of_class\")\n",
    "        #print(\"----------\")\n",
    "        #print(grouped_list_of_class)\n",
    "        #print(\"----------\")\n",
    "\n",
    "        tmp = np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        intrinsic_value = -1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        GainRatio = self.Information_Gain(list_of_class, grouped_list_of_class)/ intrinsic_value\n",
    "\n",
    "        sol1 = a1 * infoGain + a2 * GainRatio\n",
    "        sol2 = (a1 + ( a2 / intrinsic_value) ) * infoGain \n",
    "\n",
    "        return sol2\n",
    "\n",
    "    def orderByGainOrRatio(self, D, A, by='Gain'):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function = self.Information_Gain if by=='Gain' else self.Information_Ratio\n",
    "\n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "\n",
    "                for val in possibleVal:\n",
    "\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "\n",
    "                IC_value=target_function(D[:, -1], grouped_list_of_class)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "\n",
    "                # continuous\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "\n",
    "                for point in split_points:\n",
    "                    smaller_set=D[np.argwhere(D[:, info[0]]<=str(point)), -1].flatten()\n",
    "                    bigger_set=D[np.argwhere(D[:, info[0]]>str(point)), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    IC_tmp=target_function(D[:, -1], (smaller_set, bigger_set))\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def orderByIGGR(self, D, A, a1, a2):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function1 = self.Information_Gain\n",
    "        target_function2 = self.Information_Ratio\n",
    "        target_function3 = self.IGGR\n",
    "        \n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "                for val in possibleVal:\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "                IC_value=target_function3(D[:, -1], grouped_list_of_class, a1, a2)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "                # continuous\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "                for point in split_points:\n",
    "                    smaller_set=D[np.argwhere(D[:, info[0]]<=str(point)), -1].flatten()\n",
    "                    bigger_set=D[np.argwhere(D[:, info[0]]>str(point)), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    # list_of_class, grouped_list_of_class, a1, a2\n",
    "                    IC_tmp=target_function3(D[:, -1], (smaller_set, bigger_set), a1, a2)\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def chooseAttribute(self, D, A):\n",
    "\n",
    "        '''\n",
    "        Choose an attribute from A according to the metrics above.\n",
    "        For the definition of D and A, see method 'fit'.\n",
    "        Different principal for different tree types:\n",
    "        ID3: choose the attribute that maximizes the Information Gain.\n",
    "        C4.5: \n",
    "            1, choose those attributes whose Information Gain are above average.\n",
    "            2, choose the one that maximizes the Gain Ratio from these attributes.\n",
    "        CART: choose the attribute that minimizes the Gini Index.\n",
    "        IG_GR: \n",
    "        '''\n",
    "        if self.treeType=='ID3':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='C4.5':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "\n",
    "            # for C4.5, we choose the attributes whose Gain are above average\n",
    "            # and then order them by Ratio.\n",
    "\n",
    "            sub_A={key: A[key] for key in attr_list}\n",
    "            attr_list=self.orderByGainOrRatio(D, sub_A, by='Ratio')\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='IGGR':\n",
    "\n",
    "            attr_list=self.orderByIGGR(D, A, 0.3, 0.7)\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "    def fit(self, D, A):\n",
    "        '''\n",
    "        Train the tree.\n",
    "        To save the training result:\n",
    "        >> self.tree=self.fit(D, A)\n",
    "        D: the training set, a size [m, n+1] numpy array (with str type elements), \n",
    "            where m is the number of training data and n is the number of attributes.\n",
    "            The last column of D is the classifications (or labels).\n",
    "        A: the attributes set. It is a dict with its structure being like \n",
    "            {attr_name: [index_in_D_columns, possibleVal_or_threshold], ...}\n",
    "            attr_name: name of the attribute\n",
    "            index_in_D_columns: the corresponding index of the attribute in ndarray D (starting from 0)\n",
    "            possibleVal_or_threshold: \n",
    "                ###################################################\n",
    "                ## This value may not always be available in A   ##\n",
    "                ## it is added after 'chooseAttribute' is called ##\n",
    "                ## And it will be updated after each call        ##\n",
    "                ###################################################\n",
    "                1, if the attribute is discrete, then it is a ndarray containing all possible values \n",
    "                    of this attribute.\n",
    "                2, if the attribute is continuous, then possibleVal_or_threshold is the most recent \n",
    "                    threshold.\n",
    "        '''\n",
    "        if len(D)==0:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=self.tmp_classification)\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(np.unique(D[:, -1]))<=1:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=D[0, -1])\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(A)==0 or len(np.unique(D[:, :-1], axis=0))<=1:\n",
    "            count_dict={}\n",
    "            for key in D[:, -1]:\n",
    "                count_dict[key]=count_dict.get(key, 0)+1\n",
    "            most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=most_frequent)\n",
    "            self.leaf_count+=1\n",
    "            return node \n",
    "\n",
    "        count_dict={}\n",
    "        for key in D[:, -1]:\n",
    "            count_dict[key]=count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "        self.tmp_classification=most_frequent\n",
    "\n",
    "        # choose target attribute\n",
    "        target_attr=self.chooseAttribute(D, A)\n",
    "        # print(target_attr)\n",
    "\n",
    "        # generate nodes for each possible values of the target attribute if it's discrete\n",
    "        # generate two nodes for the two classification if it's continuous\n",
    "        # related information is stored in A[target_attr][1] now, \n",
    "        # since we have called chooseAttribute at least once.\n",
    "        info=A[target_attr]\n",
    "\n",
    "        if self.feature_discrete[target_attr]:\n",
    "\n",
    "            node=Node(feature_name=target_attr, discrete=True, isLeaf=False)\n",
    "            # generate nodes for each possible values\n",
    "            for possibleVal in info[1]:\n",
    "                keys=set(A.keys()).difference({target_attr})\n",
    "                # connect node to its child\n",
    "                tmp_D=D[np.argwhere(D[:, info[0]]==possibleVal), :]\n",
    "                tmp_A={key: A[key] for key in keys}\n",
    "\n",
    "                node[possibleVal]=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), tmp_A)\n",
    "        \n",
    "        else:\n",
    "            # continuous\n",
    "            threshold=info[1]\n",
    "            node=Node(feature_name=target_attr, discrete=False, threshold=threshold,isLeaf=False)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]<=str(threshold)), :]\n",
    "            node['<=']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]>str(threshold)), :]\n",
    "            node['>']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "            \n",
    "        \n",
    "        return node\n",
    "\n",
    "    def post_prune(self, training_D, testing_D, A, current=None, parent=None):\n",
    "        '''\n",
    "        self.tree is required.\n",
    "        This method conducts the post-pruning to enhance the model performance.\n",
    "        To make sure this method will work, set \n",
    "        >> current=self.tree\n",
    "        when you call it.\n",
    "        '''\n",
    "        self.current_accuracy=self.evaluate(testing_D, A)\n",
    "        count_dict={}\n",
    "        if len(training_D)==0:\n",
    "            return \n",
    "        # print(training_D)\n",
    "        for key in training_D[:, -1]:\n",
    "            count_dict[key]=count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(training_D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "        leaf_parent=True\n",
    "        for key, node in current.map.items():\n",
    "            if not node.isLeaf:\n",
    "                leaf_parent=False\n",
    "                # Recursion, DFS\n",
    "                if node.discrete:\n",
    "                    tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]==key), :]\n",
    "                else:\n",
    "                    if key=='<=':\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]<=str(node.threshold)), :]\n",
    "                    else:\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]>str(node.threshold)), :]\n",
    "                self.post_prune(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), testing_D, A, parent=current, current=node)\n",
    "        \n",
    "        tmp_node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, classification=most_frequent)\n",
    "        \n",
    "        if parent:\n",
    "            # when current node is not the root\n",
    "            for key, node in parent.map.items():\n",
    "                if node==current:\n",
    "                    parent.map[key]=tmp_node\n",
    "                    saved_key=key\n",
    "                    break\n",
    "            # compare the evaluation, if it is enhanced then prune the tree.\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                parent.map[saved_key]=current\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "        else:\n",
    "            # when current node is the root\n",
    "            saved_tree=self.tree\n",
    "            self.tree=tmp_node\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                self.tree=saved_tree\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "\n",
    "    def predict(self, D, A):\n",
    "        '''\n",
    "        Predict the classification for the data in D.\n",
    "        For the definition of A, see method 'fit'. \n",
    "        There is one critical difference between D and that defined in 'fit':\n",
    "            the last column may or may not be the labels. \n",
    "            This method works as long as the feature index in A matches the corresponding\n",
    "            column in D.\n",
    "        '''\n",
    "        row, _=D.shape\n",
    "        pred=np.empty((row, 1), dtype=str)\n",
    "        tmp_data={key: None for key in A.keys()}\n",
    "        for i in range(len(D)):\n",
    "            for key, info in A.items():\n",
    "                tmp_data[key]=D[i, info[0]]\n",
    "            pred[i]=self.tree(tmp_data)\n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, testing_D, A):\n",
    "        '''\n",
    "        Evaluate the performance of decision tree. (Accuracy)\n",
    "        For definition of testing_D and A, see 'predict'.\n",
    "        However, here the testing_D is required to be labelled, that is, its last column \n",
    "        should be labels of the data.\n",
    "        '''\n",
    "        true_label=testing_D[:, -1]\n",
    "        pred_label=self.predict(testing_D, A)\n",
    "        \n",
    "        success_count=0\n",
    "        for i in range(len(true_label)):\n",
    "            if true_label[i]==pred_label[i]:\n",
    "                success_count+=1\n",
    "\n",
    "        return success_count/len(true_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree \"works\" with Adult Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age          Workclass  Fnlwgt   Education  EducationNum  \\\n",
       "0   39          State-gov   77516   Bachelors            13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
       "2   38            Private  215646     HS-grad             9   \n",
       "3   53            Private  234721        11th             7   \n",
       "4   28            Private  338409   Bachelors            13   \n",
       "\n",
       "         MaritalStatus          Occupation    Relationship    Race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   CapitalGain  CapitalLoss  HoursPerWeek         Country  Income  \n",
       "0         2174            0            40   United-States       1  \n",
       "1            0            0            13   United-States       1  \n",
       "2            0            0            40   United-States       1  \n",
       "3            0            0            40   United-States       1  \n",
       "4            0            0            40            Cuba       1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "row_names = [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"EducationNum\", \"MaritalStatus\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Gender\", \"CapitalGain\", \"CapitalLoss\",\n",
    "        \"HoursPerWeek\", \"Country\", \"Income\"]\n",
    "us_adult_income = pd.read_csv(dataset, names=row_names,na_values=[' ?'])\n",
    "us_adult_income[\"Income\"] = pd.Categorical(us_adult_income[\"Income\"])\n",
    "us_adult_income[\"Income\"] = us_adult_income[\"Income\"].cat.codes\n",
    "us_adult_income[\"Income\"] = 1 - us_adult_income[\"Income\"]\n",
    "\n",
    "us_adult_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 15)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult2 = us_adult_income.sample(frac=0.01, random_state=42)\n",
    "adult2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult25 = adult2.copy()\n",
    "adult25[\"Workclass\"] = pd.Categorical(adult25[\"Workclass\"])\n",
    "adult25[\"Workclass\"] = adult25[\"Workclass\"].cat.codes\n",
    "\n",
    "adult25[\"Education\"] = pd.Categorical(adult25[\"Education\"])\n",
    "adult25[\"Education\"] = adult25[\"Education\"].cat.codes\n",
    "\n",
    "adult25[\"MaritalStatus\"] = pd.Categorical(adult25[\"MaritalStatus\"])\n",
    "adult25[\"MaritalStatus\"] = adult25[\"MaritalStatus\"].cat.codes\n",
    "\n",
    "adult25[\"Occupation\"] = pd.Categorical(adult25[\"Occupation\"])\n",
    "adult25[\"Occupation\"] = adult25[\"Occupation\"].cat.codes\n",
    "\n",
    "adult25[\"Relationship\"] = pd.Categorical(adult25[\"Relationship\"])\n",
    "adult25[\"Relationship\"] = adult25[\"Relationship\"].cat.codes\n",
    "\n",
    "adult25[\"Race\"] = pd.Categorical(adult25[\"Race\"])\n",
    "adult25[\"Race\"] = adult25[\"Race\"].cat.codes\n",
    "\n",
    "adult25[\"Gender\"] = pd.Categorical(adult25[\"Gender\"])\n",
    "adult25[\"Gender\"] = adult25[\"Gender\"].cat.codes\n",
    "\n",
    "adult25[\"Country\"] = pd.Categorical(adult25[\"Country\"])\n",
    "adult25[\"Country\"] = adult25[\"Country\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18780</th>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14969</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15301</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23444</th>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Education  Occupation  Gender  CapitalLoss  Income\n",
       "14160   27         13           0       0            0       1\n",
       "27048   45         10           2       0            0       1\n",
       "28868   29          8           2       1            0       0\n",
       "5667    30          8           5       0            0       1\n",
       "7827    29         13           1       1            0       1\n",
       "...    ...        ...         ...     ...          ...     ...\n",
       "18780   26          8           8       0            0       1\n",
       "14969   59          1           1       1            0       1\n",
       "3383    40          7           1       1            0       1\n",
       "15301   39         13          12       1            0       1\n",
       "23444   24          8          10       1            0       1\n",
       "\n",
       "[326 rows x 6 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult25[[\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\", \"Income\"]]\n",
    "adult23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'sepal length': [0, 7.05],\n",
       "  'sepal width': [1, 2.55],\n",
       "  'petal length': [2, 5.35],\n",
       "  'petal width': [3, 1.55]},\n",
       " 'feature_discrete': {'sepal length': False,\n",
       "  'sepal width': False,\n",
       "  'petal length': False,\n",
       "  'petal width': False},\n",
       " 'treeType': 'IGGR'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D= np.genfromtxt('datafiles/iris_train.txt', dtype=str)\n",
    "np.random.shuffle(D)\n",
    "\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['62', '1', '136787', ..., '40', '13', '1'],\n",
       "       ['66', '2', '180211', ..., '30', '10', '1'],\n",
       "       ['45', '2', '38950', ..., '40', '13', '1'],\n",
       "       ...,\n",
       "       ['46', '3', '168796', ..., '55', '13', '1'],\n",
       "       ['40', '2', '289309', ..., '48', '13', '1'],\n",
       "       ['29', '4', '189346', ..., '50', '13', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D4 = D2.astype(str)\n",
    "D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['37', '9', '8', '0', '2559', '0'],\n",
       "       ['37', '10', '4', '1', '0', '1'],\n",
       "       ['90', '6', '0', '1', '0', '1'],\n",
       "       ...,\n",
       "       ['54', '10', '0', '1', '0', '1'],\n",
       "       ['24', '10', '2', '1', '0', '1'],\n",
       "       ['46', '10', '1', '1', '0', '0']], dtype='<U21')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D2= np.genfromtxt('datafiles/iris_train.txt', dtype=str)\n",
    "D2 = adult23.to_numpy()\n",
    "D2 = D2.astype(str)\n",
    "np.random.shuffle(D2)\n",
    "\n",
    "k=len(D2)\n",
    "\n",
    "# attribute_name=['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "A={name: [i] for i, name in enumerate(attribute_name)}\n",
    "# feature_discrete = {name: False for name in attribute_name}\n",
    "feature_discrete={name: False if isinstance(adult2[name].iloc[0], np.integer) else True for name in attribute_name}\n",
    "\n",
    "conf={'A': A, \n",
    "      'feature_discrete': feature_discrete, \n",
    "      'treeType':'IGGR'}\n",
    "\n",
    "dt=decisionTree(conf)\n",
    "dt.train(D2[:len(D2)//2])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D2[len(D2)//2:]))\n",
    "dt.prune(D2[:len(D2)//2], D2[len(D2)//2:])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D2[len(D2)//2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2[len(D2)//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': False,\n",
       " 'Education': True,\n",
       " 'Occupation': True,\n",
       " 'Relationship': True,\n",
       " 'Gender': True,\n",
       " 'CapitalLoss': False}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_discrete # both are in use now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.94 0.96, 0.98 1.0 - IGGR\n",
    "\n",
    "#### 0.92 0.94 0.96 0.98 - C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'sepal length': [0],\n",
       "  'sepal width': [1],\n",
       "  'petal length': [2],\n",
       "  'petal width': [3]},\n",
       " 'feature_discrete': {'sepal length': False,\n",
       "  'sepal width': False,\n",
       "  'petal length': False,\n",
       "  'petal width': False},\n",
       " 'treeType': 'CART'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Root===========\n",
      "[petal width]: {<= 0.75 leaf-0, > 0.75 petal length}\n",
      "\n",
      "[leaf-0]: {0} +++ [petal length]: {<= 4.75 leaf-1, > 4.75 petal width}\n",
      "\n",
      "[leaf-1]: {1} +++ [petal width]: {<= 1.55 petal width, > 1.55 leaf-4}\n",
      "\n",
      "[petal width]: {<= 1.45 leaf-2, > 1.45 leaf-3} +++ [leaf-4]: {2}\n",
      "\n",
      "[leaf-2]: {2} +++ [leaf-3]: {1}\n",
      "============Leaf===========\n"
     ]
    }
   ],
   "source": [
    "print(dt.tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree \n",
    "\n",
    "Code (modified) from https://github.com/sam-fletcher/SNR_DP_Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently includes methods as:\n",
    "        # _init_  : _trees\n",
    "        # get_domains\n",
    "        # reduce trees\n",
    "        # evaluate_accuracy_with_voting\n",
    "\n",
    "class DPRF_Forest:  \n",
    "    def __init__(self, \n",
    "                 training_data, # 2D list of the training data where the columns are the attributes, and the first column is the class attribute\n",
    "                 # test_data, # T\n",
    "                 epsilon, # epsilon, Budget, B, the total privacy budget\n",
    "                 f, # n of attributes to be split used by each dctree f\n",
    "                 max_depth # max tree depth \n",
    "                 ):\n",
    "\n",
    "        self._trees = []\n",
    "\n",
    "\n",
    "        ''' Some initialization '''\n",
    "        # Attribute set F\n",
    "        attribute_values = self.get_domains(training_data)\n",
    "\n",
    "        # Class values set\n",
    "        class_values = [str(y) for y in list(set([x[len(x)-1] for x in training_data]))]\n",
    "        attribute_indexes = [int(k) for k,v in attribute_values.items()]\n",
    "\n",
    "\n",
    "        print(attribute_values)\n",
    "        print(\"------------\")\n",
    "        print(class_values)\n",
    "        print(\"------------\")\n",
    "        print(attribute_indexes)\n",
    "        print(\"-----------\")\n",
    "\n",
    "        \n",
    "        # |D|\n",
    "        dataset_size = len(training_data)\n",
    "\n",
    "\n",
    "        valid_attribute_sizes = [[int(k),len(v)] for k,v in attribute_values.items()]\n",
    "\n",
    "        print(\"valid attribute sizes\")\n",
    "        print(valid_attribute_sizes)\n",
    "\n",
    "        # Number of trees t\n",
    "        num_trees = len(attribute_indexes)\n",
    "\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] )\n",
    "\n",
    "        # e = B / t\n",
    "        epsilon_per_tree = epsilon / float(num_trees)\n",
    "\n",
    "        ''' Calculating minimum support threshold '''\n",
    "        # estimated_support_min_depth = dataset_size / (average_domain**2) # a large number\n",
    "        # estimated_support_max_depth = dataset_size / (average_domain ** (len(attribute_indexes)/2)) # max tree depth is k/2 # a small number\n",
    "        # epsilon_per_tree = epsilon / float(num_trees)\n",
    "        # min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER # the minimum support in order for S>N\n",
    "\n",
    "\n",
    "        # while estimated_support_max_depth > min_support: # then we can have more trees\n",
    "        #    num_trees += 1\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        # while min_support > estimated_support_min_depth and num_trees>1: # then we need to have less trees\n",
    "        #    num_trees, valid_attribute_sizes, estimated_support_min_depth = self.reduce_trees(num_trees, valid_attribute_sizes, dataset_size)\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        print(\"NUM TREES = {} & EPSILON PER TREE = {}\".format(num_trees, epsilon_per_tree))\n",
    "\n",
    "        # ? \n",
    "        root_attributes = []\n",
    "\n",
    "       \n",
    "        for a in attribute_indexes:\n",
    "            if a in [x[0] for x in valid_attribute_sizes]:\n",
    "                root_attributes.append(a) # OR: [index, support, gini]\n",
    "\n",
    "        print(\"root attributes\")\n",
    "        print(root_attributes)\n",
    "\n",
    "        # for treeId = 1,2 ... t do:\n",
    "        for t in range(num_trees):\n",
    "\n",
    "            if not root_attributes:\n",
    "                root_attributes = attribute_indexes[:]\n",
    "\n",
    "            # randomly extract |D| samples from D w/ a bagging algo?\n",
    "            root = random.choice(root_attributes)\n",
    "            root_attributes.remove(root)\n",
    "\n",
    "            # TREE BUILDTREE(D, A, C, eps, f, Dm)\n",
    "            # D :\n",
    "            tree = Tree_DPDT(attribute_indexes, attribute_values, root, class_values, dataset_size, epsilon_per_tree) \n",
    "           \n",
    "            # TO EVOLVE HERE \n",
    "\n",
    "            # attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "            # A={name: [i] for i, name in enumerate(attribute_name)}\n",
    "            # feature_discrete = {name: False for name in attribute_name}\n",
    "            # feature_discrete={name: False if isinstance(adult2[name].iloc[0], np.integer) else True for name in attribute_name}\n",
    "\n",
    "            # conf={'A': A, \n",
    "            #      'feature_discrete': feature_discrete, \n",
    "            #       'treeType':'IGGR'}\n",
    "            # tree = decisionTree(conf)\n",
    "\n",
    "            num_unclassified = tree.filter_training_data_and_count(training_data, epsilon_per_tree, class_values)\n",
    "            \n",
    "            if num_unclassified > 0:\n",
    "                print(\"number of unclassified records = {}\".format(num_unclassified))\n",
    "            tree.prune_tree()\n",
    "\n",
    "            # FOREST = FOREST U TREE\n",
    "            self._trees.append(tree)\n",
    "\n",
    "\n",
    "    def get_domains(self, data):\n",
    "        attr_domains = {}\n",
    "        transData = np.transpose(data)\n",
    "        for i in range(0,len(data[0]) - 1):\n",
    "            attr_domains[str(i)] = [str(x) for x in set(transData[i])]\n",
    "            print(\"original domain length of categorical attribute {}: {}\".format(i, len(attr_domains[str(i)])))\n",
    "        return attr_domains\n",
    "\n",
    "\n",
    "    def reduce_trees(self, num_trees, valid_attribute_sizes, dataset_size):\n",
    "        num_trees -= 1\n",
    "        largest_attribute = sorted(valid_attribute_sizes, key=lambda x:x[1], reverse=True)[0]\n",
    "        #print(\"Removing att{} with domain size {}\".format(largest_attribute[0], largest_attribute[1])) \n",
    "        new_valids = [ x for x in valid_attribute_sizes if x[0] != largest_attribute[0] ]\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] ) # average with all attributes\n",
    "        narrowed_average_domain = np.mean([x[1] for x in valid_attribute_sizes if x[0] in [y[0] for y in new_valids] ]) # average without the big attributes\n",
    "        estimated_support_squared = dataset_size / (average_domain * narrowed_average_domain)\n",
    "        return num_trees, new_valids, estimated_support_squared\n",
    "\n",
    "\n",
    "    # GET MAJORITY LABELS (Forest, T, C) equivalent from pseudocode\n",
    "    def evaluate_accuracy_with_voting(self, records, class_index=0):\n",
    "\n",
    "        ''' Calculate the Prediction Accuracy of the Forest. '''\n",
    "        actual_labels = [x[class_index] for x in records]\n",
    "        predicted_labels = []\n",
    "        leafs_not_used = 0\n",
    "        count_of_averages_used = 0\n",
    "\n",
    "        # FOR EACH RECORD X IN DATASET DO\n",
    "        for rec in records:\n",
    "\n",
    "            class_value_fractions = defaultdict(list)\n",
    "\n",
    "            # FOR EACH TREE DO\n",
    "            for tree in self._trees:\n",
    "\n",
    "                # GET PREDICTED CLASSIFICATION RESULT\n",
    "                node, leaf_not_used = tree._classify(tree._root_node, rec)\n",
    "\n",
    "                noisy_class_counts = node._noisy_class_counts\n",
    "                leafs_not_used += leaf_not_used\n",
    "\n",
    "                support = float(sum([v for k,v in noisy_class_counts.items()]))\n",
    "                for k,v in noisy_class_counts.items():\n",
    "                    class_value_fractions[k].append(v/support)\n",
    "            best_confidences = {}\n",
    "\n",
    "            for k,lis in class_value_fractions.items():\n",
    "               best_confidences[k] = max(lis)\n",
    "            best = [None, 0.0]\n",
    "\n",
    "            for k,class_best in best_confidences.items():\n",
    "                if class_best > best[1]:\n",
    "                    best = [k, class_best]\n",
    "            average_used = False\n",
    "\n",
    "            for k,class_best in best_confidences.items():\n",
    "\n",
    "                if class_best == best[1] and k != best[0]:\n",
    "\n",
    "                    average_used = True\n",
    "                    #print(\"original best: {} vs. contender: {}\".format(best, (k,class_best)))\n",
    "                    orig_average = np.mean(class_value_fractions[best[0]])\n",
    "                    contender_average = np.mean(class_value_fractions[k])\n",
    "                    #print(\"original average: {} vs. contender: {}\".format(orig_average, contender_average))\n",
    "                    if contender_average > orig_average:\n",
    "                        best = [k, class_best]\n",
    "\n",
    "            count_of_averages_used += 1 if average_used else 0\n",
    "            predicted_labels.append(int(best[0]))\n",
    "\n",
    "        counts = Counter([x == y for x, y in zip(predicted_labels, actual_labels)])\n",
    "        \n",
    "        # acc = float(counts[True]) / len(records)\n",
    "        \n",
    "        return float(counts[True]) / len(records),   leafs_not_used / (len(records)*len(self._trees)),   count_of_averages_used / len(records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original domain length of categorical attribute 0: 55\n",
      "original domain length of categorical attribute 1: 13\n",
      "original domain length of categorical attribute 2: 13\n",
      "original domain length of categorical attribute 3: 2\n",
      "original domain length of categorical attribute 4: 10\n",
      "{'0': ['39', '51', '49', '22', '31', '25', '66', '34', '43', '28', '52', '48', '54', '53', '27', '42', '64', '35', '26', '59', '36', '81', '18', '37', '33', '20', '57', '50', '67', '61', '58', '62', '29', '24', '45', '19', '69', '30', '72', '63', '38', '41', '17', '56', '40', '46', '21', '32', '90', '60', '55', '68', '47', '23', '44'], '1': ['5', '0', '8', '6', '1', '10', '9', '2', '4', '7', '11', '13', '12'], '2': ['3', '5', '0', '8', '6', '1', '10', '2', '9', '4', '-1', '11', '12'], '3': ['0', '1'], '4': ['0', '1617', '1887', '2377', '2559', '1902', '1876', '2051', '1977', '1848']}\n",
      "------------\n",
      "['0', '1']\n",
      "------------\n",
      "[0, 1, 2, 3, 4]\n",
      "-----------\n",
      "valid attribute sizes\n",
      "[[0, 55], [1, 13], [2, 13], [3, 2], [4, 10]]\n",
      "NUM TREES = 5 & EPSILON PER TREE = 0.04\n",
      "root attributes\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tree_DPDT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-14ceba710a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdiff_priv_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPRF_Forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-254-ef38dbaf60ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data, epsilon, f, max_depth)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# TREE BUILDTREE(D, A, C, eps, f, Dm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree_DPDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_per_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# TO EVOLVE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tree_DPDT' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input:\n",
    "    # Training set D \n",
    "    # Privacy budget B (epsilon)\n",
    "\n",
    "    # Test Set T (?)  (Maybe?)\n",
    "\n",
    "    # Number of attributes to be split used by each dec tree f \n",
    "    # Max tree depth d_m\n",
    "\n",
    "B = 0.2\n",
    "f = 3\n",
    "dm = 5\n",
    "\n",
    "diff_priv_forest = DPRF_Forest(D2[100:], B, f, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['42', '10', '1', '0', '0', '1'],\n",
       "       ['50', '10', '1', '1', '0', '0'],\n",
       "       ['18', '1', '6', '0', '0', '1'],\n",
       "       ...,\n",
       "       ['47', '10', '0', '0', '0', '1'],\n",
       "       ['32', '8', '8', '1', '0', '1'],\n",
       "       ['29', '13', '0', '1', '0', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the classification result set is the left column \n",
    "D2[100:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
