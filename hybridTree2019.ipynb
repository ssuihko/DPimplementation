{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/step-by-step-decision-tree-id3-algorithm-from-scratch-in-python-no-fancy-library-4822bbfdd88f\n",
    "# ID3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute vs Measure\n",
    "https://analystanswers.com/what-is-a-data-attribute-definition-types-examples/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DunZhang/DPTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetiris = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "row_names = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"]\n",
    "\n",
    "irisdata = pd.read_csv(datasetiris, names=row_names,na_values=[' ?'])\n",
    "irisdata['class'].replace(to_replace=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], value=[1, 2, 3], inplace=True)\n",
    "irisdata[\"class\"] = irisdata[\"class\"].astype(int)\n",
    "Diris = irisdata.to_numpy()\n",
    "# Diris = Diris.astype(str)\n",
    "Diris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Diris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age          Workclass  Fnlwgt   Education  EducationNum  \\\n",
       "0   39          State-gov   77516   Bachelors            13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
       "2   38            Private  215646     HS-grad             9   \n",
       "3   53            Private  234721        11th             7   \n",
       "4   28            Private  338409   Bachelors            13   \n",
       "\n",
       "         MaritalStatus          Occupation    Relationship    Race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   CapitalGain  CapitalLoss  HoursPerWeek         Country  Income  \n",
       "0         2174            0            40   United-States       1  \n",
       "1            0            0            13   United-States       1  \n",
       "2            0            0            40   United-States       1  \n",
       "3            0            0            40   United-States       1  \n",
       "4            0            0            40            Cuba       1  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "row_names = [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"EducationNum\", \"MaritalStatus\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Gender\", \"CapitalGain\", \"CapitalLoss\",\n",
    "        \"HoursPerWeek\", \"Country\", \"Income\"]\n",
    "us_adult_income = pd.read_csv(dataset, names=row_names,na_values=[' ?'])\n",
    "us_adult_income[\"Income\"] = pd.Categorical(us_adult_income[\"Income\"])\n",
    "us_adult_income[\"Income\"] = us_adult_income[\"Income\"].cat.codes\n",
    "us_adult_income[\"Income\"] = 1 - us_adult_income[\"Income\"]\n",
    "\n",
    "us_adult_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 15)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "adult2 = us_adult_income.sample(frac=0.05, random_state=42)\n",
    "adult2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult25 = adult2.copy()\n",
    "adult25[\"Workclass\"] = pd.Categorical(adult25[\"Workclass\"])\n",
    "adult25[\"Workclass\"] = adult25[\"Workclass\"].cat.codes\n",
    "\n",
    "adult25[\"Education\"] = pd.Categorical(adult25[\"Education\"])\n",
    "adult25[\"Education\"] = adult25[\"Education\"].cat.codes\n",
    "\n",
    "adult25[\"MaritalStatus\"] = pd.Categorical(adult25[\"MaritalStatus\"])\n",
    "adult25[\"MaritalStatus\"] = adult25[\"MaritalStatus\"].cat.codes\n",
    "\n",
    "adult25[\"Occupation\"] = pd.Categorical(adult25[\"Occupation\"])\n",
    "adult25[\"Occupation\"] = adult25[\"Occupation\"].cat.codes\n",
    "\n",
    "adult25[\"Relationship\"] = pd.Categorical(adult25[\"Relationship\"])\n",
    "adult25[\"Relationship\"] = adult25[\"Relationship\"].cat.codes\n",
    "\n",
    "adult25[\"Race\"] = pd.Categorical(adult25[\"Race\"])\n",
    "adult25[\"Race\"] = adult25[\"Race\"].cat.codes\n",
    "\n",
    "adult25[\"Gender\"] = pd.Categorical(adult25[\"Gender\"])\n",
    "adult25[\"Gender\"] = adult25[\"Gender\"].cat.codes\n",
    "\n",
    "adult25[\"Country\"] = pd.Categorical(adult25[\"Country\"])\n",
    "adult25[\"Country\"] = adult25[\"Country\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17491</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18954</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15112</th>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1628 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Education  Occupation  Gender  Income\n",
       "14160   27         15           0       0       1\n",
       "27048   45         11           3       0       1\n",
       "28868   29          9           3       1       0\n",
       "5667    30          9           6       0       1\n",
       "7827    29         15           2       1       1\n",
       "...    ...        ...         ...     ...     ...\n",
       "17491   19         11          11       0       1\n",
       "18954   50          8           9       1       1\n",
       "15112   44         11           2       1       1\n",
       "5477    19         15           9       1       1\n",
       "1441    61         11          -1       1       1\n",
       "\n",
       "[1628 rows x 5 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult25[[\"Age\", \"Education\", \"Occupation\", \"Gender\", \"Income\"]] # Education, Occupation and Gender are discrete. \n",
    "adult23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 9     232\n",
       " 3     208\n",
       " 2     198\n",
       " 0     190\n",
       " 11    184\n",
       " 7     150\n",
       "-1     101\n",
       " 6      95\n",
       " 13     87\n",
       " 5      61\n",
       " 4      47\n",
       " 10     36\n",
       " 12     34\n",
       "Name: Occupation, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult23[adult23[\"Occupation\"] != 1]\n",
    "adult23 = adult23[adult23[\"Occupation\"] != 8]\n",
    "adult23[\"Occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    504\n",
       "15    362\n",
       "9     256\n",
       "12     95\n",
       "8      67\n",
       "1      63\n",
       "7      62\n",
       "0      46\n",
       "5      38\n",
       "14     36\n",
       "6      32\n",
       "2      24\n",
       "10     16\n",
       "4      15\n",
       "Name: Education, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult23[adult23[\"Education\"] != 13]\n",
    "adult23 = adult23[adult23[\"Education\"] != 3]\n",
    "adult23[\"Education\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(97)\n",
    "D2 = adult23.to_numpy()\n",
    "D2 = D2.astype(str)\n",
    "np.random.shuffle(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['34', '9', '9', '0', '1'],\n",
       "       ['26', '14', '9', '1', '0'],\n",
       "       ['57', '4', '2', '1', '1'],\n",
       "       ...,\n",
       "       ['38', '9', '11', '1', '0'],\n",
       "       ['41', '12', '9', '0', '1'],\n",
       "       ['23', '15', '5', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D4 = D2.astype(str)\n",
    "D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both test and training data have all classes in them\n",
    "# the code will not run if D88 and D99 do not print the same arrays\n",
    "D22 = D2[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D88 = D22[:len(D22)//2]\n",
    "np.sort(np.unique(D88[:,2]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D99 = D22[len(D22)//2:]\n",
    "np.sort(np.unique(D99[:,2]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D88 = D22[:len(D22)//2]\n",
    "np.sort(np.unique(D88[:,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D99 = D22[len(D22)//2:]\n",
    "np.sort(np.unique(D99[:,1]).astype(int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/loginaway/DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    '''\n",
    "    Basic element for decision tree.\n",
    "    Initialization: to initialize a tree node, at least feature_name is needed.\n",
    "    Connection: to connect different nodes, there are two situations.\n",
    "        1, if the parent node is discrete (i.e. node.discrete==True), \n",
    "            then \n",
    "            >> node[feature_value]=childnode \n",
    "            will do the connection.\n",
    "        2, if the parent node is continuous, then \n",
    "            >> node['<=']=childnode \n",
    "            or \n",
    "            >> node['>']=childnode \n",
    "            will assign its children. To be specific, \n",
    "            node['<=']=childnode means if the feature value is bigger than \n",
    "            node.threshold, then the workflow goes left to the childnode, and vice versa.\n",
    "    \n",
    "    Classification: if you have constructed a tree, you may want to classify a \n",
    "        certain group of data. Simply call the root node will do this task, i.e.\n",
    "        >> root(data)\n",
    "        where data should be a dictionary containing the corresponding key and value.\n",
    "        And this method will return the classification result.\n",
    "\n",
    "    Visualization: A naive visualization is implemented here. \n",
    "        For example, you may visualize a subtree rooted at 'node' by\n",
    "        >> print(node)\n",
    "        The tree structure will then be printed on the console.\n",
    "    '''\n",
    "    def __init__(self, feature_name='', discrete=True, threshold=0, depth=0, isLeaf=False, classification=None):\n",
    "        # for discrete node: the next node can be retrieved by self.map[feature_value]\n",
    "        # for continuous node: by self.map['<='] (when value <= self.threshold)\n",
    "        #                       by self.map['>'] (when value > self.threshold)\n",
    "\n",
    "        self.map=dict()\n",
    "\n",
    "        self.discrete=discrete\n",
    "        self.feature_name=feature_name\n",
    "        self.isLeaf=isLeaf\n",
    "        if isLeaf:\n",
    "            self.classification=classification\n",
    "        if not discrete:\n",
    "            self.threshold=threshold\n",
    "        self.depth=depth\n",
    "        self._class_counts = defaultdict(int)\n",
    "        self._noisy_class_counts = None\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.map[key]=value\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.map.get(key)\n",
    "\n",
    "    def __getdepth__(self):\n",
    "        return self.depth \n",
    "\n",
    "    def increment_class_count(self, class_value):\n",
    "        self._class_counts[str(class_value)] += 1\n",
    "\n",
    "    def __noisy_class_counts__(self, epsilon, class_values):\n",
    "        \n",
    "        ''' Add noise to the class counts of this leaf. Not performed on parents. '''\n",
    "        if not self._noisy_class_counts and not self._children: # to make sure this code is only run once per leaf\n",
    "            \n",
    "            counts = {}\n",
    "            for val in class_values:\n",
    "\n",
    "                if val in self._class_counts:\n",
    "                    # Laplace noise added to THE CLASS COUNTS\n",
    "                    counts[val] = max( 0, int(self._class_counts[val] + np.random.laplace(scale=float(1./epsilon))) )\n",
    "                else: # original count was 0\n",
    "                    counts[val] = max( 0, int(np.random.laplace(scale=float(1./epsilon))) )\n",
    "\n",
    "            self._noisy_class_counts = counts\n",
    "\n",
    "            # what I'm assuming is equation (10)\n",
    "            self._signal_to_noise = (epsilon * 1.0 * sum([v for k,v in counts.items()])) / (math.sqrt(2*1.0) * len(counts)) # enS / (sqrt(2n) * num_classes)\n",
    "         \n",
    "            #print(\"self._signal_to_noise = {}\".format(self._signal_to_noise))\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 # we're summing the redundant calls\n",
    "\n",
    "    def __call__(self, data): # returns the classification result! \n",
    "        \n",
    "        '''\n",
    "        This method should be used on the root to predict its classification.\n",
    "        data: a dict with its key being the features and value being \n",
    "        the corresponding value.\n",
    "        '''\n",
    "\n",
    "        # print(\"Inside __call__ method of Node\")\n",
    "\n",
    "        #print(\"data: \")\n",
    "        #print(data)\n",
    "\n",
    "        #print(\"feature name: \")\n",
    "        #print(self.feature_name)\n",
    "\n",
    "        #print(\"the map right now\")\n",
    "        #print(self.map)\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classification\n",
    "\n",
    "        # if the node has a discrete feature, use __getitem__ to find the next node\n",
    "        # then call the next node.\n",
    "        if self.discrete:\n",
    "            return self.map[data[self.feature_name]](data)\n",
    "\n",
    "        else:\n",
    "        # node is not discrete: print the self.feature_name\n",
    "            #print(\"continuous name\")\n",
    "            #print(self.feature_name)\n",
    "\n",
    "            if data[self.feature_name].astype(np.float32)>self.threshold:\n",
    "                return self.map['>'](data)\n",
    "            else:\n",
    "                return self.map['<='](data)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This method is designed for printing the subtree rooted at the current node.\n",
    "        To print the tree, try print(self).\n",
    "        '''\n",
    "        hierarchy={}\n",
    "        stack=[(0, 0, self)]\n",
    "        count=0\n",
    "        while stack:\n",
    "            # BFS\n",
    "            (layer_index, node_index, current)=stack.pop(0)\n",
    "            layer_index+=1\n",
    "\n",
    "            hierarchy[layer_index]=hierarchy.get(layer_index, [])\n",
    "            hierarchy[layer_index].append((node_index, current.feature_name,\\\n",
    "                    [(str(current.classification),)] if current.isLeaf else \\\n",
    "                     [(str(key), '' if current.discrete else str(current.threshold), item.feature_name) for key, item in current.map.items()]))\n",
    "            # print(hierarchy)\n",
    "\n",
    "            if current.isLeaf:\n",
    "                continue\n",
    "            for _, item in current.map.items():\n",
    "                count+=1\n",
    "                stack.append((layer_index, count, item))\n",
    "            # print(item_index_map)\n",
    "\n",
    "        totallist=[]\n",
    "        for layer, layer_content in hierarchy.items():\n",
    "            rowlist=[]\n",
    "            for i in layer_content:\n",
    "                rowlist.append('['+i[1]+']'+': {'+', '.join([' '.join(item) for item in i[2]])+'}')\n",
    "            rowstr=' +++ '.join(rowlist)\n",
    "            totallist.append(rowstr)\n",
    "\n",
    "        return '============Root===========\\n'+'\\n\\n'.join(totallist)+'\\n============Leaf==========='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '399']\n",
      " ['1' '1217']]\n"
     ]
    }
   ],
   "source": [
    "herlist = D2[:, 4]\n",
    "unique, counts = np.unique(herlist, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2333333333333334"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = 5\n",
    "sum(2/(dm - i) for i in range(0, (dm - 2))) + (2 / (dm - (dm-1) + 2 )) + 1 # assuming we reach maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently includes methods as:\n",
    "        # _init_  : _trees\n",
    "        # get_domains\n",
    "        # reduce trees\n",
    "        # evaluate_accuracy_with_voting\n",
    "\n",
    "class DPRF_Forest:  \n",
    "    def __init__(self, \n",
    "                 training_nonprocessed,\n",
    "                 training_dataframe, \n",
    "                 training_data, # 2D list of the training data where the columns are the attributes, and the first column is the class attribute\n",
    "                 # test_data, # T\n",
    "                 epsilon, # epsilon, Budget, B, the total privacy budget\n",
    "                 f, # n of attributes to be split used by each dctree f\n",
    "                 max_depth, # max tree depth,\n",
    "                 feature_discrete \n",
    "                 ):\n",
    "\n",
    "        # Forest = {}\n",
    "        self._trees = []\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.feature_discrete = feature_discrete\n",
    "\n",
    "\n",
    "        ''' Some initialization '''\n",
    "        # Attribute set F\n",
    "        attribute_values = self.get_domains(training_data)\n",
    "\n",
    "        # Class values set\n",
    "        class_values = [str(y) for y in list(set([x[len(x)-1] for x in training_data]))]\n",
    "        attribute_indexes = [int(k) for k,v in attribute_values.items()]\n",
    "\n",
    "\n",
    "        #print(attribute_values)\n",
    "        #print(\"------------\")\n",
    "        #print(class_values)\n",
    "        #print(\"------------\")\n",
    "        #print(attribute_indexes)\n",
    "        #print(\"-----------\")\n",
    "\n",
    "        \n",
    "        # |D| is the sum of classes in the dataset Sigma n_c, n_c in N^C_D\n",
    "        # It determines the random data points that we're going to select from D\n",
    "        valid_attribute_sizes = [[int(k),len(v)] for k,v in attribute_values.items()]\n",
    "\n",
    "        # Number of trees t\n",
    "        num_trees = len(attribute_indexes) \n",
    "\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] )\n",
    "\n",
    "        # e = B / t\n",
    "        epsilon_per_tree = epsilon / float(num_trees)\n",
    "\n",
    "        ''' Calculating minimum support threshold '''\n",
    "        # estimated_support_min_depth = dataset_size / (average_domain**2) # a large number\n",
    "        # estimated_support_max_depth = dataset_size / (average_domain ** (len(attribute_indexes)/2)) # max tree depth is k/2 # a small number\n",
    "        # epsilon_per_tree = epsilon / float(num_trees)\n",
    "        # min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER # the minimum support in order for S>N\n",
    "\n",
    "\n",
    "        # while estimated_support_max_depth > min_support: # then we can have more trees\n",
    "        #    num_trees += 1\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        # while min_support > estimated_support_min_depth and num_trees>1: # then we need to have less trees\n",
    "        #    num_trees, valid_attribute_sizes, estimated_support_min_depth = self.reduce_trees(num_trees, valid_attribute_sizes, dataset_size)\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        print(\"NUM TREES = {} & EPSILON PER TREE = {}\".format(num_trees, epsilon_per_tree))\n",
    "\n",
    "        # ? \n",
    "        root_attributes = []\n",
    "\n",
    "       \n",
    "        for a in attribute_indexes:\n",
    "            if a in [x[0] for x in valid_attribute_sizes]:\n",
    "                root_attributes.append(a) # OR: [index, support, gini]\n",
    "\n",
    "        print(\"root attributes\")\n",
    "        print(root_attributes)\n",
    "\n",
    "        # for treeId = 1,2 ... t do:\n",
    "        for t in range(num_trees):\n",
    "\n",
    "            if not root_attributes:\n",
    "                root_attributes = attribute_indexes[:]\n",
    "\n",
    "            # randomly extract |D| samples from D w/ a bagging algo?\n",
    "            root = random.choice(root_attributes)\n",
    "            root_attributes.remove(root)\n",
    "\n",
    "            print(\"what is root attributes: \")\n",
    "            print(root_attributes)\n",
    "\n",
    "\n",
    "            # TREE BUILDTREE(D, A, C, eps, f, Dm)\n",
    "            # D :\n",
    "\n",
    "            # attribute_indexes, - A_ind\n",
    "            # attribute_values, A - not exactly the same after all! \n",
    "            # root, \n",
    "            # class_values, \n",
    "            # feature_discrete\n",
    "            # treetype \n",
    "            # dataset_size, \n",
    "            # epsilon_per_tree\n",
    "\n",
    "            A={name: [i] for i, name in enumerate(training_dataframe.columns[:-1])}\n",
    "\n",
    "            # {'Age': False, 'Education': True, 'Occupation': True, 'Gender': True, 'CapitalLoss': False}\n",
    "            # if the column in the original dataframe is integervalued: False, \n",
    "            # this needs to be passed to the algorithm... \n",
    "            #feature_discre={name: False if isinstance(training_nonprocessed[name].iloc[0], np.integer) else True for name in training_dataframe.columns[:-1]}\n",
    "\n",
    "            # print(\"feature discrete\")\n",
    "            # print(feature_discre)\n",
    "\n",
    "            #print(\"---------------\")\n",
    "            #print(\"A: \")\n",
    "            #print(A)\n",
    "            #print(\"---------------\")\n",
    "            #print(\"attribute values: \")\n",
    "            #print(attribute_values)\n",
    "            #print(\"---------------\")\n",
    "            #print(\"---------------\")\n",
    "\n",
    "\n",
    "            # -> bagging algorithm?\n",
    "            l = random.sample(range(0, len(training_data)), 1180)\n",
    "            tr_data = training_data[l]\n",
    "\n",
    "            dataset_size = len(tr_data)\n",
    "\n",
    "            tree = Tree_DPDT(attribute_indexes, \n",
    "                            A, # original style A\n",
    "                            attribute_values, # new style attribute vals\n",
    "                            root, \n",
    "                            class_values, \n",
    "                            self.feature_discrete, \n",
    "                            'IGGR',  # treetype\n",
    "                            dataset_size, \n",
    "                            epsilon_per_tree,\n",
    "                            max_depth) \n",
    "\n",
    "            # each tree needs to be trained on a different subsample of the training data! \n",
    "            tree.train(training_data[:len(tr_data)//2], 1)\n",
    "           \n",
    "            # TO EVOLVE HERE \n",
    "\n",
    "            # attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "            #num_unclassified = tree.filter_training_data_and_count(training_data, epsilon_per_tree, class_values)\n",
    "            \n",
    "            #if num_unclassified > 0:\n",
    "            #    print(\"number of unclassified records = {}\".format(num_unclassified))\n",
    "            \n",
    "\n",
    "            print(\"currently pruning! \")\n",
    "            # D2[:len(D2)//2], D2[len(D2)//2:]\n",
    "            tree.prune(training_data[:len(tr_data)//2], tr_data[len(tr_data)//2:])\n",
    "\n",
    "            # FOREST = FOREST U TREE\n",
    "            self._trees.append(tree)\n",
    "\n",
    "        # print(self._trees)\n",
    "        # print(len(self._trees))\n",
    "        # print(\"succesfully ended building tree dictionary\")\n",
    "\n",
    "\n",
    "    def get_domains(self, data):\n",
    "        attr_domains = {}\n",
    "        transData = np.transpose(data)\n",
    "        for i in range(0,len(data[0]) - 1):\n",
    "            attr_domains[str(i)] = [str(x) for x in set(transData[i])]\n",
    "            print(\"original domain length of categorical attribute {}: {}\".format(i, len(attr_domains[str(i)])))\n",
    "        return attr_domains\n",
    "\n",
    "\n",
    "    def reduce_trees(self, num_trees, valid_attribute_sizes, dataset_size):\n",
    "        num_trees -= 1\n",
    "        largest_attribute = sorted(valid_attribute_sizes, key=lambda x:x[1], reverse=True)[0]\n",
    "        #print(\"Removing att{} with domain size {}\".format(largest_attribute[0], largest_attribute[1])) \n",
    "        new_valids = [ x for x in valid_attribute_sizes if x[0] != largest_attribute[0] ]\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] ) # average with all attributes\n",
    "        narrowed_average_domain = np.mean([x[1] for x in valid_attribute_sizes if x[0] in [y[0] for y in new_valids] ]) # average without the big attributes\n",
    "        estimated_support_squared = dataset_size / (average_domain * narrowed_average_domain)\n",
    "        return num_trees, new_valids, estimated_support_squared\n",
    "\n",
    "\n",
    "    # GET MAJORITY LABELS (Forest, T, C) equivalent from pseudocode\n",
    "    def evaluate_accuracy_with_voting(self, records, class_index=4):\n",
    "\n",
    "        ''' Calculate the Prediction Accuracy of the Forest. '''\n",
    "        actual_labels = [x[class_index] for x in records]\n",
    "\n",
    "        predicted_labels = []\n",
    "        leafs_not_used = 0\n",
    "        count_of_averages_used = 0\n",
    "\n",
    "        class_counts = defaultdict(list)\n",
    "\n",
    "        # FOR EACH RECORD X IN DATASET DO\n",
    "        # Strange for we need to access individual predictions here...\n",
    "        for rec in records:\n",
    "\n",
    "            h = []\n",
    "\n",
    "            class_value_fractions = defaultdict(list)\n",
    "\n",
    "            # FOR EACH TREE DO\n",
    "            for tree in self._trees:\n",
    "\n",
    "                # GET PREDICTED CLASSIFICATION RESULT for a single record\n",
    "                # node, leaf_not_used = tree._classify(tree._root_node, rec)\n",
    "                result = tree.pred(np.array([rec]))\n",
    "\n",
    "                h.append(*result)\n",
    "\n",
    "            # majority vote\n",
    "\n",
    "            # IndexError: index 1 is out of bounds for axis 0 with size 1\n",
    "            hh = [int(*el) for el in h]\n",
    "\n",
    "            print(\"votes: \")\n",
    "            print(hh)\n",
    "\n",
    "            c = Counter(hh)\n",
    "            ans = c.most_common(1)\n",
    "\n",
    "            predicted_labels.append(ans[0][0])\n",
    "\n",
    "        print(\"data and trees looped through!\")\n",
    "\n",
    "        # counts how many records we got right!        \n",
    "        print(type(predicted_labels[0]))\n",
    "        print(type(actual_labels[0]))\n",
    "\n",
    "        # print(zip(predicted_labels, actual_labels))\n",
    "\n",
    "        counts = Counter([x == y for x, y in zip(map(str, predicted_labels), actual_labels)])\n",
    "        \n",
    "        # acc = float(counts[True]) / len(records)\n",
    "        \n",
    "        return float(counts[True]) / len(records)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[0, 2, 3, 4, 5]\n",
      "[0, 1, 3, 4, 5]\n",
      "[0, 1, 2, 4, 5]\n",
      "[0, 1, 2, 3, 5]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "l = list(range(0,6))\n",
    "\n",
    "for i in l:\n",
    "    h = [g for g in l if g != i]\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "#from Node import Node\n",
    "\n",
    "# set the maximal recursion limits here.\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# to be modified to single class\n",
    "\n",
    "# attribute_indexes, - A_ind\n",
    "# attribute_values, A\n",
    "# root, \n",
    "# class_values, \n",
    "# feature_discrete\n",
    "# treetype \n",
    "# dataset_size, \n",
    "# epsilon_per_tree\n",
    "\n",
    "\n",
    "class Tree_DPDT(DPRF_Forest):\n",
    "\n",
    "    '''\n",
    "    The main class of decision tree.\n",
    "    '''\n",
    "    def __init__(self, A_ind, A, attribute_values, root, class_values, feature_discrete, treetype, dataset_size, epsilon_per_tree, max_depth, current_depth=None):\n",
    "        '''\n",
    "        attribute_values : attribute_values\n",
    "        A : orignal\n",
    "        feature_discrete: a dict with its each key-value pair being (feature_name: True/False),\n",
    "            where True means the feature is discrete and False means the feature is \n",
    "            continuous. \n",
    "        type: ID3/C4.5/CART\n",
    "        pruning: pre/post\n",
    "        '''\n",
    "\n",
    "        self.A=A\n",
    "        self.attribute_values=attribute_values\n",
    "        self.A_ind=A_ind\n",
    "        self.feature_discrete= feature_discrete\n",
    "        self.treeType=treetype\n",
    "        self.leaf_count=0\n",
    "        self.tmp_classification=''\n",
    "        self.class_values=class_values\n",
    "        self._root_node = root\n",
    "        self.tree=None\n",
    "        self.dataset_size=dataset_size\n",
    "        self.epsilon = epsilon_per_tree # B / t\n",
    "        self.current_depth = current_depth if current_depth else 0\n",
    "        self.dm = max_depth\n",
    "        self.current_IGGR = 1\n",
    "        self.w = None\n",
    "\n",
    "        self.eu = epsilon_per_tree / sum(2/(self.dm - i) for i in range(0, (self.dm - 2))) + (2 / (self.dm - (self.dm-1) + 2 )) + 1\n",
    "        self.ei = self.eu\n",
    "        self.ei1 = 0 \n",
    "        self.ei2 = self.ei\n",
    "        self.current_node = None\n",
    "\n",
    "    def Entropy(self, list_of_class):\n",
    "        '''\n",
    "        Compute the entropy for the given list of class.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        'duck': 2/3, 'dolphin': 1/3, so the entropy for this array is 0.918\n",
    "        '''\n",
    "        count={}\n",
    "        for key in list_of_class:\n",
    "            count[key]=count.get(key, 0)+1\n",
    "\n",
    "            \n",
    "        frequency=np.array(tuple(count.values()))/len(list_of_class)\n",
    "        return -1*np.vdot(frequency, np.log2(frequency))\n",
    "\n",
    "    def Information_Gain(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Gain.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Gain for this example is 0.2516.\n",
    "        '''\n",
    "        sec2=np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        return self.Entropy(list_of_class)-sec2\n",
    "\n",
    "\n",
    "    def Information_Ratio(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Ratio.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Ratio for this example is 0.2740.\n",
    "        '''\n",
    "        tmp=np.array( [len(item)/len(list_of_class) for item in grouped_list_of_class] )\n",
    "        \n",
    "        # Here we assume instrinsic_value is SplitInformation! \n",
    "        intrinsic_value=-1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        return self.Information_Gain(list_of_class, grouped_list_of_class) / intrinsic_value , intrinsic_value\n",
    "\n",
    "\n",
    "    def IGGR(self, list_of_class, grouped_list_of_class, a1, a2, eps, D, possibleVal, info ):      \n",
    "\n",
    "        infoGainRes = self.Information_Gain(list_of_class, grouped_list_of_class)\n",
    "\n",
    "        # unpredictable behavior due to split-information...\n",
    "        GainRatioRes, intrinsic_value_res = self.Information_Ratio(list_of_class, grouped_list_of_class)\n",
    "\n",
    "        # f(x)\n",
    "        # sol1 = a1 * infoGainRes + a2 * GainRatioRes\n",
    "        sol2 = (a1 + ( a2 / intrinsic_value_res) ) * infoGainRes \n",
    "\n",
    "        # f(y) \n",
    "        #print(list_of_class)\n",
    "        #print(grouped_list_of_class)\n",
    "\n",
    "\n",
    "        # what is the sensitivity of the score function? \n",
    "        \n",
    "        # sens_d = self.sensitivityIGGR(D, possibleVal, list_of_class, a1, a2, sol2, info)\n",
    "\n",
    "\n",
    "        # exp_noise = np.exp( (eps * sol2) / (2 * sens_delta) )\n",
    "\n",
    "        #exp_n = 6\n",
    "        # return np.random.choice(R, 1, p=probabilities)[0] -> not applicable here\n",
    "\n",
    "        # return sol2, exp_noise\n",
    "\n",
    "        return sol2\n",
    "\n",
    "\n",
    "    def orderByIGGR(self, D, A, a1, a2, eps):\n",
    "\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        for attr, info in A.items():\n",
    "\n",
    "            possibleVal=np.unique(D[:, info[0]])  #info[0] is the index to the array column where possible values of attribute are in D\n",
    "            # this should not affect sensitivity...\n",
    "\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "\n",
    "                for val in possibleVal: # 1, 2 (gender)\n",
    "\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val) # indices in the D[gender] where gender is \"men\", e.g\n",
    "                    # what are the class values of these rows? \n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten()) \n",
    "\n",
    "\n",
    "                IC_value = self.IGGR( D[:, -1], grouped_list_of_class, a1, a2, eps, D, possibleVal, info[0] )\n",
    "\n",
    "\n",
    "                # {\"Gender\" :  4.439376852676813e-293 } # how good the output is for input D\n",
    "                tmp_value_dict[attr] = IC_value\n",
    "\n",
    "            else:\n",
    "\n",
    "                # continuous\n",
    "\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "                for point in split_points:\n",
    "\n",
    "                    # modified now to float comparisons\n",
    "                    # causes the code to crash to max recursion\n",
    "                    # smaller_set = D[np.argwhere(D[:, info[0]].astype(float) <= point ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    # bigger_set = D[np.argwhere(D[:, info[0]].astype(float) > point ), -1].flatten()\n",
    "                    \n",
    "                    smaller_set = D[np.argwhere(D[:, info[0]] <= str(point) ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    bigger_set = D[np.argwhere(D[:, info[0]] > str(point) ), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    # list_of_class, grouped_list_of_class, a1, a2\n",
    "                    IC_tmp = self.IGGR( D[:, -1], (smaller_set, bigger_set), a1, a2, eps, D, possibleVal, info[0] )\n",
    "\n",
    "\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                    #print(\"attribute was: \", attr)\n",
    "                    #print(\"sets (small and big)\")\n",
    "                    #print(smaller_set)\n",
    "                    #print(bigger_set)\n",
    "\n",
    "                    # IC_tmp=0.0\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # exponential noise\n",
    "        probabilities = { el : np.exp(eps * tmp_value_dict[el] ) / (2 * 1) for el in tmp_value_dict }\n",
    "\n",
    "        g2 = list(probabilities.values())\n",
    "        probs = g2 / np.linalg.norm(g2, ord=1)\n",
    "\n",
    "        probabilities = dict(zip( list(probabilities.keys()), probs))\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        # attr_list=list(tmp_value_dict.keys())\n",
    "        # attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        attr_list=list(probabilities.keys())\n",
    "        attr_list.sort(key=lambda x: probabilities[x])\n",
    "\n",
    "        #print(\"val dict: \", tmp_value_dict)\n",
    "        #print(\"exponentioally: \", probabilities )\n",
    "        # val dict:  {'Age': 1.0000787448493484, 'Education': 1.0000514406039214, 'Occupation': 1.000072095453044, 'Gender': 1.0000031999841101}\n",
    "        # print(\"attribute list: \", attr_list)\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def chooseAttribute(self, D, A, eps):\n",
    "\n",
    "        '''\n",
    "        Choose an attribute from A according to the metrics above.\n",
    "        For the definition of D and A, see method 'fit'.\n",
    "        Different principal for different tree types:\n",
    "        ID3: choose the attribute that maximizes the Information Gain.\n",
    "        C4.5: \n",
    "            1, choose those attributes whose Information Gain are above average.\n",
    "            2, choose the one that maximizes the Gain Ratio from these attributes.\n",
    "        CART: choose the attribute that minimizes the Gini Index.\n",
    "        IG_GR: \n",
    "        '''\n",
    "\n",
    "        # print(\"epsilon passed: \", eps)\n",
    "\n",
    "        if self.treeType=='ID3':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='C4.5':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "\n",
    "            # for C4.5, we choose the attributes whose Gain are above average\n",
    "            # and then order them by Ratio.\n",
    "\n",
    "            sub_A={key: A[key] for key in attr_list}\n",
    "            attr_list=self.orderByGainOrRatio(D, sub_A, by='Ratio')\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='IGGR':\n",
    "\n",
    "            attr_list=self.orderByIGGR(D, A, 0.4, 0.6, eps)\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "    # tree.train(training_data[:len(training_data)//2], 1)\n",
    "    def train(self, D, depth):\n",
    "        self.tree = self.fit(D, self.A, depth)\n",
    "\n",
    "    def fit(self, D, A, depth):\n",
    "\n",
    "        '''\n",
    "        Train the tree.\n",
    "        To save the training result:\n",
    "        >> self.tree=self.fit(D, A)\n",
    "        D: the training set, a size [m, n+1] numpy array (with str type elements), \n",
    "            where m is the number of training data and n is the number of attributes.\n",
    "            The last column of D is the classifications (or labels).\n",
    "        A: the attributes set. It is a dict with its structure being like \n",
    "            {attr_name: [index_in_D_columns, possibleVal_or_threshold], ...}\n",
    "            attr_name: name of the attribute\n",
    "            index_in_D_columns: the corresponding index of the attribute in ndarray D (starting from 0)\n",
    "            possibleVal_or_threshold: \n",
    "                ###################################################\n",
    "                ## This value may not always be available in A   ##\n",
    "                ## it is added after 'chooseAttribute' is called ##\n",
    "                ## And it will be updated after each call        ##\n",
    "                ###################################################\n",
    "                1, if the attribute is discrete, then it is a ndarray containing all possible values \n",
    "                    of this attribute.\n",
    "                2, if the attribute is continuous, then possibleVal_or_threshold is the most recent \n",
    "                    threshold.\n",
    "        '''\n",
    "\n",
    "        # termination conditions \n",
    "\n",
    "        # the training set is empty\n",
    "        if len(D)==0:\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=self.current_depth, isLeaf=True, \\\n",
    "                classification=self.tmp_classification)\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        # only one type of classification is left \n",
    "        if len(np.unique(D[:, -1]))<=1:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=self.current_depth, isLeaf=True, \\\n",
    "                classification=D[0, -1])\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(A)==0 or len(np.unique(D[:, :-1], axis=0))<=1:\n",
    "\n",
    "            count_dict={}\n",
    "\n",
    "            for key in D[:, -1]:\n",
    "\n",
    "                count_dict[key]=count_dict.get(key, 0)+1\n",
    "\n",
    "            most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=self.current_depth, isLeaf=True, classification=most_frequent)\n",
    "\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "        \n",
    "        # stop building at max depth\n",
    "        if self.current_node is not None: \n",
    "            if self.dm <= self.current_depth:\n",
    "\n",
    "                count_dict={}\n",
    "\n",
    "                for key in D[:, -1]:\n",
    "                    count_dict[key]=count_dict.get(key, 0)+1\n",
    "\n",
    "                most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "                node = Node(feature_name='leaf-'+str(self.leaf_count), depth=depth, isLeaf=True, classification=most_frequent)\n",
    "                self.leaf_count+=1\n",
    "\n",
    "                return node\n",
    "\n",
    "\n",
    "        # stop conditions did not apply -> continue building the tree\n",
    "\n",
    "        self.w = 2 / (self.dm - self.current_depth) # not completely correct\n",
    "        self.budget =  self.w * self.eu # (e / w)\n",
    "\n",
    "        count_dict={}\n",
    "\n",
    "        \n",
    "        print(\"current depth: \", self.current_depth)\n",
    "\n",
    "        # the count query usese the Laplace mechanism to add noise to the class count\n",
    "        for key in D[:, -1]:\n",
    "            count_dict[key] = count_dict.get(key, 0)+ 1 + np.random.laplace(scale=(1/self.budget/2))\n",
    "\n",
    "        # print(\"count dictionary: \")\n",
    "        # print(count_dict)\n",
    "        # count query : how many records have each class value? \n",
    "        # {'1': 449, '0': 162}\n",
    "        # {'0': 4, '1': 5}\n",
    "        \n",
    "        most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "        #print(\"most frequent : \")\n",
    "        # count dictionary: \n",
    "        #{'0': 4, '1': 1}\n",
    "        # most frequent : \n",
    "        # 0\n",
    "\n",
    "        self.tmp_classification = most_frequent\n",
    "\n",
    "        # choose target attribute\n",
    "        target_attr=self.chooseAttribute(D, A, self.budget/2)\n",
    "\n",
    "\n",
    "        # print(\"target attr: \", target_attr)\n",
    "\n",
    "\n",
    "        # generate nodes for each possible value of the target attribute if it's discrete\n",
    "        # related information is stored in A[target_attr][1] now, \n",
    "        # since we have called chooseAttribute at least once.\n",
    "        # \"divide the current node to MULTIPLE CHILD NODES according to class labels\n",
    "        # a new node\n",
    "        info=A[target_attr]\n",
    "\n",
    "        # print(\"target attr: \", target_attr, info, self.feature_discrete[target_attr])\n",
    "\n",
    "        if self.feature_discrete[target_attr]:\n",
    "\n",
    "            node = Node(feature_name=target_attr, discrete=True, depth=self.current_depth, isLeaf=False)\n",
    "            self.current_node = node\n",
    "\n",
    "            self.current_depth = node.depth + 1\n",
    "\n",
    "            # generate nodes for each possible value\n",
    "\n",
    "            # print(\"info1: \", info[1]) \n",
    "            # info1:  ['-1' '0' '10' '11' '12' '13' '2' '3' '4' '5' '6' '7' '9']\n",
    "\n",
    "            for possibleVal in info[1]:\n",
    "\n",
    "                keys=set(A.keys()).difference({target_attr})\n",
    "                # connect node to its child\n",
    "                tmp_D=D[np.argwhere(D[:, info[0]]==possibleVal), :]\n",
    "                \n",
    "                tmp_A={key: A[key] for key in keys}\n",
    "                # tmp A:  {'Age': [0, 25.5]} \n",
    "                # tmp A:  {'Occupation': [2, array(['-1', '0', '10', '11', '12', '13', '2', '3', '4', '5', '6', '7',\n",
    "                # '9'], dtype='<U21')], 'Age': [0, 29.5]}\n",
    "\n",
    "                # print(\"keys: \")  {'Occupation', 'Age', 'Education'}, {'Occupation', 'Age'}\n",
    "                # print(\"tmp_D: \")\n",
    "                # [[['65' '0' '11' '0' '1']]\n",
    "                # [['32' '0' '2' '0' '1']]\n",
    "                # [['24' '0' '6' '0' '1']]]\n",
    "                \n",
    "                node[possibleVal] = self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), tmp_A, self.current_depth)\n",
    "        \n",
    "        else:\n",
    "            # generate two nodes for the two classification if it's continuous\n",
    "            # continuous\n",
    "            threshold=info[1]\n",
    "            node=Node(feature_name=target_attr, discrete=False, threshold=threshold, depth=self.current_depth, isLeaf=False)\n",
    "            self.current_node = node\n",
    "\n",
    "            self.current_depth = node.depth + 1\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]<=str(threshold)), :]\n",
    "            node['<=']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A, self.current_depth)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]>str(threshold)), :]\n",
    "            node['>']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A, self.current_depth)\n",
    "            \n",
    "        \n",
    "        return node\n",
    "\n",
    "    def prune(self, training_D, testing_D):\n",
    "        self.post_prune(training_D, testing_D, self.A, current=self.tree)\n",
    "\n",
    "    def post_prune(self, training_D, testing_D, A, current=None, parent=None):\n",
    "    \n",
    "        '''\n",
    "        self.tree is required.\n",
    "        This method conducts the post-pruning to enhance the model performance.\n",
    "        To make sure this method will work, set \n",
    "        >> current=self.tree\n",
    "        when you call it.\n",
    "        '''\n",
    "\n",
    "        self.current_accuracy=self.evaluate(testing_D, A)\n",
    "\n",
    "        count_dict={}\n",
    "        if len(training_D)==0:\n",
    "            return \n",
    "\n",
    "        for key in training_D[:, -1]:\n",
    "            count_dict[key] = count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(training_D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "        leaf_parent=True\n",
    "        for key, node in current.map.items():\n",
    "            if not node.isLeaf:\n",
    "                leaf_parent=False\n",
    "\n",
    "                # Recursion, DFS\n",
    "                if node.discrete:\n",
    "                    tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]==key), :]\n",
    "                else:\n",
    "                    if key=='<=':\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]<=str(node.threshold)), :]\n",
    "                    else:\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]>str(node.threshold)), :]\n",
    "                self.post_prune(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), testing_D, A, parent=current, current=node)\n",
    "        \n",
    "        tmp_node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, classification=most_frequent)\n",
    "        \n",
    "        if parent:\n",
    "            # when current node is not the root\n",
    "            for key, node in parent.map.items():\n",
    "                if node==current:\n",
    "                    parent.map[key]=tmp_node\n",
    "                    saved_key=key\n",
    "                    break\n",
    "\n",
    "            # compare the evaluation, if it is enhanced then prune the tree.\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                parent.map[saved_key]=current\n",
    "                \n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "        else:\n",
    "\n",
    "            # when current node is the root\n",
    "            saved_tree=self.tree\n",
    "            self.tree=tmp_node\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                self.tree=saved_tree\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "\n",
    "    def pred(self, D):\n",
    "        return self.predict(D, self.A)\n",
    "    \n",
    "    def eval(self, D):\n",
    "        return self.evaluate(D, self.A)\n",
    "\n",
    "    def predict(self, D, A):\n",
    "        '''\n",
    "        Predict the classification for the data in D.\n",
    "        For the definition of A, see method 'fit'. \n",
    "        There is one critical difference between D and that defined in 'fit':\n",
    "            the last column may or may not be the labels. \n",
    "            This method works as long as the feature index in A matches the corresponding\n",
    "            column in D.\n",
    "        '''\n",
    "\n",
    "        # why the loop can not be dismissed?\n",
    "        row, _= D.shape # for the entire testing data \n",
    "        pred = np.empty((row, 1), dtype=str)\n",
    "        tmp_data={key: None for key in A.keys()}\n",
    "\n",
    "        # print(\"the tree is a: \", type(self.tree))\n",
    "        # the tree is a:  <class '__main__.Node'>\n",
    "\n",
    "        for i in range(len(D)): # only 1 row?\n",
    "            for key, info in A.items():\n",
    "\n",
    "                tmp_data[key] = D[i, info[0]]\n",
    "\n",
    "            #print(\"data given to tree: \")\n",
    "            #print(tmp_data)\n",
    "            # {'Age': '20', 'Education': '8', 'Occupation': '4', 'Gender': '1'}\n",
    "\n",
    "            # but self.tree is evidently initialized as none?\n",
    "\n",
    "            pred[i] = self.tree(tmp_data)\n",
    "        \n",
    "      \n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, testing_D, A):\n",
    "        '''\n",
    "        Evaluate the performance of decision tree. (Accuracy)\n",
    "        For definition of testing_D and A, see 'predict'.\n",
    "        However, here the testing_D is required to be labelled, that is, its last column \n",
    "        should be labels of the data.\n",
    "        '''\n",
    "        true_label=testing_D[:, -1]\n",
    "        pred_label=self.predict(testing_D, A)\n",
    "        \n",
    "        success_count=0\n",
    "        for i in range(len(true_label)):\n",
    "            if true_label[i]==pred_label[i]:\n",
    "                success_count+=1\n",
    "\n",
    "        return success_count/len(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diris.astype('<U21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2[400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['73', '9', '9', '1', '1'],\n",
       "       ['31', '9', '9', '1', '0'],\n",
       "       ['23', '15', '11', '0', '1'],\n",
       "       ...,\n",
       "       ['47', '11', '2', '1', '1'],\n",
       "       ['29', '9', '11', '1', '1'],\n",
       "       ['48', '11', '13', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = random.sample(range(0, 1216), 1170)\n",
    "D2[400:][l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype='<U21')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(D2[400:][:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['57', '15', '10', '1', '1'],\n",
       "       ['37', '10', '9', '1', '0'],\n",
       "       ['37', '7', '3', '0', '1'],\n",
       "       ...,\n",
       "       ['38', '9', '11', '1', '0'],\n",
       "       ['41', '12', '9', '0', '1'],\n",
       "       ['23', '15', '5', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original domain length of categorical attribute 0: 64\n",
      "original domain length of categorical attribute 1: 14\n",
      "original domain length of categorical attribute 2: 13\n",
      "original domain length of categorical attribute 3: 2\n",
      "NUM TREES = 4 & EPSILON PER TREE = 0.05\n",
      "root attributes\n",
      "[0, 1, 2, 3]\n",
      "what is root attributes: \n",
      "[1, 2, 3]\n",
      "current depth:  0\n",
      "vl:  [0.030879681919627488, 0.03943501261927678, 0.051242725884337356, 0.059289316766729216, 0.06906514316881963, 0.07620568516611481, 0.08274583963986824, 0.09205371695610096, 0.07478318600025574, 0.08625768496252735, 0.09435098438088649, 0.09662790022456817, 0.08864060922268426, 0.06840492745308649, 0.05877076613611384, 0.04753633249975632, 0.04335220690681583, 0.037815759388609343, 0.037798760625690826, 0.0296399948185853, 0.024646557698311252, 0.019854819339581047, 0.018448363519704605, 0.02016126177072656, 0.017701700956790363, 0.016028328057380695, 0.0132378321075447, 0.014255083313911746, 0.014705464732858733, 0.011398735486546938, 0.008826702428945427, 0.0037953203556966276, 0.004837420395501617, 0.001994485171360584, 0.004028129681306478, 0.0033647167786069582, 0.001047702317523079, 0.0003209514910577481, 5.588790119725448e-05, 0.0002095054773590116, 0.0006871444339280828, 4.0170330187823056e-07, 1.443938573727015e-05, 8.275774435411229e-06, 2.1226859032040867e-05, 0.00019085985525912818, 0.0006479099213972186, 0.0023282393142426525, 0.004971601592619191, 0.03598353825475495, 0.033078396232502486, 0.030879681919627488, 0.02965392149783031, 0.028300867946652705, 0.026757080958062073, 0.02488924365378155, 0.02231118478303431, 0.057471832294025284, 0.05694119508777039, 0.0385414205476284]\n",
      "current depth:  1\n",
      "vl:  [0.0035198969843555743, 0.005324390140207013, 0.008517681752187216, 0.011223573416760417, 0.015273746948293965, 0.018960494892492293, 0.023111045272675632, 0.030991937932601493, 0.00017747689804055485, 0.0027545816641791963, 0.009894606454673658, 0.020245968859226277, 0.02837500170015306, 0.0012594786290051631]\n",
      "current depth:  2\n",
      "vl:  [0.04603081823964156, 0.00381360075729397, 0.0006959241504366129, 0.04124634444084425, 0.05954528143392814, 0.008536086776932987]\n",
      "current depth:  3\n",
      "vl:  [0.08438361916666769, 0.14946827370263738, 0.04902649676234745, 0.03542818005814553]\n",
      "current depth:  4\n",
      "vl:  [0.31127812445913283]\n",
      "current depth:  5\n",
      "vl:  [0.7149464321932129, 0.2650621922278978]\n",
      "current depth:  6\n",
      "vl:  [0.5141474411982839, 0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  7\n",
      "vl:  [0.010734768729084809, 0.001401359211548798, 0.0008549363905300818, 9.017586007716538e-05, 9.635751501388946e-05, 0.0001351354748299566, 0.0005366908286921249, 9.204247056883071e-05, 4.919252253990104e-05, 7.3408939715780014e-06, 5.218775597686166e-06, 0.0002825483698108367, 0.0002392992408659232, 0.0002854151812644228, 7.709236139296997e-05, 0.00032187445047754626, 0.0006865173154534817, 0.0002248642226898494, 3.7169108674268006e-05, 0.000619676500501682, 0.00015030392254740301, 0.0011700670980981365, 9.979669243331177e-05, 0.00016861026074666767, 0.001390863378309639, 0.002395060101443749, 0.003226292477481012, 0.0022302206388378223, 0.0011402919246601229, 0.003230660792478765, 0.002492076839951506, 0.0030866328063772043, 0.003088305327429188, 0.0040481635013628415, 0.005479616438390019, 0.009240521601106576, 0.013647398110743406, 0.0535117969023159, 0.04874376680938845, 0.04518769879457573, 0.04322632060505464, 0.04107970319727106, 0.0386550632983844, 0.035757723403438274, 0.03182508717585144, 0.06625994613382209, 0.05640900832699647, 0.037681994354507635]\n",
      "current depth:  8\n",
      "vl:  [0.34023563896561493, 0.027152778399104325, 0.03887502105131293, 0.05087149367984439, 0.07838991168482612, 0.09500500259896874, 0.11447914209644945, 0.13792538097003, 0.0016359318241285707, 0.00687524507973983, 0.016908179203691677, 0.03462284214290566, 0.06750483525400915, 0.03887502105131293, 0.027152778399104325, 0.03887502105131293]\n",
      "current depth:  9\n",
      "vl:  [0.19612159738166063, 0.01198952204774404, 0.01698263015444497, 0.027395433230846245, 0.03330267503958374, 0.03993345046788936, 0.047527026164452785, 0.056410690158848795, 0.06706251861118169, 0.08022387742249984, 0.09712387202482009, 0.11997846065653582, 0.15326570719831423, 0.31755009778351, 0.7291027835589593, 0.027395433230846245]\n",
      "current depth:  10\n",
      "vl:  [0.05052570591040738, 0.014832984025098094, 0.018267793265292324, 0.004596077390556937, 0.010195172230317703, 0.006609718715428308, 0.011919850687677171, 0.0026374108365939883, 0.0011225297690564834, 0.0023954163174116153, 0.0012990708384834995, 9.617267234366695e-05, 1.6955023128192103e-05, 0.0, 0.0010502470617058792, 0.00012571873375754933, 7.078596147207369e-05, 1.5328335270061705e-07, 9.406258256205018e-05, 0.0006449160701062308, 0.0017284664983540378, 0.002510741989324778, 0.00024457717703982675, 0.0005910360419637994, 0.00019131317507558256, 0.0006019234042447072, 0.0006416938210098005, 0.0001440214840542883, 0.006446366751179467, 0.014832984025098094, 0.011383235217817263, 0.004688221692059634, 0.001822364027438434, 7.609521540528405e-05, 0.042016104141699956, 0.03764571226490886, 0.03216897434440206, 0.010049501688486695]\n",
      "current depth:  11\n",
      "vl:  [0.03887502105131293, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  12\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  13\n",
      "vl:  [0.06815929623785226, 0.08657716662093085, 0.00016488511098049232, 0.004657353816991324, 0.04599596008291421, 0.09938585979410257, 0.07325321053265862, 0.05189844865589339, 0.1738094976639552, 0.14886703214742603, 0.1263787220271058, 0.10578599083774445, 0.08657716662093085, 0.06815929623785226, 0.049421802558923394, 0.015140683931871676]\n",
      "current depth:  14\n",
      "vl:  [0.1419708451631329, 0.19118345999979763, 0.01637917007713985, 0.016111606370189935, 0.0013517947353627185, 0.01637917007713985, 0.0005875744549013991, 0.014590558552361604, 0.027464412176933718]\n",
      "current depth:  15\n",
      "vl:  [0.19655090395693994, 0.003548762540498691, 0.05198705216732828, 0.07454094323250514, 0.018103770678494495, 0.0017006663354044642, 0.003548762540498691]\n",
      "current depth:  16\n",
      "vl:  [0.006271075603255408, 0.02033163327920574, 0.04727298257039715, 0.006271075603255408, 0.00911437305383058, 0.02033163327920574]\n",
      "current depth:  17\n",
      "vl:  [0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  18\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  19\n",
      "vl:  [0.35472437774145976]\n",
      "current depth:  20\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  21\n",
      "vl:  [0.04228053312209247, 0.06062882370722798, 0.07948182147590495, 0.12304146538516741, 0.1495705622172283, 0.18090300973893061, 0.21899525866266328, 0.2669934813934353, 0.060411360676619764, 0.09694558259460298, 0.026225073521217532, 0.08045249824505055, 0.04228053312209247, 0.060411360676619764]\n",
      "current depth:  22\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845, 0.00654650269402557, 0.18112206671815106, 0.00654650269402557]\n",
      "current depth:  23\n",
      "vl:  [0.1444372797703026, 0.0, 0.04646521764489747, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  24\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652, 0.02033163327920574]\n",
      "current depth:  25\n",
      "vl:  [0.0, 0.35472437774145976, 0.0]\n",
      "current depth:  26\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  27\n",
      "vl:  [1.0]\n",
      "current depth:  28\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  29\n",
      "vl:  [1.0]\n",
      "current depth:  30\n",
      "vl:  [0.0779818028030355, 0.048882064661275385, 0.0054224909598283594, 0.04963026762482462, 0.016371948435554104, 0.0011194760755969455, 0.0054224909598283594, 0.0779818028030355]\n",
      "current depth:  31\n",
      "vl:  [0.3249617925104421, 0.03972587535899602, 0.09291597794545213, 0.034851554559677256, 0.005906304043889955, 0.0017484967921683212, 0.0920686615848161]\n",
      "current depth:  32\n",
      "vl:  [0.06857481897208437, 0.026079279777968374, 0.002579146528037616, 0.002579146528037616, 0.026079279777968374, 0.06857481897208437]\n",
      "current depth:  33\n",
      "vl:  [0.08405645018009537, 0.01616219857701356, 0.0, 0.01616219857701356, 0.08428582039971266]\n",
      "current depth:  34\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  35\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  36\n",
      "vl:  [0.020852561843756504, 0.0009261497914662364, 0.009817027949690466, 0.003453258331108513, 0.020187920786114054, 0.0013487739431090884, 0.006016209808195638, 0.003804840490958069, 0.010853525209190388, 2.8751596427134063e-05, 0.002472881111753963, 0.00015347767280390667, 0.0015816158282129559, 0.0009261497914662364, 0.020852561843756504, 0.058104949988288565, 0.058104949988288565]\n",
      "current depth:  37\n",
      "vl:  [0.018743401896210654, 0.00044811094733179964, 0.012443846027352644, 0.005502431564327079, 0.026099571722987002, 0.003089098987912278, 0.009772854760729227, 0.007808096164386923, 0.018231740147425648, 0.0012232601991285462, 0.007186819176508965, 0.0007734502525808069, 0.00044811094733179964, 0.018743401896210654, 0.061660370264667315]\n",
      "current depth:  38\n",
      "vl:  [0.016580059332658315, 0.00012247416370816166, 0.015686028838327846, 0.008335896863428343, 0.03363296539530409, 0.0059039478617227996, 0.015135412345049164, 0.014268391478479345, 0.02941126601351029, 0.004950792111860722, 0.01628490817238772, 0.0063508161357078595, 0.016580059332658315, 0.19660295645039516]\n",
      "current depth:  39\n",
      "vl:  [0.02335725413439119, 0.0012737284368484216, 0.009902426765893943, 0.0032440084023068897, 0.021826733569586903, 0.0010655309018225891, 0.006077489978553174, 0.0037062770092080467, 0.012501267428849779, 5.560390497059565e-05, 0.002263907666084008, 0.001643705962615162, 0.05955982280780615]\n",
      "current depth:  40\n",
      "vl:  [0.020820607572057514, 0.0006239138921804833, 0.013058192040234536, 0.005707586917505622, 0.029570912871580948, 0.0031343154353240367, 0.011002039255494372, 0.009318758799868187, 0.02403547481390802, 0.0011239881119591664, 0.009990344389723916, 0.0006239138921804833]\n",
      "current depth:  41\n",
      "vl:  [0.10720059946682817, 0.03487631451464511, 0.0, 0.08975463992235706]\n",
      "current depth:  42\n",
      "vl:  [0.03887502105131293, 0.09500500259896874, 0.06387040410564794]\n",
      "current depth:  43\n",
      "vl:  [0.1740193478031239]\n",
      "current depth:  44\n",
      "vl:  [0.21961295293160082, 0.009877066173124747, 0.0004609563850202613, 0.006918065391871371, 0.006918065391871371, 0.0004609563850202613, 0.002851106566676797]\n",
      "current depth:  45\n",
      "vl:  [0.09516074617477319, 0.012198670872602811, 0.0433033446503569, 0.0, 0.010372390556268395, 0.0]\n",
      "current depth:  46\n",
      "vl:  [0.0012009492048155508, 0.024503372401150855, 0.0034428499155211363, 0.0034428499155211363, 0.0012009492048155508]\n",
      "current depth:  47\n",
      "vl:  [0.1396618834407261]\n",
      "current depth:  48\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845]\n",
      "current depth:  49\n",
      "vl:  [0.1444372797703026, 0.0]\n",
      "current depth:  50\n",
      "vl:  [0.02033163327920574]\n",
      "current depth:  51\n",
      "vl:  [0.1331347651277193, 0.0007215434706190365, 0.027220540954955064, 0.028041082636984186, 0.003692285358543064, 0.03545285774285796, 0.006820417506904309, 0.01616747901883308, 0.0007215434706190365, 0.1620598823995545, 0.003692285358543064]\n",
      "current depth:  52\n",
      "vl:  [0.12223259425191503, 0.003511835147933055, 0.04996085093256333, 0.06105378373381032, 0.02042632416598845, 0.09371621719616725, 0.042873746809138226, 0.18112206671815106, 0.12223259425191503, 0.0]\n",
      "current depth:  53\n",
      "vl:  [0.1642830436827974, 0.0, 0.020976263380628444, 0.020976263380628444, 0.0, 0.030904096500060884, 0.0, 0.0]\n",
      "current depth:  54\n",
      "vl:  [0.14993460792792548, 0.07454094323250514, 0.05198705216732828, 0.003548762540498691, 0.05488014986908448, 0.0017006663354044642, 0.007260555362580846]\n",
      "current depth:  55\n",
      "vl:  [0.03766823861199383, 0.02904940554533142, 0.0, 0.03766823861199383, 0.0, 0.0]\n",
      "current depth:  56\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  57\n",
      "vl:  [0.00654650269402557, 0.02042632416598845, 0.02042632416598845, 0.00654650269402557, 0.00654650269402557]\n",
      "current depth:  58\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  59\n",
      "vl:  [0.35472437774145976, 0.0]\n",
      "current depth:  60\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  61\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  62\n",
      "vl:  [1.0]\n",
      "current depth:  63\n",
      "vl:  [0.009463160379606159, 0.03197309653622524, 0.0, 0.006487271342959594, 0.0, 0.009463160379606159, 0.0, 0.019264840827561348, 0.08537890326205116, 0.006487271342959594]\n",
      "current depth:  64\n",
      "vl:  [0.01637917007713985, 0.04706157416498513, 0.0013517947353627185, 0.016111606370189935, 0.0033062079013812117, 0.0005875744549013991, 0.014590558552361604, 0.2286431947829483, 0.016111606370189935]\n",
      "current depth:  65\n",
      "vl:  [0.005502431564327079, 0.026099571722987002, 0.0019104704348305221, 0.0019104704348305221, 0.005502431564327079, 0.12361826349058296, 0.08282250195514274, 0.060128238249178026]\n",
      "current depth:  66\n",
      "vl:  [0.024503372401150855, 0.07231627283656572, 0.0034428499155211363, 0.03389505987592521, 0.015976004572494196, 0.07231627283656572]\n",
      "current depth:  67\n",
      "vl:  [0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  68\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  69\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  70\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  71\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  72\n",
      "vl:  [0.3311625470900562, 0.20914596325234783, 0.10803154614560007, 0.08039995788957639, 0.0592555474406796, 0.04203498942904823, 0.026975228230812633, 0.08039995788957639]\n",
      "current depth:  73\n",
      "vl:  [1.0]\n",
      "current depth:  74\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  75\n",
      "vl:  [0.007157449141035996, 0.09246429045939385, 0.0027870034023649063, 0.004959879357329997, 0.006088468344589545, 0.0007449302002757883, 0.011163022344172942, 0.06602388591355077, 0.05066277424892794, 0.013345544354931434, 0.0032826780163701464, 0.02358490948403256, 0.008506855978897658, 0.004309936428344649, 0.03545446166785676, 0.0007449302002757883]\n",
      "current depth:  76\n",
      "vl:  [0.004525530074339163, 0.14069233542150236, 0.0017223852227089363, 0.022944064086203204, 0.005256999442785119, 0.022763588734758467, 0.09071860255200705, 0.0663214281567171, 0.019694198438098005, 0.006239087059719598, 0.030743786818436213, 0.012373363424652308, 0.002686284993320699, 0.03142613766226453, 3.370468062445997e-05]\n",
      "current depth:  77\n",
      "vl:  [0.03659373782715106, 0.09336196680283423, 0.00539351844742539, 0.00018831520214906817, 0.010793067371245064, 0.07270431652766694, 0.053224353653063716, 0.012871483030632947, 0.0027294766596929836, 0.0234792444112598, 0.007963060143941176, 0.00539351844742539, 0.039413567611848936, 0.0014103879650374585]\n",
      "current depth:  78\n",
      "vl:  [0.03756940810593821, 0.034802386592657025, 0.004929846084096145, 0.02572055932891343, 0.10518048868764784, 0.07221749844314218, 0.020083564845238468, 0.005880168258493163, 0.03160529880983776, 0.01220469469276198, 0.003394597420929121, 0.034802386592657025, 0.00017981398492684288]\n",
      "current depth:  79\n",
      "vl:  [0.029340994900501946, 0.0, 0.2650621922278978, 0.1801484152912169, 0.1801484152912169]\n",
      "current depth:  80\n",
      "vl:  [0.0, 0.08170416594551044, 0.2525355747672442]\n",
      "current depth:  81\n",
      "vl:  [0.02033163327920574, 0.02033163327920574]\n",
      "current depth:  82\n",
      "vl:  [1.0]\n",
      "current depth:  83\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  84\n",
      "vl:  [0.07758643025994172, 0.0025672227726755356, 0.0155259523373791, 0.03950085359661396, -1.1102230246251565e-16, 0.006031486952556862, 0.06761836267935474, 0.15471661539411724, -1.1102230246251565e-16]\n",
      "current depth:  85\n",
      "vl:  [0.05043207711856275, 0.042475702157007356, 0.0013364153922302152, 0.0013364153922302152, 0.13942673923528215, 0.10433780877834073, 0.048966862572090125, 0.18380975803906682]\n",
      "current depth:  86\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  87\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  88\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  89\n",
      "vl:  [0.10129345660749313, 0.048966862572090125, 0.042475702157007356, 0.01294045001527316, 0.0013364153922302152, 0.0013364153922302152, 0.042475702157007356, 0.11553480482056264, 0.45485387246188064, 0.45485387246188064]\n",
      "current depth:  90\n",
      "vl:  [0.24712122803354972, 0.026975228230812633, 0.04203498942904823, 0.0592555474406796, 0.08039995788957639, 0.10803154614560007, 0.20914596325234783, 0.3311625470900562, 0.7875982374357124]\n",
      "current depth:  91\n",
      "vl:  [0.04783270126825073, 0.003717540254242189, 0.018720426663081775, 0.0007890371333189823, 0.011374969176105029, 0.006718043051334351, 0.00446441583143226, 0.009329006553674328, 0.006877535687617238, 0.012511297110181077, 0.007551023416123786, 0.003965077922143384, 0.0008018335706630937, 0.006541597429715463, 0.0034221761629009938, 0.0012734078861444157, 0.0005772589897835625, 0.0, 0.0005504338382089635, 0.00010540887821455216, 0.004498491726679301, 0.0015683536713117128, 0.000605874733919088, 0.010079924206169802, 0.007329728996123117, 0.002643261550262635, 0.04162795820420849, 0.03762920453508633, 0.033470046182529876, 0.02898709223878468, 0.023719490062496455, 0.07907199320695064]\n",
      "current depth:  92\n",
      "vl:  [0.031286292512233346, 0.3041622065107638, 0.20088564400085046, 0.1504404455257053, 0.1201043167945054, 0.08474626586718464, 0.06429406990686946, 0.050653561093855534, 0.03666866736525018, 0.03305835661787534, 0.026870720784848874, 0.021701451798451888, 0.01940124826854422, 0.01725116356984072, 0.015226001472835976, 0.013301934507303383, 0.011454011332793133, 0.00965144716122575, 0.007845136117141499, 0.005915714662382942]\n",
      "current depth:  93\n",
      "vl:  [0.08155831841638188, 0.057359722698295576, 0.0023405741912092726, 0.009888860111328203, 0.000297636641670596, 0.0026058239049142557, 0.005967171067418116, 0.0008086158294396622, 0.0028972221235702158, 0.0011727919682250644, 0.0035209830776774718, 0.0008086158294396622, 0.0006021615438858776, 0.0027811610845727966, 0.006823482022831692, 0.0013474455511088856, 0.005039710939941871, 0.0003746232606243009, 0.000297636641670596, 0.003591083049637215, 0.0002291247454691225, 0.0023405741912092726, 0.04490956958029419]\n",
      "current depth:  94\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  95\n",
      "vl:  [1.0]\n",
      "current depth:  96\n",
      "vl:  [0.1857207293893467, 0.006031486952556862, 0.006031486952556862, 0.1074835602147594]\n",
      "current depth:  97\n",
      "vl:  [0.39632815712783614, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  98\n",
      "vl:  [0.1396618834407261, 0.1396618834407261]\n",
      "current depth:  99\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  100\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  101\n",
      "vl:  [0.04880068876220159, 0.08324234418491715, 0.19989968667103028, 0.12923753484913508, 0.08324234418491715, 0.04880068876220159]\n",
      "current depth:  102\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  103\n",
      "vl:  [0.11131292859758138, 0.029340994900501946, 0.07317342651141098, 0.01840970721714974, 0.0, 0.029340994900501946, 0.11131292859758138]\n",
      "current depth:  104\n",
      "vl:  [0.29956720366734785, 0.16341875261776725, 0.04879494069539858, 0.003321436156323081, 0.01790512783062725, 0.13889234683021912]\n",
      "current depth:  105\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474, 0.1074835602147594]\n",
      "current depth:  106\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  107\n",
      "vl:  [0.32770706232704216, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  108\n",
      "vl:  [1.0]\n",
      "current depth:  109\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  110\n",
      "vl:  [1.0]\n",
      "current depth:  111\n",
      "vl:  [0.5678946871213929, 0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.31127812445913283]\n",
      "current depth:  112\n",
      "vl:  [0.18147620125065958, 0.03887502105131293, 0.06387040410564794, 0.13792538097003, 0.20490965566138727, 0.3345383316670124, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  113\n",
      "vl:  [1.0, 1.0]\n",
      "current depth:  114\n",
      "vl:  [0.08231985153261825, 0.013107492497438625, 0.018740337027706402, 0.03068579095793265, 0.03758184782085702, 0.054578998670749966, 0.06550784501721568, 0.03758184782085702, 0.03068579095793265, 0.024484933210301835, 0.018740337027706402, 0.013107492497438625, 0.018740337027706402]\n",
      "current depth:  115\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  116\n",
      "vl:  [0.36872135900975694, 0.03887502105131293, 0.06387040410564794, 0.20490965566138727, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  117\n",
      "vl:  [0.21142802859192256, 0.035892627884231906, 0.026501087386621498, 0.0011314390428371153, 0.0004750838548505314, 0.01654986256698211, 0.08216086067917011, 0.05572581616707231, 0.034583896956835725, 0.006179770723736218, 0.12866687041864272, 0.10410696679251233, 0.08096483286568819, 0.05784506765656581, 0.2137542834718897]\n",
      "current depth:  118\n",
      "vl:  [0.09862060778170113, 0.020246622059563374, 0.030413087345934543, 0.05418666832673801, 0.06952093867273432, 0.08880563947945669, 0.06952093867273432, 0.05418666832673801, 0.04144982886425281, 0.030413087345934543, 0.020246622059563374]\n",
      "current depth:  119\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  120\n",
      "vl:  [1.0]\n",
      "current depth:  121\n",
      "vl:  [0.6718700105021306, 0.18112206671815106, 0.02042632416598845, 0.12923753484913508, 0.00654650269402557, 0.28015270043979906]\n",
      "current depth:  122\n",
      "vl:  [0.22814608396437117, 0.014638089798646399, 0.03758891464919054, 0.001894368099625091, 0.01299810100075221, 0.032727248369402676, 0.10015300584040142, 0.07645364175044272, 0.02600866900982578, 0.06500303062583873, 0.01685932408212934, 0.010008745144475025, 0.19212097886551394, 0.0009799994598076586]\n",
      "current depth:  123\n",
      "vl:  [1.0]\n",
      "current depth:  124\n",
      "vl:  [0.2650621922278978, 0.9673183336217959, 0.2650621922278978]\n",
      "current depth:  125\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442, 0.2525355747672442]\n",
      "current depth:  126\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  127\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  128\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  129\n",
      "vl:  [0.08914841432492489, 4.321439369549142e-05, 0.0052414724546126605, 0.010068132866203379, 0.0001293166900281584, 0.0015085020979276843, 0.003925355357878909, 0.0014125866706859592, 0.00025593788194577913, 0.0014790283745211043, 0.006637729346751337, 0.0038081716466179883, 1.025205919724182e-06, 0.0014790283745211043, 0.00025593788194577913, 0.0014122435529148607, 0.011894346701497116, 0.007436095461039109, 0.004140697484148831, 0.0001293166900281584, 0.005454538047930771, 0.001886217819233869, 0.0021637833739358665, 0.007713557129005132, 0.0026194696426508008, 7.893321778618192e-05, 0.0017329763713131317, 0.011036169182915903, 0.03679284857620948, 0.022238263140437243, 0.07973348687787753, 0.051195445668940964]\n",
      "current depth:  130\n",
      "vl:  [0.4189939410656296, 0.04646521764489747, 0.0, 0.2650621922278978, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  131\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.1740193478031239, 0.08975463992235706, 0.08975463992235706]\n",
      "current depth:  132\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  133\n",
      "vl:  [0.35472437774145976, 1.0, 0.35472437774145976, 0.35472437774145976]\n",
      "current depth:  134\n",
      "vl:  [0.14047499629754506, 0.0010659650610598374, 0.04320100006041985, 0.03578634272103474, 0.003691010892848074, 0.003691010892848074, 0.03578634272103474, 0.11850307846721093, 0.04320100006041985, 0.0010659650610598374, 0.14047499629754506, 0.017059384364149003]\n",
      "current depth:  135\n",
      "vl:  [0.1642830436827974, 0.2525355747672442, 0.09826454934165416, 0.020976263380628444, 0.0, 0.020976263380628444, 0.09826454934165416, 0.030904096500060884, 0.0, 0.1642830436827974, 0.030904096500060884]\n",
      "current depth:  136\n",
      "vl:  [0.00911437305383058, 0.006271075603255408, 0.04727298257039715, 0.12451124978365313, 0.26102902170468606, 0.09865444076641222, 0.00911437305383058, 0.13263705581879656, 0.21045859350620652]\n",
      "current depth:  137\n",
      "vl:  [0.3335952857724869, 0.19087450462110933, 0.11499831118948668, 0.06405612194077612, 0.06405612194077612]\n",
      "current depth:  138\n",
      "vl:  [1.0]\n",
      "current depth:  139\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.9245112497836533]\n",
      "current depth:  140\n",
      "vl:  [0.08728792846171006, 0.06500303062583873, 0.006704446094201026, 0.0394158831863924, 0.10015300584040142, 0.0612181055938347, 0.01299810100075221, 0.001894368099625091, 0.03758891464919054, 0.15817614246080478, 0.12256503176022776, 0.08728792846171006, 0.27396906204000565]\n",
      "current depth:  141\n",
      "vl:  [0.06923999653869647, 0.14077739888486904, 0.04424017696155053, 0.1371676772027261, 0.09363914620504095, 0.03398864924743632, 0.014603213499233988, 0.12876564130825247, 0.09865100594634758, 0.06923999653869647]\n",
      "current depth:  142\n",
      "vl:  [0.39632815712783614]\n",
      "current depth:  143\n",
      "vl:  [0.06405612194077612, 0.04900704072750519, 0.018139365349190032, 0.0, 0.004126846387264912, 0.08759794173047272, 0.06405612194077612, 0.04252438688979406]\n",
      "current depth:  144\n",
      "vl:  [0.11008497243260636, 0.026079279777968374, 0.002579146528037616, 0.026079279777968374, 0.09693457468543067]\n",
      "current depth:  145\n",
      "vl:  [0.4326479595302938, 0.06761836267935474, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  146\n",
      "vl:  [0.06405612194077612, 0.19087450462110933, 0.3335952857724869]\n",
      "current depth:  147\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  148\n",
      "vl:  [0.1215042344189909, 0.012878775802370554, 0.005566002752151681, 0.0218531497079759, 0.015092388500114292, 0.002090880863272472, 0.01169274068482661, 0.0282716093567718, 0.008732096099167024, 0.000796980166438043, 0.024729996871962125, 0.008357772647312495, 0.027347471568835758, 0.009230400966805932, 0.0003353562483830987, 0.005566002752151681, 0.03914234439236314, 0.012878775802370554, 0.10905688765012257, 0.25832327657166604]\n",
      "current depth:  149\n",
      "vl:  [0.09424586253166718, 0.004288014491355668, 0.0069103049299918794, 0.010164948277620982, 0.0005614141439900361, 0.014503881500296746, 0.006175390197816655, 0.00020634253667313202, 0.012146214614021857, 0.002059001680907775, 0.023172957667532402, 0.007238121724252496, 8.543901623825849e-05, 0.0069103049299918794, 0.04450434928089019, 0.004288014491355668, 0.16129894007328135]\n",
      "current depth:  150\n",
      "vl:  [0.0874531077292179, 0.007657454841850576, 0.012517685605470192, 0.018696436805359134, 0.003629117727040704, 0.02718759723451945, 0.018385120552399863, 0.005278584526202198, 0.03411003466668784, 0.014943592248227559, 0.0668011715287747, 0.03881742693898308, 0.016228133983856783, 0.0016594679819640055, 0.007657454841850576, 0.0874531077292179]\n",
      "current depth:  151\n",
      "vl:  [0.1720827023145395, 0.051256867002420284, 0.0450029868398238, 0.01378090580310471, 0.04930445582025929, 0.03030514483932223, 0.01104889222972839, 0.04727298257039715, 0.02255184928323372, 0.08208161086595903, 0.04885904963406792, 0.021834240393222914, 0.0033227607981208018, 0.0055077079623934285, 0.09507636686268137]\n",
      "current depth:  152\n",
      "vl:  [0.010008745144475025, 0.01685932408212934, 0.0012297505299947705, 0.02600866900982578, 0.01617703842307018, 0.003447287770155632, 0.032727248369402676, 0.01299810100075221, 0.06709948575852374, 0.03758891464919054, 0.014638089798646399, 0.0009799994598076586, 0.010008745144475025, 0.08728792846171006]\n",
      "current depth:  153\n",
      "vl:  [0.0072953990280224335, 0.012471147310100805, 0.00015877214942946945, 0.019538390282887227, 0.00939281786187296, 0.0004714108476760307, 0.02142638837195634, 0.004987053402159863, 0.04902012774520869, 0.020747109935735, 0.0025190298819811018, 0.0072953990280224335, 0.18577740590888553]\n",
      "current depth:  154\n",
      "vl:  [0.013561092612075002, 0.023720771016214483, 0.0031344330674255745, 0.03822469823189057, 0.030247485498529632, 0.010257637084757874, 0.06665019121715889, 0.03548936959226058, 0.20366244598014902, 0.1619498622418021, 0.12377240478608444, 0.08669202239196727]\n",
      "current depth:  155\n",
      "vl:  [0.0010659650610598374, 0.0020461667745987904, 0.007067259725524144, 0.003691010892848074, 0.0020461667745987904, 0.04320100006041985, 0.0010659650610598374, 0.1764059496755188]\n",
      "current depth:  156\n",
      "vl:  [0.00539351844742539, 0.010793067371245064, 0.000596864170486781, 0.02072083962390814, 0.007963060143941176, 0.00539351844742539, 0.1270419185712244]\n",
      "current depth:  157\n",
      "vl:  [0.0017006663354044642, 0.003548762540498691, 0.007260555362580846, 0.007260555362580846, 0.0017006663354044642, 0.19655090395693994]\n",
      "current depth:  158\n",
      "vl:  [0.00911437305383058, 0.02033163327920574, 0.0, 0.04727298257039715, 0.13263705581879656]\n",
      "current depth:  159\n",
      "vl:  [0.003040199406150453, 0.0072535960017724105, 0.0072535960017724105, 0.019288288480290336]\n",
      "current depth:  160\n",
      "vl:  [0.0, 0.0, 0.2525355747672442]\n",
      "current depth:  161\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  162\n",
      "vl:  [0.0]\n",
      "currently pruning! \n",
      "what is root attributes: \n",
      "[1, 3]\n",
      "current depth:  0\n",
      "vl:  [0.030879681919627488, 0.03943501261927678, 0.051242725884337356, 0.059289316766729216, 0.06906514316881963, 0.07620568516611481, 0.08274583963986824, 0.09205371695610096, 0.07478318600025574, 0.08625768496252735, 0.09435098438088649, 0.09662790022456817, 0.08864060922268426, 0.06840492745308649, 0.05877076613611384, 0.04753633249975632, 0.04335220690681583, 0.037815759388609343, 0.037798760625690826, 0.0296399948185853, 0.024646557698311252, 0.019854819339581047, 0.018448363519704605, 0.02016126177072656, 0.017701700956790363, 0.016028328057380695, 0.0132378321075447, 0.014255083313911746, 0.014705464732858733, 0.011398735486546938, 0.008826702428945427, 0.0037953203556966276, 0.004837420395501617, 0.001994485171360584, 0.004028129681306478, 0.0033647167786069582, 0.001047702317523079, 0.0003209514910577481, 5.588790119725448e-05, 0.0002095054773590116, 0.0006871444339280828, 4.0170330187823056e-07, 1.443938573727015e-05, 8.275774435411229e-06, 2.1226859032040867e-05, 0.00019085985525912818, 0.0006479099213972186, 0.0023282393142426525, 0.004971601592619191, 0.03598353825475495, 0.033078396232502486, 0.030879681919627488, 0.02965392149783031, 0.028300867946652705, 0.026757080958062073, 0.02488924365378155, 0.02231118478303431, 0.057471832294025284, 0.05694119508777039, 0.0385414205476284]\n",
      "current depth:  1\n",
      "vl:  [0.0035198969843555743, 0.005324390140207013, 0.008517681752187216, 0.011223573416760417, 0.015273746948293965, 0.018960494892492293, 0.023111045272675632, 0.030991937932601493, 0.00017747689804055485, 0.0027545816641791963, 0.009894606454673658, 0.020245968859226277, 0.02837500170015306, 0.0012594786290051631]\n",
      "current depth:  2\n",
      "vl:  [0.04603081823964156, 0.00381360075729397, 0.0006959241504366129, 0.04124634444084425, 0.05954528143392814, 0.008536086776932987]\n",
      "current depth:  3\n",
      "vl:  [0.08438361916666769, 0.14946827370263738, 0.04902649676234745, 0.03542818005814553]\n",
      "current depth:  4\n",
      "vl:  [0.31127812445913283]\n",
      "current depth:  5\n",
      "vl:  [0.7149464321932129, 0.2650621922278978]\n",
      "current depth:  6\n",
      "vl:  [0.5141474411982839, 0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  7\n",
      "vl:  [0.010734768729084809, 0.001401359211548798, 0.0008549363905300818, 9.017586007716538e-05, 9.635751501388946e-05, 0.0001351354748299566, 0.0005366908286921249, 9.204247056883071e-05, 4.919252253990104e-05, 7.3408939715780014e-06, 5.218775597686166e-06, 0.0002825483698108367, 0.0002392992408659232, 0.0002854151812644228, 7.709236139296997e-05, 0.00032187445047754626, 0.0006865173154534817, 0.0002248642226898494, 3.7169108674268006e-05, 0.000619676500501682, 0.00015030392254740301, 0.0011700670980981365, 9.979669243331177e-05, 0.00016861026074666767, 0.001390863378309639, 0.002395060101443749, 0.003226292477481012, 0.0022302206388378223, 0.0011402919246601229, 0.003230660792478765, 0.002492076839951506, 0.0030866328063772043, 0.003088305327429188, 0.0040481635013628415, 0.005479616438390019, 0.009240521601106576, 0.013647398110743406, 0.0535117969023159, 0.04874376680938845, 0.04518769879457573, 0.04322632060505464, 0.04107970319727106, 0.0386550632983844, 0.035757723403438274, 0.03182508717585144, 0.06625994613382209, 0.05640900832699647, 0.037681994354507635]\n",
      "current depth:  8\n",
      "vl:  [0.34023563896561493, 0.027152778399104325, 0.03887502105131293, 0.05087149367984439, 0.07838991168482612, 0.09500500259896874, 0.11447914209644945, 0.13792538097003, 0.0016359318241285707, 0.00687524507973983, 0.016908179203691677, 0.03462284214290566, 0.06750483525400915, 0.03887502105131293, 0.027152778399104325, 0.03887502105131293]\n",
      "current depth:  9\n",
      "vl:  [0.19612159738166063, 0.01198952204774404, 0.01698263015444497, 0.027395433230846245, 0.03330267503958374, 0.03993345046788936, 0.047527026164452785, 0.056410690158848795, 0.06706251861118169, 0.08022387742249984, 0.09712387202482009, 0.11997846065653582, 0.15326570719831423, 0.31755009778351, 0.7291027835589593, 0.027395433230846245]\n",
      "current depth:  10\n",
      "vl:  [0.05052570591040738, 0.014832984025098094, 0.018267793265292324, 0.004596077390556937, 0.010195172230317703, 0.006609718715428308, 0.011919850687677171, 0.0026374108365939883, 0.0011225297690564834, 0.0023954163174116153, 0.0012990708384834995, 9.617267234366695e-05, 1.6955023128192103e-05, 0.0, 0.0010502470617058792, 0.00012571873375754933, 7.078596147207369e-05, 1.5328335270061705e-07, 9.406258256205018e-05, 0.0006449160701062308, 0.0017284664983540378, 0.002510741989324778, 0.00024457717703982675, 0.0005910360419637994, 0.00019131317507558256, 0.0006019234042447072, 0.0006416938210098005, 0.0001440214840542883, 0.006446366751179467, 0.014832984025098094, 0.011383235217817263, 0.004688221692059634, 0.001822364027438434, 7.609521540528405e-05, 0.042016104141699956, 0.03764571226490886, 0.03216897434440206, 0.010049501688486695]\n",
      "current depth:  11\n",
      "vl:  [0.03887502105131293, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  12\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  13\n",
      "vl:  [0.06815929623785226, 0.08657716662093085, 0.00016488511098049232, 0.004657353816991324, 0.04599596008291421, 0.09938585979410257, 0.07325321053265862, 0.05189844865589339, 0.1738094976639552, 0.14886703214742603, 0.1263787220271058, 0.10578599083774445, 0.08657716662093085, 0.06815929623785226, 0.049421802558923394, 0.015140683931871676]\n",
      "current depth:  14\n",
      "vl:  [0.1419708451631329, 0.19118345999979763, 0.01637917007713985, 0.016111606370189935, 0.0013517947353627185, 0.01637917007713985, 0.0005875744549013991, 0.014590558552361604, 0.027464412176933718]\n",
      "current depth:  15\n",
      "vl:  [0.19655090395693994, 0.003548762540498691, 0.05198705216732828, 0.07454094323250514, 0.018103770678494495, 0.0017006663354044642, 0.003548762540498691]\n",
      "current depth:  16\n",
      "vl:  [0.006271075603255408, 0.02033163327920574, 0.04727298257039715, 0.006271075603255408, 0.00911437305383058, 0.02033163327920574]\n",
      "current depth:  17\n",
      "vl:  [0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  18\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  19\n",
      "vl:  [0.35472437774145976]\n",
      "current depth:  20\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  21\n",
      "vl:  [0.04228053312209247, 0.06062882370722798, 0.07948182147590495, 0.12304146538516741, 0.1495705622172283, 0.18090300973893061, 0.21899525866266328, 0.2669934813934353, 0.060411360676619764, 0.09694558259460298, 0.026225073521217532, 0.08045249824505055, 0.04228053312209247, 0.060411360676619764]\n",
      "current depth:  22\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845, 0.00654650269402557, 0.18112206671815106, 0.00654650269402557]\n",
      "current depth:  23\n",
      "vl:  [0.1444372797703026, 0.0, 0.04646521764489747, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  24\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652, 0.02033163327920574]\n",
      "current depth:  25\n",
      "vl:  [0.0, 0.35472437774145976, 0.0]\n",
      "current depth:  26\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  27\n",
      "vl:  [1.0]\n",
      "current depth:  28\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  29\n",
      "vl:  [1.0]\n",
      "current depth:  30\n",
      "vl:  [0.0779818028030355, 0.048882064661275385, 0.0054224909598283594, 0.04963026762482462, 0.016371948435554104, 0.0011194760755969455, 0.0054224909598283594, 0.0779818028030355]\n",
      "current depth:  31\n",
      "vl:  [0.3249617925104421, 0.03972587535899602, 0.09291597794545213, 0.034851554559677256, 0.005906304043889955, 0.0017484967921683212, 0.0920686615848161]\n",
      "current depth:  32\n",
      "vl:  [0.06857481897208437, 0.026079279777968374, 0.002579146528037616, 0.002579146528037616, 0.026079279777968374, 0.06857481897208437]\n",
      "current depth:  33\n",
      "vl:  [0.08405645018009537, 0.01616219857701356, 0.0, 0.01616219857701356, 0.08428582039971266]\n",
      "current depth:  34\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  35\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  36\n",
      "vl:  [0.020852561843756504, 0.0009261497914662364, 0.009817027949690466, 0.003453258331108513, 0.020187920786114054, 0.0013487739431090884, 0.006016209808195638, 0.003804840490958069, 0.010853525209190388, 2.8751596427134063e-05, 0.002472881111753963, 0.00015347767280390667, 0.0015816158282129559, 0.0009261497914662364, 0.020852561843756504, 0.058104949988288565, 0.058104949988288565]\n",
      "current depth:  37\n",
      "vl:  [0.018743401896210654, 0.00044811094733179964, 0.012443846027352644, 0.005502431564327079, 0.026099571722987002, 0.003089098987912278, 0.009772854760729227, 0.007808096164386923, 0.018231740147425648, 0.0012232601991285462, 0.007186819176508965, 0.0007734502525808069, 0.00044811094733179964, 0.018743401896210654, 0.061660370264667315]\n",
      "current depth:  38\n",
      "vl:  [0.016580059332658315, 0.00012247416370816166, 0.015686028838327846, 0.008335896863428343, 0.03363296539530409, 0.0059039478617227996, 0.015135412345049164, 0.014268391478479345, 0.02941126601351029, 0.004950792111860722, 0.01628490817238772, 0.0063508161357078595, 0.016580059332658315, 0.19660295645039516]\n",
      "current depth:  39\n",
      "vl:  [0.02335725413439119, 0.0012737284368484216, 0.009902426765893943, 0.0032440084023068897, 0.021826733569586903, 0.0010655309018225891, 0.006077489978553174, 0.0037062770092080467, 0.012501267428849779, 5.560390497059565e-05, 0.002263907666084008, 0.001643705962615162, 0.05955982280780615]\n",
      "current depth:  40\n",
      "vl:  [0.020820607572057514, 0.0006239138921804833, 0.013058192040234536, 0.005707586917505622, 0.029570912871580948, 0.0031343154353240367, 0.011002039255494372, 0.009318758799868187, 0.02403547481390802, 0.0011239881119591664, 0.009990344389723916, 0.0006239138921804833]\n",
      "current depth:  41\n",
      "vl:  [0.10720059946682817, 0.03487631451464511, 0.0, 0.08975463992235706]\n",
      "current depth:  42\n",
      "vl:  [0.03887502105131293, 0.09500500259896874, 0.06387040410564794]\n",
      "current depth:  43\n",
      "vl:  [0.1740193478031239]\n",
      "current depth:  44\n",
      "vl:  [0.21961295293160082, 0.009877066173124747, 0.0004609563850202613, 0.006918065391871371, 0.006918065391871371, 0.0004609563850202613, 0.002851106566676797]\n",
      "current depth:  45\n",
      "vl:  [0.09516074617477319, 0.012198670872602811, 0.0433033446503569, 0.0, 0.010372390556268395, 0.0]\n",
      "current depth:  46\n",
      "vl:  [0.0012009492048155508, 0.024503372401150855, 0.0034428499155211363, 0.0034428499155211363, 0.0012009492048155508]\n",
      "current depth:  47\n",
      "vl:  [0.1396618834407261]\n",
      "current depth:  48\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845]\n",
      "current depth:  49\n",
      "vl:  [0.1444372797703026, 0.0]\n",
      "current depth:  50\n",
      "vl:  [0.02033163327920574]\n",
      "current depth:  51\n",
      "vl:  [0.1331347651277193, 0.0007215434706190365, 0.027220540954955064, 0.028041082636984186, 0.003692285358543064, 0.03545285774285796, 0.006820417506904309, 0.01616747901883308, 0.0007215434706190365, 0.1620598823995545, 0.003692285358543064]\n",
      "current depth:  52\n",
      "vl:  [0.12223259425191503, 0.003511835147933055, 0.04996085093256333, 0.06105378373381032, 0.02042632416598845, 0.09371621719616725, 0.042873746809138226, 0.18112206671815106, 0.12223259425191503, 0.0]\n",
      "current depth:  53\n",
      "vl:  [0.1642830436827974, 0.0, 0.020976263380628444, 0.020976263380628444, 0.0, 0.030904096500060884, 0.0, 0.0]\n",
      "current depth:  54\n",
      "vl:  [0.14993460792792548, 0.07454094323250514, 0.05198705216732828, 0.003548762540498691, 0.05488014986908448, 0.0017006663354044642, 0.007260555362580846]\n",
      "current depth:  55\n",
      "vl:  [0.03766823861199383, 0.02904940554533142, 0.0, 0.03766823861199383, 0.0, 0.0]\n",
      "current depth:  56\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  57\n",
      "vl:  [0.00654650269402557, 0.02042632416598845, 0.02042632416598845, 0.00654650269402557, 0.00654650269402557]\n",
      "current depth:  58\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  59\n",
      "vl:  [0.35472437774145976, 0.0]\n",
      "current depth:  60\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  61\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  62\n",
      "vl:  [1.0]\n",
      "current depth:  63\n",
      "vl:  [0.009463160379606159, 0.03197309653622524, 0.0, 0.006487271342959594, 0.0, 0.009463160379606159, 0.0, 0.019264840827561348, 0.08537890326205116, 0.006487271342959594]\n",
      "current depth:  64\n",
      "vl:  [0.01637917007713985, 0.04706157416498513, 0.0013517947353627185, 0.016111606370189935, 0.0033062079013812117, 0.0005875744549013991, 0.014590558552361604, 0.2286431947829483, 0.016111606370189935]\n",
      "current depth:  65\n",
      "vl:  [0.005502431564327079, 0.026099571722987002, 0.0019104704348305221, 0.0019104704348305221, 0.005502431564327079, 0.12361826349058296, 0.08282250195514274, 0.060128238249178026]\n",
      "current depth:  66\n",
      "vl:  [0.024503372401150855, 0.07231627283656572, 0.0034428499155211363, 0.03389505987592521, 0.015976004572494196, 0.07231627283656572]\n",
      "current depth:  67\n",
      "vl:  [0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  68\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  69\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  70\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  71\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  72\n",
      "vl:  [0.3311625470900562, 0.20914596325234783, 0.10803154614560007, 0.08039995788957639, 0.0592555474406796, 0.04203498942904823, 0.026975228230812633, 0.08039995788957639]\n",
      "current depth:  73\n",
      "vl:  [1.0]\n",
      "current depth:  74\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  75\n",
      "vl:  [0.007157449141035996, 0.09246429045939385, 0.0027870034023649063, 0.004959879357329997, 0.006088468344589545, 0.0007449302002757883, 0.011163022344172942, 0.06602388591355077, 0.05066277424892794, 0.013345544354931434, 0.0032826780163701464, 0.02358490948403256, 0.008506855978897658, 0.004309936428344649, 0.03545446166785676, 0.0007449302002757883]\n",
      "current depth:  76\n",
      "vl:  [0.004525530074339163, 0.14069233542150236, 0.0017223852227089363, 0.022944064086203204, 0.005256999442785119, 0.022763588734758467, 0.09071860255200705, 0.0663214281567171, 0.019694198438098005, 0.006239087059719598, 0.030743786818436213, 0.012373363424652308, 0.002686284993320699, 0.03142613766226453, 3.370468062445997e-05]\n",
      "current depth:  77\n",
      "vl:  [0.03659373782715106, 0.09336196680283423, 0.00539351844742539, 0.00018831520214906817, 0.010793067371245064, 0.07270431652766694, 0.053224353653063716, 0.012871483030632947, 0.0027294766596929836, 0.0234792444112598, 0.007963060143941176, 0.00539351844742539, 0.039413567611848936, 0.0014103879650374585]\n",
      "current depth:  78\n",
      "vl:  [0.03756940810593821, 0.034802386592657025, 0.004929846084096145, 0.02572055932891343, 0.10518048868764784, 0.07221749844314218, 0.020083564845238468, 0.005880168258493163, 0.03160529880983776, 0.01220469469276198, 0.003394597420929121, 0.034802386592657025, 0.00017981398492684288]\n",
      "current depth:  79\n",
      "vl:  [0.029340994900501946, 0.0, 0.2650621922278978, 0.1801484152912169, 0.1801484152912169]\n",
      "current depth:  80\n",
      "vl:  [0.0, 0.08170416594551044, 0.2525355747672442]\n",
      "current depth:  81\n",
      "vl:  [0.02033163327920574, 0.02033163327920574]\n",
      "current depth:  82\n",
      "vl:  [1.0]\n",
      "current depth:  83\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  84\n",
      "vl:  [0.07758643025994172, 0.0025672227726755356, 0.0155259523373791, 0.03950085359661396, -1.1102230246251565e-16, 0.006031486952556862, 0.06761836267935474, 0.15471661539411724, -1.1102230246251565e-16]\n",
      "current depth:  85\n",
      "vl:  [0.05043207711856275, 0.042475702157007356, 0.0013364153922302152, 0.0013364153922302152, 0.13942673923528215, 0.10433780877834073, 0.048966862572090125, 0.18380975803906682]\n",
      "current depth:  86\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  87\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  88\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  89\n",
      "vl:  [0.10129345660749313, 0.048966862572090125, 0.042475702157007356, 0.01294045001527316, 0.0013364153922302152, 0.0013364153922302152, 0.042475702157007356, 0.11553480482056264, 0.45485387246188064, 0.45485387246188064]\n",
      "current depth:  90\n",
      "vl:  [0.24712122803354972, 0.026975228230812633, 0.04203498942904823, 0.0592555474406796, 0.08039995788957639, 0.10803154614560007, 0.20914596325234783, 0.3311625470900562, 0.7875982374357124]\n",
      "current depth:  91\n",
      "vl:  [0.04783270126825073, 0.003717540254242189, 0.018720426663081775, 0.0007890371333189823, 0.011374969176105029, 0.006718043051334351, 0.00446441583143226, 0.009329006553674328, 0.006877535687617238, 0.012511297110181077, 0.007551023416123786, 0.003965077922143384, 0.0008018335706630937, 0.006541597429715463, 0.0034221761629009938, 0.0012734078861444157, 0.0005772589897835625, 0.0, 0.0005504338382089635, 0.00010540887821455216, 0.004498491726679301, 0.0015683536713117128, 0.000605874733919088, 0.010079924206169802, 0.007329728996123117, 0.002643261550262635, 0.04162795820420849, 0.03762920453508633, 0.033470046182529876, 0.02898709223878468, 0.023719490062496455, 0.07907199320695064]\n",
      "current depth:  92\n",
      "vl:  [0.031286292512233346, 0.3041622065107638, 0.20088564400085046, 0.1504404455257053, 0.1201043167945054, 0.08474626586718464, 0.06429406990686946, 0.050653561093855534, 0.03666866736525018, 0.03305835661787534, 0.026870720784848874, 0.021701451798451888, 0.01940124826854422, 0.01725116356984072, 0.015226001472835976, 0.013301934507303383, 0.011454011332793133, 0.00965144716122575, 0.007845136117141499, 0.005915714662382942]\n",
      "current depth:  93\n",
      "vl:  [0.08155831841638188, 0.057359722698295576, 0.0023405741912092726, 0.009888860111328203, 0.000297636641670596, 0.0026058239049142557, 0.005967171067418116, 0.0008086158294396622, 0.0028972221235702158, 0.0011727919682250644, 0.0035209830776774718, 0.0008086158294396622, 0.0006021615438858776, 0.0027811610845727966, 0.006823482022831692, 0.0013474455511088856, 0.005039710939941871, 0.0003746232606243009, 0.000297636641670596, 0.003591083049637215, 0.0002291247454691225, 0.0023405741912092726, 0.04490956958029419]\n",
      "current depth:  94\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  95\n",
      "vl:  [1.0]\n",
      "current depth:  96\n",
      "vl:  [0.1857207293893467, 0.006031486952556862, 0.006031486952556862, 0.1074835602147594]\n",
      "current depth:  97\n",
      "vl:  [0.39632815712783614, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  98\n",
      "vl:  [0.1396618834407261, 0.1396618834407261]\n",
      "current depth:  99\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  100\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  101\n",
      "vl:  [0.04880068876220159, 0.08324234418491715, 0.19989968667103028, 0.12923753484913508, 0.08324234418491715, 0.04880068876220159]\n",
      "current depth:  102\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  103\n",
      "vl:  [0.11131292859758138, 0.029340994900501946, 0.07317342651141098, 0.01840970721714974, 0.0, 0.029340994900501946, 0.11131292859758138]\n",
      "current depth:  104\n",
      "vl:  [0.29956720366734785, 0.16341875261776725, 0.04879494069539858, 0.003321436156323081, 0.01790512783062725, 0.13889234683021912]\n",
      "current depth:  105\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474, 0.1074835602147594]\n",
      "current depth:  106\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  107\n",
      "vl:  [0.32770706232704216, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  108\n",
      "vl:  [1.0]\n",
      "current depth:  109\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  110\n",
      "vl:  [1.0]\n",
      "current depth:  111\n",
      "vl:  [0.5678946871213929, 0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.31127812445913283]\n",
      "current depth:  112\n",
      "vl:  [0.18147620125065958, 0.03887502105131293, 0.06387040410564794, 0.13792538097003, 0.20490965566138727, 0.3345383316670124, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  113\n",
      "vl:  [1.0, 1.0]\n",
      "current depth:  114\n",
      "vl:  [0.08231985153261825, 0.013107492497438625, 0.018740337027706402, 0.03068579095793265, 0.03758184782085702, 0.054578998670749966, 0.06550784501721568, 0.03758184782085702, 0.03068579095793265, 0.024484933210301835, 0.018740337027706402, 0.013107492497438625, 0.018740337027706402]\n",
      "current depth:  115\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  116\n",
      "vl:  [0.36872135900975694, 0.03887502105131293, 0.06387040410564794, 0.20490965566138727, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  117\n",
      "vl:  [0.21142802859192256, 0.035892627884231906, 0.026501087386621498, 0.0011314390428371153, 0.0004750838548505314, 0.01654986256698211, 0.08216086067917011, 0.05572581616707231, 0.034583896956835725, 0.006179770723736218, 0.12866687041864272, 0.10410696679251233, 0.08096483286568819, 0.05784506765656581, 0.2137542834718897]\n",
      "current depth:  118\n",
      "vl:  [0.09862060778170113, 0.020246622059563374, 0.030413087345934543, 0.05418666832673801, 0.06952093867273432, 0.08880563947945669, 0.06952093867273432, 0.05418666832673801, 0.04144982886425281, 0.030413087345934543, 0.020246622059563374]\n",
      "current depth:  119\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  120\n",
      "vl:  [1.0]\n",
      "current depth:  121\n",
      "vl:  [0.6718700105021306, 0.18112206671815106, 0.02042632416598845, 0.12923753484913508, 0.00654650269402557, 0.28015270043979906]\n",
      "current depth:  122\n",
      "vl:  [0.22814608396437117, 0.014638089798646399, 0.03758891464919054, 0.001894368099625091, 0.01299810100075221, 0.032727248369402676, 0.10015300584040142, 0.07645364175044272, 0.02600866900982578, 0.06500303062583873, 0.01685932408212934, 0.010008745144475025, 0.19212097886551394, 0.0009799994598076586]\n",
      "current depth:  123\n",
      "vl:  [1.0]\n",
      "current depth:  124\n",
      "vl:  [0.2650621922278978, 0.9673183336217959, 0.2650621922278978]\n",
      "current depth:  125\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442, 0.2525355747672442]\n",
      "current depth:  126\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  127\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  128\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  129\n",
      "vl:  [0.08914841432492489, 4.321439369549142e-05, 0.0052414724546126605, 0.010068132866203379, 0.0001293166900281584, 0.0015085020979276843, 0.003925355357878909, 0.0014125866706859592, 0.00025593788194577913, 0.0014790283745211043, 0.006637729346751337, 0.0038081716466179883, 1.025205919724182e-06, 0.0014790283745211043, 0.00025593788194577913, 0.0014122435529148607, 0.011894346701497116, 0.007436095461039109, 0.004140697484148831, 0.0001293166900281584, 0.005454538047930771, 0.001886217819233869, 0.0021637833739358665, 0.007713557129005132, 0.0026194696426508008, 7.893321778618192e-05, 0.0017329763713131317, 0.011036169182915903, 0.03679284857620948, 0.022238263140437243, 0.07973348687787753, 0.051195445668940964]\n",
      "current depth:  130\n",
      "vl:  [0.4189939410656296, 0.04646521764489747, 0.0, 0.2650621922278978, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  131\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.1740193478031239, 0.08975463992235706, 0.08975463992235706]\n",
      "current depth:  132\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  133\n",
      "vl:  [0.35472437774145976, 1.0, 0.35472437774145976, 0.35472437774145976]\n",
      "current depth:  134\n",
      "vl:  [0.14047499629754506, 0.0010659650610598374, 0.04320100006041985, 0.03578634272103474, 0.003691010892848074, 0.003691010892848074, 0.03578634272103474, 0.11850307846721093, 0.04320100006041985, 0.0010659650610598374, 0.14047499629754506, 0.017059384364149003]\n",
      "current depth:  135\n",
      "vl:  [0.1642830436827974, 0.2525355747672442, 0.09826454934165416, 0.020976263380628444, 0.0, 0.020976263380628444, 0.09826454934165416, 0.030904096500060884, 0.0, 0.1642830436827974, 0.030904096500060884]\n",
      "current depth:  136\n",
      "vl:  [0.00911437305383058, 0.006271075603255408, 0.04727298257039715, 0.12451124978365313, 0.26102902170468606, 0.09865444076641222, 0.00911437305383058, 0.13263705581879656, 0.21045859350620652]\n",
      "current depth:  137\n",
      "vl:  [0.3335952857724869, 0.19087450462110933, 0.11499831118948668, 0.06405612194077612, 0.06405612194077612]\n",
      "current depth:  138\n",
      "vl:  [1.0]\n",
      "current depth:  139\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.9245112497836533]\n",
      "current depth:  140\n",
      "vl:  [0.08728792846171006, 0.06500303062583873, 0.006704446094201026, 0.0394158831863924, 0.10015300584040142, 0.0612181055938347, 0.01299810100075221, 0.001894368099625091, 0.03758891464919054, 0.15817614246080478, 0.12256503176022776, 0.08728792846171006, 0.27396906204000565]\n",
      "current depth:  141\n",
      "vl:  [0.06923999653869647, 0.14077739888486904, 0.04424017696155053, 0.1371676772027261, 0.09363914620504095, 0.03398864924743632, 0.014603213499233988, 0.12876564130825247, 0.09865100594634758, 0.06923999653869647]\n",
      "current depth:  142\n",
      "vl:  [0.39632815712783614]\n",
      "current depth:  143\n",
      "vl:  [0.06405612194077612, 0.04900704072750519, 0.018139365349190032, 0.0, 0.004126846387264912, 0.08759794173047272, 0.06405612194077612, 0.04252438688979406]\n",
      "current depth:  144\n",
      "vl:  [0.11008497243260636, 0.026079279777968374, 0.002579146528037616, 0.026079279777968374, 0.09693457468543067]\n",
      "current depth:  145\n",
      "vl:  [0.4326479595302938, 0.06761836267935474, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  146\n",
      "vl:  [0.06405612194077612, 0.19087450462110933, 0.3335952857724869]\n",
      "current depth:  147\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  148\n",
      "vl:  [0.1215042344189909, 0.012878775802370554, 0.005566002752151681, 0.0218531497079759, 0.015092388500114292, 0.002090880863272472, 0.01169274068482661, 0.0282716093567718, 0.008732096099167024, 0.000796980166438043, 0.024729996871962125, 0.008357772647312495, 0.027347471568835758, 0.009230400966805932, 0.0003353562483830987, 0.005566002752151681, 0.03914234439236314, 0.012878775802370554, 0.10905688765012257, 0.25832327657166604]\n",
      "current depth:  149\n",
      "vl:  [0.09424586253166718, 0.004288014491355668, 0.0069103049299918794, 0.010164948277620982, 0.0005614141439900361, 0.014503881500296746, 0.006175390197816655, 0.00020634253667313202, 0.012146214614021857, 0.002059001680907775, 0.023172957667532402, 0.007238121724252496, 8.543901623825849e-05, 0.0069103049299918794, 0.04450434928089019, 0.004288014491355668, 0.16129894007328135]\n",
      "current depth:  150\n",
      "vl:  [0.0874531077292179, 0.007657454841850576, 0.012517685605470192, 0.018696436805359134, 0.003629117727040704, 0.02718759723451945, 0.018385120552399863, 0.005278584526202198, 0.03411003466668784, 0.014943592248227559, 0.0668011715287747, 0.03881742693898308, 0.016228133983856783, 0.0016594679819640055, 0.007657454841850576, 0.0874531077292179]\n",
      "current depth:  151\n",
      "vl:  [0.1720827023145395, 0.051256867002420284, 0.0450029868398238, 0.01378090580310471, 0.04930445582025929, 0.03030514483932223, 0.01104889222972839, 0.04727298257039715, 0.02255184928323372, 0.08208161086595903, 0.04885904963406792, 0.021834240393222914, 0.0033227607981208018, 0.0055077079623934285, 0.09507636686268137]\n",
      "current depth:  152\n",
      "vl:  [0.010008745144475025, 0.01685932408212934, 0.0012297505299947705, 0.02600866900982578, 0.01617703842307018, 0.003447287770155632, 0.032727248369402676, 0.01299810100075221, 0.06709948575852374, 0.03758891464919054, 0.014638089798646399, 0.0009799994598076586, 0.010008745144475025, 0.08728792846171006]\n",
      "current depth:  153\n",
      "vl:  [0.0072953990280224335, 0.012471147310100805, 0.00015877214942946945, 0.019538390282887227, 0.00939281786187296, 0.0004714108476760307, 0.02142638837195634, 0.004987053402159863, 0.04902012774520869, 0.020747109935735, 0.0025190298819811018, 0.0072953990280224335, 0.18577740590888553]\n",
      "current depth:  154\n",
      "vl:  [0.013561092612075002, 0.023720771016214483, 0.0031344330674255745, 0.03822469823189057, 0.030247485498529632, 0.010257637084757874, 0.06665019121715889, 0.03548936959226058, 0.20366244598014902, 0.1619498622418021, 0.12377240478608444, 0.08669202239196727]\n",
      "current depth:  155\n",
      "vl:  [0.0010659650610598374, 0.0020461667745987904, 0.007067259725524144, 0.003691010892848074, 0.0020461667745987904, 0.04320100006041985, 0.0010659650610598374, 0.1764059496755188]\n",
      "current depth:  156\n",
      "vl:  [0.00539351844742539, 0.010793067371245064, 0.000596864170486781, 0.02072083962390814, 0.007963060143941176, 0.00539351844742539, 0.1270419185712244]\n",
      "current depth:  157\n",
      "vl:  [0.0017006663354044642, 0.003548762540498691, 0.007260555362580846, 0.007260555362580846, 0.0017006663354044642, 0.19655090395693994]\n",
      "current depth:  158\n",
      "vl:  [0.00911437305383058, 0.02033163327920574, 0.0, 0.04727298257039715, 0.13263705581879656]\n",
      "current depth:  159\n",
      "vl:  [0.003040199406150453, 0.0072535960017724105, 0.0072535960017724105, 0.019288288480290336]\n",
      "current depth:  160\n",
      "vl:  [0.0, 0.0, 0.2525355747672442]\n",
      "current depth:  161\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  162\n",
      "vl:  [0.0]\n",
      "currently pruning! \n",
      "what is root attributes: \n",
      "[1]\n",
      "current depth:  0\n",
      "vl:  [0.030879681919627488, 0.03943501261927678, 0.051242725884337356, 0.059289316766729216, 0.06906514316881963, 0.07620568516611481, 0.08274583963986824, 0.09205371695610096, 0.07478318600025574, 0.08625768496252735, 0.09435098438088649, 0.09662790022456817, 0.08864060922268426, 0.06840492745308649, 0.05877076613611384, 0.04753633249975632, 0.04335220690681583, 0.037815759388609343, 0.037798760625690826, 0.0296399948185853, 0.024646557698311252, 0.019854819339581047, 0.018448363519704605, 0.02016126177072656, 0.017701700956790363, 0.016028328057380695, 0.0132378321075447, 0.014255083313911746, 0.014705464732858733, 0.011398735486546938, 0.008826702428945427, 0.0037953203556966276, 0.004837420395501617, 0.001994485171360584, 0.004028129681306478, 0.0033647167786069582, 0.001047702317523079, 0.0003209514910577481, 5.588790119725448e-05, 0.0002095054773590116, 0.0006871444339280828, 4.0170330187823056e-07, 1.443938573727015e-05, 8.275774435411229e-06, 2.1226859032040867e-05, 0.00019085985525912818, 0.0006479099213972186, 0.0023282393142426525, 0.004971601592619191, 0.03598353825475495, 0.033078396232502486, 0.030879681919627488, 0.02965392149783031, 0.028300867946652705, 0.026757080958062073, 0.02488924365378155, 0.02231118478303431, 0.057471832294025284, 0.05694119508777039, 0.0385414205476284]\n",
      "current depth:  1\n",
      "vl:  [0.0035198969843555743, 0.005324390140207013, 0.008517681752187216, 0.011223573416760417, 0.015273746948293965, 0.018960494892492293, 0.023111045272675632, 0.030991937932601493, 0.00017747689804055485, 0.0027545816641791963, 0.009894606454673658, 0.020245968859226277, 0.02837500170015306, 0.0012594786290051631]\n",
      "current depth:  2\n",
      "vl:  [0.04603081823964156, 0.00381360075729397, 0.0006959241504366129, 0.04124634444084425, 0.05954528143392814, 0.008536086776932987]\n",
      "current depth:  3\n",
      "vl:  [0.08438361916666769, 0.14946827370263738, 0.04902649676234745, 0.03542818005814553]\n",
      "current depth:  4\n",
      "vl:  [0.31127812445913283]\n",
      "current depth:  5\n",
      "vl:  [0.7149464321932129, 0.2650621922278978]\n",
      "current depth:  6\n",
      "vl:  [0.5141474411982839, 0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  7\n",
      "vl:  [0.010734768729084809, 0.001401359211548798, 0.0008549363905300818, 9.017586007716538e-05, 9.635751501388946e-05, 0.0001351354748299566, 0.0005366908286921249, 9.204247056883071e-05, 4.919252253990104e-05, 7.3408939715780014e-06, 5.218775597686166e-06, 0.0002825483698108367, 0.0002392992408659232, 0.0002854151812644228, 7.709236139296997e-05, 0.00032187445047754626, 0.0006865173154534817, 0.0002248642226898494, 3.7169108674268006e-05, 0.000619676500501682, 0.00015030392254740301, 0.0011700670980981365, 9.979669243331177e-05, 0.00016861026074666767, 0.001390863378309639, 0.002395060101443749, 0.003226292477481012, 0.0022302206388378223, 0.0011402919246601229, 0.003230660792478765, 0.002492076839951506, 0.0030866328063772043, 0.003088305327429188, 0.0040481635013628415, 0.005479616438390019, 0.009240521601106576, 0.013647398110743406, 0.0535117969023159, 0.04874376680938845, 0.04518769879457573, 0.04322632060505464, 0.04107970319727106, 0.0386550632983844, 0.035757723403438274, 0.03182508717585144, 0.06625994613382209, 0.05640900832699647, 0.037681994354507635]\n",
      "current depth:  8\n",
      "vl:  [0.34023563896561493, 0.027152778399104325, 0.03887502105131293, 0.05087149367984439, 0.07838991168482612, 0.09500500259896874, 0.11447914209644945, 0.13792538097003, 0.0016359318241285707, 0.00687524507973983, 0.016908179203691677, 0.03462284214290566, 0.06750483525400915, 0.03887502105131293, 0.027152778399104325, 0.03887502105131293]\n",
      "current depth:  9\n",
      "vl:  [0.19612159738166063, 0.01198952204774404, 0.01698263015444497, 0.027395433230846245, 0.03330267503958374, 0.03993345046788936, 0.047527026164452785, 0.056410690158848795, 0.06706251861118169, 0.08022387742249984, 0.09712387202482009, 0.11997846065653582, 0.15326570719831423, 0.31755009778351, 0.7291027835589593, 0.027395433230846245]\n",
      "current depth:  10\n",
      "vl:  [0.05052570591040738, 0.014832984025098094, 0.018267793265292324, 0.004596077390556937, 0.010195172230317703, 0.006609718715428308, 0.011919850687677171, 0.0026374108365939883, 0.0011225297690564834, 0.0023954163174116153, 0.0012990708384834995, 9.617267234366695e-05, 1.6955023128192103e-05, 0.0, 0.0010502470617058792, 0.00012571873375754933, 7.078596147207369e-05, 1.5328335270061705e-07, 9.406258256205018e-05, 0.0006449160701062308, 0.0017284664983540378, 0.002510741989324778, 0.00024457717703982675, 0.0005910360419637994, 0.00019131317507558256, 0.0006019234042447072, 0.0006416938210098005, 0.0001440214840542883, 0.006446366751179467, 0.014832984025098094, 0.011383235217817263, 0.004688221692059634, 0.001822364027438434, 7.609521540528405e-05, 0.042016104141699956, 0.03764571226490886, 0.03216897434440206, 0.010049501688486695]\n",
      "current depth:  11\n",
      "vl:  [0.03887502105131293, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  12\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  13\n",
      "vl:  [0.06815929623785226, 0.08657716662093085, 0.00016488511098049232, 0.004657353816991324, 0.04599596008291421, 0.09938585979410257, 0.07325321053265862, 0.05189844865589339, 0.1738094976639552, 0.14886703214742603, 0.1263787220271058, 0.10578599083774445, 0.08657716662093085, 0.06815929623785226, 0.049421802558923394, 0.015140683931871676]\n",
      "current depth:  14\n",
      "vl:  [0.1419708451631329, 0.19118345999979763, 0.01637917007713985, 0.016111606370189935, 0.0013517947353627185, 0.01637917007713985, 0.0005875744549013991, 0.014590558552361604, 0.027464412176933718]\n",
      "current depth:  15\n",
      "vl:  [0.19655090395693994, 0.003548762540498691, 0.05198705216732828, 0.07454094323250514, 0.018103770678494495, 0.0017006663354044642, 0.003548762540498691]\n",
      "current depth:  16\n",
      "vl:  [0.006271075603255408, 0.02033163327920574, 0.04727298257039715, 0.006271075603255408, 0.00911437305383058, 0.02033163327920574]\n",
      "current depth:  17\n",
      "vl:  [0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  18\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  19\n",
      "vl:  [0.35472437774145976]\n",
      "current depth:  20\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  21\n",
      "vl:  [0.04228053312209247, 0.06062882370722798, 0.07948182147590495, 0.12304146538516741, 0.1495705622172283, 0.18090300973893061, 0.21899525866266328, 0.2669934813934353, 0.060411360676619764, 0.09694558259460298, 0.026225073521217532, 0.08045249824505055, 0.04228053312209247, 0.060411360676619764]\n",
      "current depth:  22\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845, 0.00654650269402557, 0.18112206671815106, 0.00654650269402557]\n",
      "current depth:  23\n",
      "vl:  [0.1444372797703026, 0.0, 0.04646521764489747, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  24\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652, 0.02033163327920574]\n",
      "current depth:  25\n",
      "vl:  [0.0, 0.35472437774145976, 0.0]\n",
      "current depth:  26\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  27\n",
      "vl:  [1.0]\n",
      "current depth:  28\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  29\n",
      "vl:  [1.0]\n",
      "current depth:  30\n",
      "vl:  [0.0779818028030355, 0.048882064661275385, 0.0054224909598283594, 0.04963026762482462, 0.016371948435554104, 0.0011194760755969455, 0.0054224909598283594, 0.0779818028030355]\n",
      "current depth:  31\n",
      "vl:  [0.3249617925104421, 0.03972587535899602, 0.09291597794545213, 0.034851554559677256, 0.005906304043889955, 0.0017484967921683212, 0.0920686615848161]\n",
      "current depth:  32\n",
      "vl:  [0.06857481897208437, 0.026079279777968374, 0.002579146528037616, 0.002579146528037616, 0.026079279777968374, 0.06857481897208437]\n",
      "current depth:  33\n",
      "vl:  [0.08405645018009537, 0.01616219857701356, 0.0, 0.01616219857701356, 0.08428582039971266]\n",
      "current depth:  34\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  35\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  36\n",
      "vl:  [0.020852561843756504, 0.0009261497914662364, 0.009817027949690466, 0.003453258331108513, 0.020187920786114054, 0.0013487739431090884, 0.006016209808195638, 0.003804840490958069, 0.010853525209190388, 2.8751596427134063e-05, 0.002472881111753963, 0.00015347767280390667, 0.0015816158282129559, 0.0009261497914662364, 0.020852561843756504, 0.058104949988288565, 0.058104949988288565]\n",
      "current depth:  37\n",
      "vl:  [0.018743401896210654, 0.00044811094733179964, 0.012443846027352644, 0.005502431564327079, 0.026099571722987002, 0.003089098987912278, 0.009772854760729227, 0.007808096164386923, 0.018231740147425648, 0.0012232601991285462, 0.007186819176508965, 0.0007734502525808069, 0.00044811094733179964, 0.018743401896210654, 0.061660370264667315]\n",
      "current depth:  38\n",
      "vl:  [0.016580059332658315, 0.00012247416370816166, 0.015686028838327846, 0.008335896863428343, 0.03363296539530409, 0.0059039478617227996, 0.015135412345049164, 0.014268391478479345, 0.02941126601351029, 0.004950792111860722, 0.01628490817238772, 0.0063508161357078595, 0.016580059332658315, 0.19660295645039516]\n",
      "current depth:  39\n",
      "vl:  [0.02335725413439119, 0.0012737284368484216, 0.009902426765893943, 0.0032440084023068897, 0.021826733569586903, 0.0010655309018225891, 0.006077489978553174, 0.0037062770092080467, 0.012501267428849779, 5.560390497059565e-05, 0.002263907666084008, 0.001643705962615162, 0.05955982280780615]\n",
      "current depth:  40\n",
      "vl:  [0.020820607572057514, 0.0006239138921804833, 0.013058192040234536, 0.005707586917505622, 0.029570912871580948, 0.0031343154353240367, 0.011002039255494372, 0.009318758799868187, 0.02403547481390802, 0.0011239881119591664, 0.009990344389723916, 0.0006239138921804833]\n",
      "current depth:  41\n",
      "vl:  [0.10720059946682817, 0.03487631451464511, 0.0, 0.08975463992235706]\n",
      "current depth:  42\n",
      "vl:  [0.03887502105131293, 0.09500500259896874, 0.06387040410564794]\n",
      "current depth:  43\n",
      "vl:  [0.1740193478031239]\n",
      "current depth:  44\n",
      "vl:  [0.21961295293160082, 0.009877066173124747, 0.0004609563850202613, 0.006918065391871371, 0.006918065391871371, 0.0004609563850202613, 0.002851106566676797]\n",
      "current depth:  45\n",
      "vl:  [0.09516074617477319, 0.012198670872602811, 0.0433033446503569, 0.0, 0.010372390556268395, 0.0]\n",
      "current depth:  46\n",
      "vl:  [0.0012009492048155508, 0.024503372401150855, 0.0034428499155211363, 0.0034428499155211363, 0.0012009492048155508]\n",
      "current depth:  47\n",
      "vl:  [0.1396618834407261]\n",
      "current depth:  48\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845]\n",
      "current depth:  49\n",
      "vl:  [0.1444372797703026, 0.0]\n",
      "current depth:  50\n",
      "vl:  [0.02033163327920574]\n",
      "current depth:  51\n",
      "vl:  [0.1331347651277193, 0.0007215434706190365, 0.027220540954955064, 0.028041082636984186, 0.003692285358543064, 0.03545285774285796, 0.006820417506904309, 0.01616747901883308, 0.0007215434706190365, 0.1620598823995545, 0.003692285358543064]\n",
      "current depth:  52\n",
      "vl:  [0.12223259425191503, 0.003511835147933055, 0.04996085093256333, 0.06105378373381032, 0.02042632416598845, 0.09371621719616725, 0.042873746809138226, 0.18112206671815106, 0.12223259425191503, 0.0]\n",
      "current depth:  53\n",
      "vl:  [0.1642830436827974, 0.0, 0.020976263380628444, 0.020976263380628444, 0.0, 0.030904096500060884, 0.0, 0.0]\n",
      "current depth:  54\n",
      "vl:  [0.14993460792792548, 0.07454094323250514, 0.05198705216732828, 0.003548762540498691, 0.05488014986908448, 0.0017006663354044642, 0.007260555362580846]\n",
      "current depth:  55\n",
      "vl:  [0.03766823861199383, 0.02904940554533142, 0.0, 0.03766823861199383, 0.0, 0.0]\n",
      "current depth:  56\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  57\n",
      "vl:  [0.00654650269402557, 0.02042632416598845, 0.02042632416598845, 0.00654650269402557, 0.00654650269402557]\n",
      "current depth:  58\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  59\n",
      "vl:  [0.35472437774145976, 0.0]\n",
      "current depth:  60\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  61\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  62\n",
      "vl:  [1.0]\n",
      "current depth:  63\n",
      "vl:  [0.009463160379606159, 0.03197309653622524, 0.0, 0.006487271342959594, 0.0, 0.009463160379606159, 0.0, 0.019264840827561348, 0.08537890326205116, 0.006487271342959594]\n",
      "current depth:  64\n",
      "vl:  [0.01637917007713985, 0.04706157416498513, 0.0013517947353627185, 0.016111606370189935, 0.0033062079013812117, 0.0005875744549013991, 0.014590558552361604, 0.2286431947829483, 0.016111606370189935]\n",
      "current depth:  65\n",
      "vl:  [0.005502431564327079, 0.026099571722987002, 0.0019104704348305221, 0.0019104704348305221, 0.005502431564327079, 0.12361826349058296, 0.08282250195514274, 0.060128238249178026]\n",
      "current depth:  66\n",
      "vl:  [0.024503372401150855, 0.07231627283656572, 0.0034428499155211363, 0.03389505987592521, 0.015976004572494196, 0.07231627283656572]\n",
      "current depth:  67\n",
      "vl:  [0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  68\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  69\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  70\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  71\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  72\n",
      "vl:  [0.3311625470900562, 0.20914596325234783, 0.10803154614560007, 0.08039995788957639, 0.0592555474406796, 0.04203498942904823, 0.026975228230812633, 0.08039995788957639]\n",
      "current depth:  73\n",
      "vl:  [1.0]\n",
      "current depth:  74\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  75\n",
      "vl:  [0.007157449141035996, 0.09246429045939385, 0.0027870034023649063, 0.004959879357329997, 0.006088468344589545, 0.0007449302002757883, 0.011163022344172942, 0.06602388591355077, 0.05066277424892794, 0.013345544354931434, 0.0032826780163701464, 0.02358490948403256, 0.008506855978897658, 0.004309936428344649, 0.03545446166785676, 0.0007449302002757883]\n",
      "current depth:  76\n",
      "vl:  [0.004525530074339163, 0.14069233542150236, 0.0017223852227089363, 0.022944064086203204, 0.005256999442785119, 0.022763588734758467, 0.09071860255200705, 0.0663214281567171, 0.019694198438098005, 0.006239087059719598, 0.030743786818436213, 0.012373363424652308, 0.002686284993320699, 0.03142613766226453, 3.370468062445997e-05]\n",
      "current depth:  77\n",
      "vl:  [0.03659373782715106, 0.09336196680283423, 0.00539351844742539, 0.00018831520214906817, 0.010793067371245064, 0.07270431652766694, 0.053224353653063716, 0.012871483030632947, 0.0027294766596929836, 0.0234792444112598, 0.007963060143941176, 0.00539351844742539, 0.039413567611848936, 0.0014103879650374585]\n",
      "current depth:  78\n",
      "vl:  [0.03756940810593821, 0.034802386592657025, 0.004929846084096145, 0.02572055932891343, 0.10518048868764784, 0.07221749844314218, 0.020083564845238468, 0.005880168258493163, 0.03160529880983776, 0.01220469469276198, 0.003394597420929121, 0.034802386592657025, 0.00017981398492684288]\n",
      "current depth:  79\n",
      "vl:  [0.029340994900501946, 0.0, 0.2650621922278978, 0.1801484152912169, 0.1801484152912169]\n",
      "current depth:  80\n",
      "vl:  [0.0, 0.08170416594551044, 0.2525355747672442]\n",
      "current depth:  81\n",
      "vl:  [0.02033163327920574, 0.02033163327920574]\n",
      "current depth:  82\n",
      "vl:  [1.0]\n",
      "current depth:  83\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  84\n",
      "vl:  [0.07758643025994172, 0.0025672227726755356, 0.0155259523373791, 0.03950085359661396, -1.1102230246251565e-16, 0.006031486952556862, 0.06761836267935474, 0.15471661539411724, -1.1102230246251565e-16]\n",
      "current depth:  85\n",
      "vl:  [0.05043207711856275, 0.042475702157007356, 0.0013364153922302152, 0.0013364153922302152, 0.13942673923528215, 0.10433780877834073, 0.048966862572090125, 0.18380975803906682]\n",
      "current depth:  86\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  87\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  88\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  89\n",
      "vl:  [0.10129345660749313, 0.048966862572090125, 0.042475702157007356, 0.01294045001527316, 0.0013364153922302152, 0.0013364153922302152, 0.042475702157007356, 0.11553480482056264, 0.45485387246188064, 0.45485387246188064]\n",
      "current depth:  90\n",
      "vl:  [0.24712122803354972, 0.026975228230812633, 0.04203498942904823, 0.0592555474406796, 0.08039995788957639, 0.10803154614560007, 0.20914596325234783, 0.3311625470900562, 0.7875982374357124]\n",
      "current depth:  91\n",
      "vl:  [0.04783270126825073, 0.003717540254242189, 0.018720426663081775, 0.0007890371333189823, 0.011374969176105029, 0.006718043051334351, 0.00446441583143226, 0.009329006553674328, 0.006877535687617238, 0.012511297110181077, 0.007551023416123786, 0.003965077922143384, 0.0008018335706630937, 0.006541597429715463, 0.0034221761629009938, 0.0012734078861444157, 0.0005772589897835625, 0.0, 0.0005504338382089635, 0.00010540887821455216, 0.004498491726679301, 0.0015683536713117128, 0.000605874733919088, 0.010079924206169802, 0.007329728996123117, 0.002643261550262635, 0.04162795820420849, 0.03762920453508633, 0.033470046182529876, 0.02898709223878468, 0.023719490062496455, 0.07907199320695064]\n",
      "current depth:  92\n",
      "vl:  [0.031286292512233346, 0.3041622065107638, 0.20088564400085046, 0.1504404455257053, 0.1201043167945054, 0.08474626586718464, 0.06429406990686946, 0.050653561093855534, 0.03666866736525018, 0.03305835661787534, 0.026870720784848874, 0.021701451798451888, 0.01940124826854422, 0.01725116356984072, 0.015226001472835976, 0.013301934507303383, 0.011454011332793133, 0.00965144716122575, 0.007845136117141499, 0.005915714662382942]\n",
      "current depth:  93\n",
      "vl:  [0.08155831841638188, 0.057359722698295576, 0.0023405741912092726, 0.009888860111328203, 0.000297636641670596, 0.0026058239049142557, 0.005967171067418116, 0.0008086158294396622, 0.0028972221235702158, 0.0011727919682250644, 0.0035209830776774718, 0.0008086158294396622, 0.0006021615438858776, 0.0027811610845727966, 0.006823482022831692, 0.0013474455511088856, 0.005039710939941871, 0.0003746232606243009, 0.000297636641670596, 0.003591083049637215, 0.0002291247454691225, 0.0023405741912092726, 0.04490956958029419]\n",
      "current depth:  94\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  95\n",
      "vl:  [1.0]\n",
      "current depth:  96\n",
      "vl:  [0.1857207293893467, 0.006031486952556862, 0.006031486952556862, 0.1074835602147594]\n",
      "current depth:  97\n",
      "vl:  [0.39632815712783614, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  98\n",
      "vl:  [0.1396618834407261, 0.1396618834407261]\n",
      "current depth:  99\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  100\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  101\n",
      "vl:  [0.04880068876220159, 0.08324234418491715, 0.19989968667103028, 0.12923753484913508, 0.08324234418491715, 0.04880068876220159]\n",
      "current depth:  102\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  103\n",
      "vl:  [0.11131292859758138, 0.029340994900501946, 0.07317342651141098, 0.01840970721714974, 0.0, 0.029340994900501946, 0.11131292859758138]\n",
      "current depth:  104\n",
      "vl:  [0.29956720366734785, 0.16341875261776725, 0.04879494069539858, 0.003321436156323081, 0.01790512783062725, 0.13889234683021912]\n",
      "current depth:  105\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474, 0.1074835602147594]\n",
      "current depth:  106\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  107\n",
      "vl:  [0.32770706232704216, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  108\n",
      "vl:  [1.0]\n",
      "current depth:  109\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  110\n",
      "vl:  [1.0]\n",
      "current depth:  111\n",
      "vl:  [0.5678946871213929, 0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.31127812445913283]\n",
      "current depth:  112\n",
      "vl:  [0.18147620125065958, 0.03887502105131293, 0.06387040410564794, 0.13792538097003, 0.20490965566138727, 0.3345383316670124, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  113\n",
      "vl:  [1.0, 1.0]\n",
      "current depth:  114\n",
      "vl:  [0.08231985153261825, 0.013107492497438625, 0.018740337027706402, 0.03068579095793265, 0.03758184782085702, 0.054578998670749966, 0.06550784501721568, 0.03758184782085702, 0.03068579095793265, 0.024484933210301835, 0.018740337027706402, 0.013107492497438625, 0.018740337027706402]\n",
      "current depth:  115\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  116\n",
      "vl:  [0.36872135900975694, 0.03887502105131293, 0.06387040410564794, 0.20490965566138727, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  117\n",
      "vl:  [0.21142802859192256, 0.035892627884231906, 0.026501087386621498, 0.0011314390428371153, 0.0004750838548505314, 0.01654986256698211, 0.08216086067917011, 0.05572581616707231, 0.034583896956835725, 0.006179770723736218, 0.12866687041864272, 0.10410696679251233, 0.08096483286568819, 0.05784506765656581, 0.2137542834718897]\n",
      "current depth:  118\n",
      "vl:  [0.09862060778170113, 0.020246622059563374, 0.030413087345934543, 0.05418666832673801, 0.06952093867273432, 0.08880563947945669, 0.06952093867273432, 0.05418666832673801, 0.04144982886425281, 0.030413087345934543, 0.020246622059563374]\n",
      "current depth:  119\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  120\n",
      "vl:  [1.0]\n",
      "current depth:  121\n",
      "vl:  [0.6718700105021306, 0.18112206671815106, 0.02042632416598845, 0.12923753484913508, 0.00654650269402557, 0.28015270043979906]\n",
      "current depth:  122\n",
      "vl:  [0.22814608396437117, 0.014638089798646399, 0.03758891464919054, 0.001894368099625091, 0.01299810100075221, 0.032727248369402676, 0.10015300584040142, 0.07645364175044272, 0.02600866900982578, 0.06500303062583873, 0.01685932408212934, 0.010008745144475025, 0.19212097886551394, 0.0009799994598076586]\n",
      "current depth:  123\n",
      "vl:  [1.0]\n",
      "current depth:  124\n",
      "vl:  [0.2650621922278978, 0.9673183336217959, 0.2650621922278978]\n",
      "current depth:  125\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442, 0.2525355747672442]\n",
      "current depth:  126\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  127\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  128\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  129\n",
      "vl:  [0.08914841432492489, 4.321439369549142e-05, 0.0052414724546126605, 0.010068132866203379, 0.0001293166900281584, 0.0015085020979276843, 0.003925355357878909, 0.0014125866706859592, 0.00025593788194577913, 0.0014790283745211043, 0.006637729346751337, 0.0038081716466179883, 1.025205919724182e-06, 0.0014790283745211043, 0.00025593788194577913, 0.0014122435529148607, 0.011894346701497116, 0.007436095461039109, 0.004140697484148831, 0.0001293166900281584, 0.005454538047930771, 0.001886217819233869, 0.0021637833739358665, 0.007713557129005132, 0.0026194696426508008, 7.893321778618192e-05, 0.0017329763713131317, 0.011036169182915903, 0.03679284857620948, 0.022238263140437243, 0.07973348687787753, 0.051195445668940964]\n",
      "current depth:  130\n",
      "vl:  [0.4189939410656296, 0.04646521764489747, 0.0, 0.2650621922278978, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  131\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.1740193478031239, 0.08975463992235706, 0.08975463992235706]\n",
      "current depth:  132\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  133\n",
      "vl:  [0.35472437774145976, 1.0, 0.35472437774145976, 0.35472437774145976]\n",
      "current depth:  134\n",
      "vl:  [0.14047499629754506, 0.0010659650610598374, 0.04320100006041985, 0.03578634272103474, 0.003691010892848074, 0.003691010892848074, 0.03578634272103474, 0.11850307846721093, 0.04320100006041985, 0.0010659650610598374, 0.14047499629754506, 0.017059384364149003]\n",
      "current depth:  135\n",
      "vl:  [0.1642830436827974, 0.2525355747672442, 0.09826454934165416, 0.020976263380628444, 0.0, 0.020976263380628444, 0.09826454934165416, 0.030904096500060884, 0.0, 0.1642830436827974, 0.030904096500060884]\n",
      "current depth:  136\n",
      "vl:  [0.00911437305383058, 0.006271075603255408, 0.04727298257039715, 0.12451124978365313, 0.26102902170468606, 0.09865444076641222, 0.00911437305383058, 0.13263705581879656, 0.21045859350620652]\n",
      "current depth:  137\n",
      "vl:  [0.3335952857724869, 0.19087450462110933, 0.11499831118948668, 0.06405612194077612, 0.06405612194077612]\n",
      "current depth:  138\n",
      "vl:  [1.0]\n",
      "current depth:  139\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.9245112497836533]\n",
      "current depth:  140\n",
      "vl:  [0.08728792846171006, 0.06500303062583873, 0.006704446094201026, 0.0394158831863924, 0.10015300584040142, 0.0612181055938347, 0.01299810100075221, 0.001894368099625091, 0.03758891464919054, 0.15817614246080478, 0.12256503176022776, 0.08728792846171006, 0.27396906204000565]\n",
      "current depth:  141\n",
      "vl:  [0.06923999653869647, 0.14077739888486904, 0.04424017696155053, 0.1371676772027261, 0.09363914620504095, 0.03398864924743632, 0.014603213499233988, 0.12876564130825247, 0.09865100594634758, 0.06923999653869647]\n",
      "current depth:  142\n",
      "vl:  [0.39632815712783614]\n",
      "current depth:  143\n",
      "vl:  [0.06405612194077612, 0.04900704072750519, 0.018139365349190032, 0.0, 0.004126846387264912, 0.08759794173047272, 0.06405612194077612, 0.04252438688979406]\n",
      "current depth:  144\n",
      "vl:  [0.11008497243260636, 0.026079279777968374, 0.002579146528037616, 0.026079279777968374, 0.09693457468543067]\n",
      "current depth:  145\n",
      "vl:  [0.4326479595302938, 0.06761836267935474, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  146\n",
      "vl:  [0.06405612194077612, 0.19087450462110933, 0.3335952857724869]\n",
      "current depth:  147\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  148\n",
      "vl:  [0.1215042344189909, 0.012878775802370554, 0.005566002752151681, 0.0218531497079759, 0.015092388500114292, 0.002090880863272472, 0.01169274068482661, 0.0282716093567718, 0.008732096099167024, 0.000796980166438043, 0.024729996871962125, 0.008357772647312495, 0.027347471568835758, 0.009230400966805932, 0.0003353562483830987, 0.005566002752151681, 0.03914234439236314, 0.012878775802370554, 0.10905688765012257, 0.25832327657166604]\n",
      "current depth:  149\n",
      "vl:  [0.09424586253166718, 0.004288014491355668, 0.0069103049299918794, 0.010164948277620982, 0.0005614141439900361, 0.014503881500296746, 0.006175390197816655, 0.00020634253667313202, 0.012146214614021857, 0.002059001680907775, 0.023172957667532402, 0.007238121724252496, 8.543901623825849e-05, 0.0069103049299918794, 0.04450434928089019, 0.004288014491355668, 0.16129894007328135]\n",
      "current depth:  150\n",
      "vl:  [0.0874531077292179, 0.007657454841850576, 0.012517685605470192, 0.018696436805359134, 0.003629117727040704, 0.02718759723451945, 0.018385120552399863, 0.005278584526202198, 0.03411003466668784, 0.014943592248227559, 0.0668011715287747, 0.03881742693898308, 0.016228133983856783, 0.0016594679819640055, 0.007657454841850576, 0.0874531077292179]\n",
      "current depth:  151\n",
      "vl:  [0.1720827023145395, 0.051256867002420284, 0.0450029868398238, 0.01378090580310471, 0.04930445582025929, 0.03030514483932223, 0.01104889222972839, 0.04727298257039715, 0.02255184928323372, 0.08208161086595903, 0.04885904963406792, 0.021834240393222914, 0.0033227607981208018, 0.0055077079623934285, 0.09507636686268137]\n",
      "current depth:  152\n",
      "vl:  [0.010008745144475025, 0.01685932408212934, 0.0012297505299947705, 0.02600866900982578, 0.01617703842307018, 0.003447287770155632, 0.032727248369402676, 0.01299810100075221, 0.06709948575852374, 0.03758891464919054, 0.014638089798646399, 0.0009799994598076586, 0.010008745144475025, 0.08728792846171006]\n",
      "current depth:  153\n",
      "vl:  [0.0072953990280224335, 0.012471147310100805, 0.00015877214942946945, 0.019538390282887227, 0.00939281786187296, 0.0004714108476760307, 0.02142638837195634, 0.004987053402159863, 0.04902012774520869, 0.020747109935735, 0.0025190298819811018, 0.0072953990280224335, 0.18577740590888553]\n",
      "current depth:  154\n",
      "vl:  [0.013561092612075002, 0.023720771016214483, 0.0031344330674255745, 0.03822469823189057, 0.030247485498529632, 0.010257637084757874, 0.06665019121715889, 0.03548936959226058, 0.20366244598014902, 0.1619498622418021, 0.12377240478608444, 0.08669202239196727]\n",
      "current depth:  155\n",
      "vl:  [0.0010659650610598374, 0.0020461667745987904, 0.007067259725524144, 0.003691010892848074, 0.0020461667745987904, 0.04320100006041985, 0.0010659650610598374, 0.1764059496755188]\n",
      "current depth:  156\n",
      "vl:  [0.00539351844742539, 0.010793067371245064, 0.000596864170486781, 0.02072083962390814, 0.007963060143941176, 0.00539351844742539, 0.1270419185712244]\n",
      "current depth:  157\n",
      "vl:  [0.0017006663354044642, 0.003548762540498691, 0.007260555362580846, 0.007260555362580846, 0.0017006663354044642, 0.19655090395693994]\n",
      "current depth:  158\n",
      "vl:  [0.00911437305383058, 0.02033163327920574, 0.0, 0.04727298257039715, 0.13263705581879656]\n",
      "current depth:  159\n",
      "vl:  [0.003040199406150453, 0.0072535960017724105, 0.0072535960017724105, 0.019288288480290336]\n",
      "current depth:  160\n",
      "vl:  [0.0, 0.0, 0.2525355747672442]\n",
      "current depth:  161\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  162\n",
      "vl:  [0.0]\n",
      "currently pruning! \n",
      "what is root attributes: \n",
      "[]\n",
      "current depth:  0\n",
      "vl:  [0.030879681919627488, 0.03943501261927678, 0.051242725884337356, 0.059289316766729216, 0.06906514316881963, 0.07620568516611481, 0.08274583963986824, 0.09205371695610096, 0.07478318600025574, 0.08625768496252735, 0.09435098438088649, 0.09662790022456817, 0.08864060922268426, 0.06840492745308649, 0.05877076613611384, 0.04753633249975632, 0.04335220690681583, 0.037815759388609343, 0.037798760625690826, 0.0296399948185853, 0.024646557698311252, 0.019854819339581047, 0.018448363519704605, 0.02016126177072656, 0.017701700956790363, 0.016028328057380695, 0.0132378321075447, 0.014255083313911746, 0.014705464732858733, 0.011398735486546938, 0.008826702428945427, 0.0037953203556966276, 0.004837420395501617, 0.001994485171360584, 0.004028129681306478, 0.0033647167786069582, 0.001047702317523079, 0.0003209514910577481, 5.588790119725448e-05, 0.0002095054773590116, 0.0006871444339280828, 4.0170330187823056e-07, 1.443938573727015e-05, 8.275774435411229e-06, 2.1226859032040867e-05, 0.00019085985525912818, 0.0006479099213972186, 0.0023282393142426525, 0.004971601592619191, 0.03598353825475495, 0.033078396232502486, 0.030879681919627488, 0.02965392149783031, 0.028300867946652705, 0.026757080958062073, 0.02488924365378155, 0.02231118478303431, 0.057471832294025284, 0.05694119508777039, 0.0385414205476284]\n",
      "current depth:  1\n",
      "vl:  [0.0035198969843555743, 0.005324390140207013, 0.008517681752187216, 0.011223573416760417, 0.015273746948293965, 0.018960494892492293, 0.023111045272675632, 0.030991937932601493, 0.00017747689804055485, 0.0027545816641791963, 0.009894606454673658, 0.020245968859226277, 0.02837500170015306, 0.0012594786290051631]\n",
      "current depth:  2\n",
      "vl:  [0.04603081823964156, 0.00381360075729397, 0.0006959241504366129, 0.04124634444084425, 0.05954528143392814, 0.008536086776932987]\n",
      "current depth:  3\n",
      "vl:  [0.08438361916666769, 0.14946827370263738, 0.04902649676234745, 0.03542818005814553]\n",
      "current depth:  4\n",
      "vl:  [0.31127812445913283]\n",
      "current depth:  5\n",
      "vl:  [0.7149464321932129, 0.2650621922278978]\n",
      "current depth:  6\n",
      "vl:  [0.5141474411982839, 0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  7\n",
      "vl:  [0.010734768729084809, 0.001401359211548798, 0.0008549363905300818, 9.017586007716538e-05, 9.635751501388946e-05, 0.0001351354748299566, 0.0005366908286921249, 9.204247056883071e-05, 4.919252253990104e-05, 7.3408939715780014e-06, 5.218775597686166e-06, 0.0002825483698108367, 0.0002392992408659232, 0.0002854151812644228, 7.709236139296997e-05, 0.00032187445047754626, 0.0006865173154534817, 0.0002248642226898494, 3.7169108674268006e-05, 0.000619676500501682, 0.00015030392254740301, 0.0011700670980981365, 9.979669243331177e-05, 0.00016861026074666767, 0.001390863378309639, 0.002395060101443749, 0.003226292477481012, 0.0022302206388378223, 0.0011402919246601229, 0.003230660792478765, 0.002492076839951506, 0.0030866328063772043, 0.003088305327429188, 0.0040481635013628415, 0.005479616438390019, 0.009240521601106576, 0.013647398110743406, 0.0535117969023159, 0.04874376680938845, 0.04518769879457573, 0.04322632060505464, 0.04107970319727106, 0.0386550632983844, 0.035757723403438274, 0.03182508717585144, 0.06625994613382209, 0.05640900832699647, 0.037681994354507635]\n",
      "current depth:  8\n",
      "vl:  [0.34023563896561493, 0.027152778399104325, 0.03887502105131293, 0.05087149367984439, 0.07838991168482612, 0.09500500259896874, 0.11447914209644945, 0.13792538097003, 0.0016359318241285707, 0.00687524507973983, 0.016908179203691677, 0.03462284214290566, 0.06750483525400915, 0.03887502105131293, 0.027152778399104325, 0.03887502105131293]\n",
      "current depth:  9\n",
      "vl:  [0.19612159738166063, 0.01198952204774404, 0.01698263015444497, 0.027395433230846245, 0.03330267503958374, 0.03993345046788936, 0.047527026164452785, 0.056410690158848795, 0.06706251861118169, 0.08022387742249984, 0.09712387202482009, 0.11997846065653582, 0.15326570719831423, 0.31755009778351, 0.7291027835589593, 0.027395433230846245]\n",
      "current depth:  10\n",
      "vl:  [0.05052570591040738, 0.014832984025098094, 0.018267793265292324, 0.004596077390556937, 0.010195172230317703, 0.006609718715428308, 0.011919850687677171, 0.0026374108365939883, 0.0011225297690564834, 0.0023954163174116153, 0.0012990708384834995, 9.617267234366695e-05, 1.6955023128192103e-05, 0.0, 0.0010502470617058792, 0.00012571873375754933, 7.078596147207369e-05, 1.5328335270061705e-07, 9.406258256205018e-05, 0.0006449160701062308, 0.0017284664983540378, 0.002510741989324778, 0.00024457717703982675, 0.0005910360419637994, 0.00019131317507558256, 0.0006019234042447072, 0.0006416938210098005, 0.0001440214840542883, 0.006446366751179467, 0.014832984025098094, 0.011383235217817263, 0.004688221692059634, 0.001822364027438434, 7.609521540528405e-05, 0.042016104141699956, 0.03764571226490886, 0.03216897434440206, 0.010049501688486695]\n",
      "current depth:  11\n",
      "vl:  [0.03887502105131293, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  12\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  13\n",
      "vl:  [0.06815929623785226, 0.08657716662093085, 0.00016488511098049232, 0.004657353816991324, 0.04599596008291421, 0.09938585979410257, 0.07325321053265862, 0.05189844865589339, 0.1738094976639552, 0.14886703214742603, 0.1263787220271058, 0.10578599083774445, 0.08657716662093085, 0.06815929623785226, 0.049421802558923394, 0.015140683931871676]\n",
      "current depth:  14\n",
      "vl:  [0.1419708451631329, 0.19118345999979763, 0.01637917007713985, 0.016111606370189935, 0.0013517947353627185, 0.01637917007713985, 0.0005875744549013991, 0.014590558552361604, 0.027464412176933718]\n",
      "current depth:  15\n",
      "vl:  [0.19655090395693994, 0.003548762540498691, 0.05198705216732828, 0.07454094323250514, 0.018103770678494495, 0.0017006663354044642, 0.003548762540498691]\n",
      "current depth:  16\n",
      "vl:  [0.006271075603255408, 0.02033163327920574, 0.04727298257039715, 0.006271075603255408, 0.00911437305383058, 0.02033163327920574]\n",
      "current depth:  17\n",
      "vl:  [0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  18\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  19\n",
      "vl:  [0.35472437774145976]\n",
      "current depth:  20\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  21\n",
      "vl:  [0.04228053312209247, 0.06062882370722798, 0.07948182147590495, 0.12304146538516741, 0.1495705622172283, 0.18090300973893061, 0.21899525866266328, 0.2669934813934353, 0.060411360676619764, 0.09694558259460298, 0.026225073521217532, 0.08045249824505055, 0.04228053312209247, 0.060411360676619764]\n",
      "current depth:  22\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845, 0.00654650269402557, 0.18112206671815106, 0.00654650269402557]\n",
      "current depth:  23\n",
      "vl:  [0.1444372797703026, 0.0, 0.04646521764489747, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  24\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652, 0.02033163327920574]\n",
      "current depth:  25\n",
      "vl:  [0.0, 0.35472437774145976, 0.0]\n",
      "current depth:  26\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  27\n",
      "vl:  [1.0]\n",
      "current depth:  28\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  29\n",
      "vl:  [1.0]\n",
      "current depth:  30\n",
      "vl:  [0.0779818028030355, 0.048882064661275385, 0.0054224909598283594, 0.04963026762482462, 0.016371948435554104, 0.0011194760755969455, 0.0054224909598283594, 0.0779818028030355]\n",
      "current depth:  31\n",
      "vl:  [0.3249617925104421, 0.03972587535899602, 0.09291597794545213, 0.034851554559677256, 0.005906304043889955, 0.0017484967921683212, 0.0920686615848161]\n",
      "current depth:  32\n",
      "vl:  [0.06857481897208437, 0.026079279777968374, 0.002579146528037616, 0.002579146528037616, 0.026079279777968374, 0.06857481897208437]\n",
      "current depth:  33\n",
      "vl:  [0.08405645018009537, 0.01616219857701356, 0.0, 0.01616219857701356, 0.08428582039971266]\n",
      "current depth:  34\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  35\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  36\n",
      "vl:  [0.020852561843756504, 0.0009261497914662364, 0.009817027949690466, 0.003453258331108513, 0.020187920786114054, 0.0013487739431090884, 0.006016209808195638, 0.003804840490958069, 0.010853525209190388, 2.8751596427134063e-05, 0.002472881111753963, 0.00015347767280390667, 0.0015816158282129559, 0.0009261497914662364, 0.020852561843756504, 0.058104949988288565, 0.058104949988288565]\n",
      "current depth:  37\n",
      "vl:  [0.018743401896210654, 0.00044811094733179964, 0.012443846027352644, 0.005502431564327079, 0.026099571722987002, 0.003089098987912278, 0.009772854760729227, 0.007808096164386923, 0.018231740147425648, 0.0012232601991285462, 0.007186819176508965, 0.0007734502525808069, 0.00044811094733179964, 0.018743401896210654, 0.061660370264667315]\n",
      "current depth:  38\n",
      "vl:  [0.016580059332658315, 0.00012247416370816166, 0.015686028838327846, 0.008335896863428343, 0.03363296539530409, 0.0059039478617227996, 0.015135412345049164, 0.014268391478479345, 0.02941126601351029, 0.004950792111860722, 0.01628490817238772, 0.0063508161357078595, 0.016580059332658315, 0.19660295645039516]\n",
      "current depth:  39\n",
      "vl:  [0.02335725413439119, 0.0012737284368484216, 0.009902426765893943, 0.0032440084023068897, 0.021826733569586903, 0.0010655309018225891, 0.006077489978553174, 0.0037062770092080467, 0.012501267428849779, 5.560390497059565e-05, 0.002263907666084008, 0.001643705962615162, 0.05955982280780615]\n",
      "current depth:  40\n",
      "vl:  [0.020820607572057514, 0.0006239138921804833, 0.013058192040234536, 0.005707586917505622, 0.029570912871580948, 0.0031343154353240367, 0.011002039255494372, 0.009318758799868187, 0.02403547481390802, 0.0011239881119591664, 0.009990344389723916, 0.0006239138921804833]\n",
      "current depth:  41\n",
      "vl:  [0.10720059946682817, 0.03487631451464511, 0.0, 0.08975463992235706]\n",
      "current depth:  42\n",
      "vl:  [0.03887502105131293, 0.09500500259896874, 0.06387040410564794]\n",
      "current depth:  43\n",
      "vl:  [0.1740193478031239]\n",
      "current depth:  44\n",
      "vl:  [0.21961295293160082, 0.009877066173124747, 0.0004609563850202613, 0.006918065391871371, 0.006918065391871371, 0.0004609563850202613, 0.002851106566676797]\n",
      "current depth:  45\n",
      "vl:  [0.09516074617477319, 0.012198670872602811, 0.0433033446503569, 0.0, 0.010372390556268395, 0.0]\n",
      "current depth:  46\n",
      "vl:  [0.0012009492048155508, 0.024503372401150855, 0.0034428499155211363, 0.0034428499155211363, 0.0012009492048155508]\n",
      "current depth:  47\n",
      "vl:  [0.1396618834407261]\n",
      "current depth:  48\n",
      "vl:  [0.28015270043979906, 0.00654650269402557, 0.02042632416598845]\n",
      "current depth:  49\n",
      "vl:  [0.1444372797703026, 0.0]\n",
      "current depth:  50\n",
      "vl:  [0.02033163327920574]\n",
      "current depth:  51\n",
      "vl:  [0.1331347651277193, 0.0007215434706190365, 0.027220540954955064, 0.028041082636984186, 0.003692285358543064, 0.03545285774285796, 0.006820417506904309, 0.01616747901883308, 0.0007215434706190365, 0.1620598823995545, 0.003692285358543064]\n",
      "current depth:  52\n",
      "vl:  [0.12223259425191503, 0.003511835147933055, 0.04996085093256333, 0.06105378373381032, 0.02042632416598845, 0.09371621719616725, 0.042873746809138226, 0.18112206671815106, 0.12223259425191503, 0.0]\n",
      "current depth:  53\n",
      "vl:  [0.1642830436827974, 0.0, 0.020976263380628444, 0.020976263380628444, 0.0, 0.030904096500060884, 0.0, 0.0]\n",
      "current depth:  54\n",
      "vl:  [0.14993460792792548, 0.07454094323250514, 0.05198705216732828, 0.003548762540498691, 0.05488014986908448, 0.0017006663354044642, 0.007260555362580846]\n",
      "current depth:  55\n",
      "vl:  [0.03766823861199383, 0.02904940554533142, 0.0, 0.03766823861199383, 0.0, 0.0]\n",
      "current depth:  56\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  57\n",
      "vl:  [0.00654650269402557, 0.02042632416598845, 0.02042632416598845, 0.00654650269402557, 0.00654650269402557]\n",
      "current depth:  58\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  59\n",
      "vl:  [0.35472437774145976, 0.0]\n",
      "current depth:  60\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  61\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  62\n",
      "vl:  [1.0]\n",
      "current depth:  63\n",
      "vl:  [0.009463160379606159, 0.03197309653622524, 0.0, 0.006487271342959594, 0.0, 0.009463160379606159, 0.0, 0.019264840827561348, 0.08537890326205116, 0.006487271342959594]\n",
      "current depth:  64\n",
      "vl:  [0.01637917007713985, 0.04706157416498513, 0.0013517947353627185, 0.016111606370189935, 0.0033062079013812117, 0.0005875744549013991, 0.014590558552361604, 0.2286431947829483, 0.016111606370189935]\n",
      "current depth:  65\n",
      "vl:  [0.005502431564327079, 0.026099571722987002, 0.0019104704348305221, 0.0019104704348305221, 0.005502431564327079, 0.12361826349058296, 0.08282250195514274, 0.060128238249178026]\n",
      "current depth:  66\n",
      "vl:  [0.024503372401150855, 0.07231627283656572, 0.0034428499155211363, 0.03389505987592521, 0.015976004572494196, 0.07231627283656572]\n",
      "current depth:  67\n",
      "vl:  [0.1740193478031239, 0.32770706232704216]\n",
      "current depth:  68\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442]\n",
      "current depth:  69\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  70\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  71\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  72\n",
      "vl:  [0.3311625470900562, 0.20914596325234783, 0.10803154614560007, 0.08039995788957639, 0.0592555474406796, 0.04203498942904823, 0.026975228230812633, 0.08039995788957639]\n",
      "current depth:  73\n",
      "vl:  [1.0]\n",
      "current depth:  74\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  75\n",
      "vl:  [0.007157449141035996, 0.09246429045939385, 0.0027870034023649063, 0.004959879357329997, 0.006088468344589545, 0.0007449302002757883, 0.011163022344172942, 0.06602388591355077, 0.05066277424892794, 0.013345544354931434, 0.0032826780163701464, 0.02358490948403256, 0.008506855978897658, 0.004309936428344649, 0.03545446166785676, 0.0007449302002757883]\n",
      "current depth:  76\n",
      "vl:  [0.004525530074339163, 0.14069233542150236, 0.0017223852227089363, 0.022944064086203204, 0.005256999442785119, 0.022763588734758467, 0.09071860255200705, 0.0663214281567171, 0.019694198438098005, 0.006239087059719598, 0.030743786818436213, 0.012373363424652308, 0.002686284993320699, 0.03142613766226453, 3.370468062445997e-05]\n",
      "current depth:  77\n",
      "vl:  [0.03659373782715106, 0.09336196680283423, 0.00539351844742539, 0.00018831520214906817, 0.010793067371245064, 0.07270431652766694, 0.053224353653063716, 0.012871483030632947, 0.0027294766596929836, 0.0234792444112598, 0.007963060143941176, 0.00539351844742539, 0.039413567611848936, 0.0014103879650374585]\n",
      "current depth:  78\n",
      "vl:  [0.03756940810593821, 0.034802386592657025, 0.004929846084096145, 0.02572055932891343, 0.10518048868764784, 0.07221749844314218, 0.020083564845238468, 0.005880168258493163, 0.03160529880983776, 0.01220469469276198, 0.003394597420929121, 0.034802386592657025, 0.00017981398492684288]\n",
      "current depth:  79\n",
      "vl:  [0.029340994900501946, 0.0, 0.2650621922278978, 0.1801484152912169, 0.1801484152912169]\n",
      "current depth:  80\n",
      "vl:  [0.0, 0.08170416594551044, 0.2525355747672442]\n",
      "current depth:  81\n",
      "vl:  [0.02033163327920574, 0.02033163327920574]\n",
      "current depth:  82\n",
      "vl:  [1.0]\n",
      "current depth:  83\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  84\n",
      "vl:  [0.07758643025994172, 0.0025672227726755356, 0.0155259523373791, 0.03950085359661396, -1.1102230246251565e-16, 0.006031486952556862, 0.06761836267935474, 0.15471661539411724, -1.1102230246251565e-16]\n",
      "current depth:  85\n",
      "vl:  [0.05043207711856275, 0.042475702157007356, 0.0013364153922302152, 0.0013364153922302152, 0.13942673923528215, 0.10433780877834073, 0.048966862572090125, 0.18380975803906682]\n",
      "current depth:  86\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  87\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  88\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  89\n",
      "vl:  [0.10129345660749313, 0.048966862572090125, 0.042475702157007356, 0.01294045001527316, 0.0013364153922302152, 0.0013364153922302152, 0.042475702157007356, 0.11553480482056264, 0.45485387246188064, 0.45485387246188064]\n",
      "current depth:  90\n",
      "vl:  [0.24712122803354972, 0.026975228230812633, 0.04203498942904823, 0.0592555474406796, 0.08039995788957639, 0.10803154614560007, 0.20914596325234783, 0.3311625470900562, 0.7875982374357124]\n",
      "current depth:  91\n",
      "vl:  [0.04783270126825073, 0.003717540254242189, 0.018720426663081775, 0.0007890371333189823, 0.011374969176105029, 0.006718043051334351, 0.00446441583143226, 0.009329006553674328, 0.006877535687617238, 0.012511297110181077, 0.007551023416123786, 0.003965077922143384, 0.0008018335706630937, 0.006541597429715463, 0.0034221761629009938, 0.0012734078861444157, 0.0005772589897835625, 0.0, 0.0005504338382089635, 0.00010540887821455216, 0.004498491726679301, 0.0015683536713117128, 0.000605874733919088, 0.010079924206169802, 0.007329728996123117, 0.002643261550262635, 0.04162795820420849, 0.03762920453508633, 0.033470046182529876, 0.02898709223878468, 0.023719490062496455, 0.07907199320695064]\n",
      "current depth:  92\n",
      "vl:  [0.031286292512233346, 0.3041622065107638, 0.20088564400085046, 0.1504404455257053, 0.1201043167945054, 0.08474626586718464, 0.06429406990686946, 0.050653561093855534, 0.03666866736525018, 0.03305835661787534, 0.026870720784848874, 0.021701451798451888, 0.01940124826854422, 0.01725116356984072, 0.015226001472835976, 0.013301934507303383, 0.011454011332793133, 0.00965144716122575, 0.007845136117141499, 0.005915714662382942]\n",
      "current depth:  93\n",
      "vl:  [0.08155831841638188, 0.057359722698295576, 0.0023405741912092726, 0.009888860111328203, 0.000297636641670596, 0.0026058239049142557, 0.005967171067418116, 0.0008086158294396622, 0.0028972221235702158, 0.0011727919682250644, 0.0035209830776774718, 0.0008086158294396622, 0.0006021615438858776, 0.0027811610845727966, 0.006823482022831692, 0.0013474455511088856, 0.005039710939941871, 0.0003746232606243009, 0.000297636641670596, 0.003591083049637215, 0.0002291247454691225, 0.0023405741912092726, 0.04490956958029419]\n",
      "current depth:  94\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  95\n",
      "vl:  [1.0]\n",
      "current depth:  96\n",
      "vl:  [0.1857207293893467, 0.006031486952556862, 0.006031486952556862, 0.1074835602147594]\n",
      "current depth:  97\n",
      "vl:  [0.39632815712783614, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  98\n",
      "vl:  [0.1396618834407261, 0.1396618834407261]\n",
      "current depth:  99\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  100\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  101\n",
      "vl:  [0.04880068876220159, 0.08324234418491715, 0.19989968667103028, 0.12923753484913508, 0.08324234418491715, 0.04880068876220159]\n",
      "current depth:  102\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  103\n",
      "vl:  [0.11131292859758138, 0.029340994900501946, 0.07317342651141098, 0.01840970721714974, 0.0, 0.029340994900501946, 0.11131292859758138]\n",
      "current depth:  104\n",
      "vl:  [0.29956720366734785, 0.16341875261776725, 0.04879494069539858, 0.003321436156323081, 0.01790512783062725, 0.13889234683021912]\n",
      "current depth:  105\n",
      "vl:  [0.06761836267935474, 0.006031486952556862, 0.006031486952556862, 0.06761836267935474, 0.1074835602147594]\n",
      "current depth:  106\n",
      "vl:  [0.04646521764489747, 0.0, 0.04646521764489747, 0.4189939410656296]\n",
      "current depth:  107\n",
      "vl:  [0.32770706232704216, 0.1740193478031239, 0.08975463992235706]\n",
      "current depth:  108\n",
      "vl:  [1.0]\n",
      "current depth:  109\n",
      "vl:  [0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  110\n",
      "vl:  [1.0]\n",
      "current depth:  111\n",
      "vl:  [0.5678946871213929, 0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.31127812445913283]\n",
      "current depth:  112\n",
      "vl:  [0.18147620125065958, 0.03887502105131293, 0.06387040410564794, 0.13792538097003, 0.20490965566138727, 0.3345383316670124, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  113\n",
      "vl:  [1.0, 1.0]\n",
      "current depth:  114\n",
      "vl:  [0.08231985153261825, 0.013107492497438625, 0.018740337027706402, 0.03068579095793265, 0.03758184782085702, 0.054578998670749966, 0.06550784501721568, 0.03758184782085702, 0.03068579095793265, 0.024484933210301835, 0.018740337027706402, 0.013107492497438625, 0.018740337027706402]\n",
      "current depth:  115\n",
      "vl:  [0.1396618834407261, 0.31127812445913283]\n",
      "current depth:  116\n",
      "vl:  [0.36872135900975694, 0.03887502105131293, 0.06387040410564794, 0.20490965566138727, 0.13792538097003, 0.09500500259896874, 0.06387040410564794, 0.03887502105131293, 0.09500500259896874]\n",
      "current depth:  117\n",
      "vl:  [0.21142802859192256, 0.035892627884231906, 0.026501087386621498, 0.0011314390428371153, 0.0004750838548505314, 0.01654986256698211, 0.08216086067917011, 0.05572581616707231, 0.034583896956835725, 0.006179770723736218, 0.12866687041864272, 0.10410696679251233, 0.08096483286568819, 0.05784506765656581, 0.2137542834718897]\n",
      "current depth:  118\n",
      "vl:  [0.09862060778170113, 0.020246622059563374, 0.030413087345934543, 0.05418666832673801, 0.06952093867273432, 0.08880563947945669, 0.06952093867273432, 0.05418666832673801, 0.04144982886425281, 0.030413087345934543, 0.020246622059563374]\n",
      "current depth:  119\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.1396618834407261]\n",
      "current depth:  120\n",
      "vl:  [1.0]\n",
      "current depth:  121\n",
      "vl:  [0.6718700105021306, 0.18112206671815106, 0.02042632416598845, 0.12923753484913508, 0.00654650269402557, 0.28015270043979906]\n",
      "current depth:  122\n",
      "vl:  [0.22814608396437117, 0.014638089798646399, 0.03758891464919054, 0.001894368099625091, 0.01299810100075221, 0.032727248369402676, 0.10015300584040142, 0.07645364175044272, 0.02600866900982578, 0.06500303062583873, 0.01685932408212934, 0.010008745144475025, 0.19212097886551394, 0.0009799994598076586]\n",
      "current depth:  123\n",
      "vl:  [1.0]\n",
      "current depth:  124\n",
      "vl:  [0.2650621922278978, 0.9673183336217959, 0.2650621922278978]\n",
      "current depth:  125\n",
      "vl:  [0.0, 0.08170416594551044, 0.0, 0.2525355747672442, 0.2525355747672442]\n",
      "current depth:  126\n",
      "vl:  [0.02033163327920574, 0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  127\n",
      "vl:  [0.0, 0.35472437774145976]\n",
      "current depth:  128\n",
      "vl:  [0.2650621922278978]\n",
      "current depth:  129\n",
      "vl:  [0.08914841432492489, 4.321439369549142e-05, 0.0052414724546126605, 0.010068132866203379, 0.0001293166900281584, 0.0015085020979276843, 0.003925355357878909, 0.0014125866706859592, 0.00025593788194577913, 0.0014790283745211043, 0.006637729346751337, 0.0038081716466179883, 1.025205919724182e-06, 0.0014790283745211043, 0.00025593788194577913, 0.0014122435529148607, 0.011894346701497116, 0.007436095461039109, 0.004140697484148831, 0.0001293166900281584, 0.005454538047930771, 0.001886217819233869, 0.0021637833739358665, 0.007713557129005132, 0.0026194696426508008, 7.893321778618192e-05, 0.0017329763713131317, 0.011036169182915903, 0.03679284857620948, 0.022238263140437243, 0.07973348687787753, 0.051195445668940964]\n",
      "current depth:  130\n",
      "vl:  [0.4189939410656296, 0.04646521764489747, 0.0, 0.2650621922278978, 0.1444372797703026, 0.04646521764489747]\n",
      "current depth:  131\n",
      "vl:  [0.08975463992235706, 0.1740193478031239, 0.1740193478031239, 0.08975463992235706, 0.08975463992235706]\n",
      "current depth:  132\n",
      "vl:  [0.9673183336217959, 0.2650621922278978, 0.2650621922278978]\n",
      "current depth:  133\n",
      "vl:  [0.35472437774145976, 1.0, 0.35472437774145976, 0.35472437774145976]\n",
      "current depth:  134\n",
      "vl:  [0.14047499629754506, 0.0010659650610598374, 0.04320100006041985, 0.03578634272103474, 0.003691010892848074, 0.003691010892848074, 0.03578634272103474, 0.11850307846721093, 0.04320100006041985, 0.0010659650610598374, 0.14047499629754506, 0.017059384364149003]\n",
      "current depth:  135\n",
      "vl:  [0.1642830436827974, 0.2525355747672442, 0.09826454934165416, 0.020976263380628444, 0.0, 0.020976263380628444, 0.09826454934165416, 0.030904096500060884, 0.0, 0.1642830436827974, 0.030904096500060884]\n",
      "current depth:  136\n",
      "vl:  [0.00911437305383058, 0.006271075603255408, 0.04727298257039715, 0.12451124978365313, 0.26102902170468606, 0.09865444076641222, 0.00911437305383058, 0.13263705581879656, 0.21045859350620652]\n",
      "current depth:  137\n",
      "vl:  [0.3335952857724869, 0.19087450462110933, 0.11499831118948668, 0.06405612194077612, 0.06405612194077612]\n",
      "current depth:  138\n",
      "vl:  [1.0]\n",
      "current depth:  139\n",
      "vl:  [0.1396618834407261, 0.31127812445913283, 0.9245112497836533, 0.9245112497836533]\n",
      "current depth:  140\n",
      "vl:  [0.08728792846171006, 0.06500303062583873, 0.006704446094201026, 0.0394158831863924, 0.10015300584040142, 0.0612181055938347, 0.01299810100075221, 0.001894368099625091, 0.03758891464919054, 0.15817614246080478, 0.12256503176022776, 0.08728792846171006, 0.27396906204000565]\n",
      "current depth:  141\n",
      "vl:  [0.06923999653869647, 0.14077739888486904, 0.04424017696155053, 0.1371676772027261, 0.09363914620504095, 0.03398864924743632, 0.014603213499233988, 0.12876564130825247, 0.09865100594634758, 0.06923999653869647]\n",
      "current depth:  142\n",
      "vl:  [0.39632815712783614]\n",
      "current depth:  143\n",
      "vl:  [0.06405612194077612, 0.04900704072750519, 0.018139365349190032, 0.0, 0.004126846387264912, 0.08759794173047272, 0.06405612194077612, 0.04252438688979406]\n",
      "current depth:  144\n",
      "vl:  [0.11008497243260636, 0.026079279777968374, 0.002579146528037616, 0.026079279777968374, 0.09693457468543067]\n",
      "current depth:  145\n",
      "vl:  [0.4326479595302938, 0.06761836267935474, 0.006031486952556862, 0.06761836267935474]\n",
      "current depth:  146\n",
      "vl:  [0.06405612194077612, 0.19087450462110933, 0.3335952857724869]\n",
      "current depth:  147\n",
      "vl:  [0.2650621922278978, 0.9673183336217959]\n",
      "current depth:  148\n",
      "vl:  [0.1215042344189909, 0.012878775802370554, 0.005566002752151681, 0.0218531497079759, 0.015092388500114292, 0.002090880863272472, 0.01169274068482661, 0.0282716093567718, 0.008732096099167024, 0.000796980166438043, 0.024729996871962125, 0.008357772647312495, 0.027347471568835758, 0.009230400966805932, 0.0003353562483830987, 0.005566002752151681, 0.03914234439236314, 0.012878775802370554, 0.10905688765012257, 0.25832327657166604]\n",
      "current depth:  149\n",
      "vl:  [0.09424586253166718, 0.004288014491355668, 0.0069103049299918794, 0.010164948277620982, 0.0005614141439900361, 0.014503881500296746, 0.006175390197816655, 0.00020634253667313202, 0.012146214614021857, 0.002059001680907775, 0.023172957667532402, 0.007238121724252496, 8.543901623825849e-05, 0.0069103049299918794, 0.04450434928089019, 0.004288014491355668, 0.16129894007328135]\n",
      "current depth:  150\n",
      "vl:  [0.0874531077292179, 0.007657454841850576, 0.012517685605470192, 0.018696436805359134, 0.003629117727040704, 0.02718759723451945, 0.018385120552399863, 0.005278584526202198, 0.03411003466668784, 0.014943592248227559, 0.0668011715287747, 0.03881742693898308, 0.016228133983856783, 0.0016594679819640055, 0.007657454841850576, 0.0874531077292179]\n",
      "current depth:  151\n",
      "vl:  [0.1720827023145395, 0.051256867002420284, 0.0450029868398238, 0.01378090580310471, 0.04930445582025929, 0.03030514483932223, 0.01104889222972839, 0.04727298257039715, 0.02255184928323372, 0.08208161086595903, 0.04885904963406792, 0.021834240393222914, 0.0033227607981208018, 0.0055077079623934285, 0.09507636686268137]\n",
      "current depth:  152\n",
      "vl:  [0.010008745144475025, 0.01685932408212934, 0.0012297505299947705, 0.02600866900982578, 0.01617703842307018, 0.003447287770155632, 0.032727248369402676, 0.01299810100075221, 0.06709948575852374, 0.03758891464919054, 0.014638089798646399, 0.0009799994598076586, 0.010008745144475025, 0.08728792846171006]\n",
      "current depth:  153\n",
      "vl:  [0.0072953990280224335, 0.012471147310100805, 0.00015877214942946945, 0.019538390282887227, 0.00939281786187296, 0.0004714108476760307, 0.02142638837195634, 0.004987053402159863, 0.04902012774520869, 0.020747109935735, 0.0025190298819811018, 0.0072953990280224335, 0.18577740590888553]\n",
      "current depth:  154\n",
      "vl:  [0.013561092612075002, 0.023720771016214483, 0.0031344330674255745, 0.03822469823189057, 0.030247485498529632, 0.010257637084757874, 0.06665019121715889, 0.03548936959226058, 0.20366244598014902, 0.1619498622418021, 0.12377240478608444, 0.08669202239196727]\n",
      "current depth:  155\n",
      "vl:  [0.0010659650610598374, 0.0020461667745987904, 0.007067259725524144, 0.003691010892848074, 0.0020461667745987904, 0.04320100006041985, 0.0010659650610598374, 0.1764059496755188]\n",
      "current depth:  156\n",
      "vl:  [0.00539351844742539, 0.010793067371245064, 0.000596864170486781, 0.02072083962390814, 0.007963060143941176, 0.00539351844742539, 0.1270419185712244]\n",
      "current depth:  157\n",
      "vl:  [0.0017006663354044642, 0.003548762540498691, 0.007260555362580846, 0.007260555362580846, 0.0017006663354044642, 0.19655090395693994]\n",
      "current depth:  158\n",
      "vl:  [0.00911437305383058, 0.02033163327920574, 0.0, 0.04727298257039715, 0.13263705581879656]\n",
      "current depth:  159\n",
      "vl:  [0.003040199406150453, 0.0072535960017724105, 0.0072535960017724105, 0.019288288480290336]\n",
      "current depth:  160\n",
      "vl:  [0.0, 0.0, 0.2525355747672442]\n",
      "current depth:  161\n",
      "vl:  [0.02033163327920574, 0.21045859350620652]\n",
      "current depth:  162\n",
      "vl:  [0.0]\n",
      "currently pruning! \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input:\n",
    "    # Training set D \n",
    "    # Privacy budget B (epsilon)\n",
    "\n",
    "    # Test Set T (?)  (Maybe?)\n",
    "\n",
    "    # Number of attributes to be split used by each dec tree f \n",
    "    # Max tree depth d_m\n",
    "\n",
    "B = 0.2 # epsilon\n",
    "f = 3\n",
    "dm = 330\n",
    "\n",
    "feature_discrete_adult = {'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}\n",
    "diff_priv_forest = DPRF_Forest(adult2, adult23, D2[400:], B, f, dm, feature_discrete_adult)\n",
    "\n",
    "\n",
    "#feature_discrete_iris =  {'sepal length': False, 'sepal width': False, 'petal length': False, 'petal width': False}\n",
    "#diff_priv_forest = DPRF_Forest(irisdata, irisdata, Diris.astype('<U21'), B, f, dm, feature_discrete_iris)\n",
    "\n",
    "#num_trees = len(our_diff_priv_forest._trees)\n",
    "#av_prunings = np.mean([x._prunings for x in our_diff_priv_forest._trees])\n",
    "#av_tree_size = np.mean([x._id-x._prunings+1 for x in our_diff_priv_forest._trees])\n",
    "#accuracy, leafs_not_most_confident, votes_requiring_average = diff_priv_forest.evaluate_accuracy_with_voting(adult23, class_index=0) # confidence count includes all trees (before voting)\n",
    "#print(\"accuracy = {}\".format(accuracy))\n",
    "\n",
    "# feature discrete\n",
    "# {'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = D2[400:][:,-4]\n",
    "#unique, counts = np.unique(g, return_counts=True)\n",
    "#print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 0, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 0, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 1, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 0, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 1, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 1, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 1, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 0, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[0, 0, 0, 0]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "votes: \n",
      "[1, 1, 1, 1]\n",
      "data and trees looped through!\n",
      "<class 'int'>\n",
      "<class 'numpy.str_'>\n",
      "accuracy = 0.8182565789473685\n"
     ]
    }
   ],
   "source": [
    "accuracy = diff_priv_forest.evaluate_accuracy_with_voting(D2[400:][:,0:5], class_index=4) # confidence count includes all trees (before voting)\n",
    "print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something's not right with the voting system! \n",
    "#accuracy = diff_priv_forest.evaluate_accuracy_with_voting(Diris.astype('<U21'), class_index=4) # confidence count includes all trees (before voting)\n",
    "#print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history -g -f hybridTree2019.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e9830449175e83b6f3fc7dabdb3df0657dd9f8c8ad4abdd216dc6acf66ebbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
