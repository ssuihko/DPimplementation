{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/step-by-step-decision-tree-id3-algorithm-from-scratch-in-python-no-fancy-library-4822bbfdd88f\n",
    "# ID3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute vs Measure\n",
    "https://analystanswers.com/what-is-a-data-attribute-definition-types-examples/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DunZhang/DPTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diris=np.genfromtxt('./datafiles/iris_train.txt', dtype=str)\n",
    "Diris # all columns are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age          Workclass  Fnlwgt   Education  EducationNum  \\\n",
       "0   39          State-gov   77516   Bachelors            13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
       "2   38            Private  215646     HS-grad             9   \n",
       "3   53            Private  234721        11th             7   \n",
       "4   28            Private  338409   Bachelors            13   \n",
       "\n",
       "         MaritalStatus          Occupation    Relationship    Race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   CapitalGain  CapitalLoss  HoursPerWeek         Country  Income  \n",
       "0         2174            0            40   United-States       1  \n",
       "1            0            0            13   United-States       1  \n",
       "2            0            0            40   United-States       1  \n",
       "3            0            0            40   United-States       1  \n",
       "4            0            0            40            Cuba       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "row_names = [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"EducationNum\", \"MaritalStatus\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Gender\", \"CapitalGain\", \"CapitalLoss\",\n",
    "        \"HoursPerWeek\", \"Country\", \"Income\"]\n",
    "us_adult_income = pd.read_csv(dataset, names=row_names,na_values=[' ?'])\n",
    "us_adult_income[\"Income\"] = pd.Categorical(us_adult_income[\"Income\"])\n",
    "us_adult_income[\"Income\"] = us_adult_income[\"Income\"].cat.codes\n",
    "us_adult_income[\"Income\"] = 1 - us_adult_income[\"Income\"]\n",
    "\n",
    "us_adult_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "adult2 = us_adult_income.sample(frac=0.05, random_state=42)\n",
    "adult2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult25 = adult2.copy()\n",
    "adult25[\"Workclass\"] = pd.Categorical(adult25[\"Workclass\"])\n",
    "adult25[\"Workclass\"] = adult25[\"Workclass\"].cat.codes\n",
    "\n",
    "adult25[\"Education\"] = pd.Categorical(adult25[\"Education\"])\n",
    "adult25[\"Education\"] = adult25[\"Education\"].cat.codes\n",
    "\n",
    "adult25[\"MaritalStatus\"] = pd.Categorical(adult25[\"MaritalStatus\"])\n",
    "adult25[\"MaritalStatus\"] = adult25[\"MaritalStatus\"].cat.codes\n",
    "\n",
    "adult25[\"Occupation\"] = pd.Categorical(adult25[\"Occupation\"])\n",
    "adult25[\"Occupation\"] = adult25[\"Occupation\"].cat.codes\n",
    "\n",
    "adult25[\"Relationship\"] = pd.Categorical(adult25[\"Relationship\"])\n",
    "adult25[\"Relationship\"] = adult25[\"Relationship\"].cat.codes\n",
    "\n",
    "adult25[\"Race\"] = pd.Categorical(adult25[\"Race\"])\n",
    "adult25[\"Race\"] = adult25[\"Race\"].cat.codes\n",
    "\n",
    "adult25[\"Gender\"] = pd.Categorical(adult25[\"Gender\"])\n",
    "adult25[\"Gender\"] = adult25[\"Gender\"].cat.codes\n",
    "\n",
    "adult25[\"Country\"] = pd.Categorical(adult25[\"Country\"])\n",
    "adult25[\"Country\"] = adult25[\"Country\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17491</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18954</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15112</th>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1628 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Education  Occupation  Gender  Income\n",
       "14160   27         15           0       0       1\n",
       "27048   45         11           3       0       1\n",
       "28868   29          9           3       1       0\n",
       "5667    30          9           6       0       1\n",
       "7827    29         15           2       1       1\n",
       "...    ...        ...         ...     ...     ...\n",
       "17491   19         11          11       0       1\n",
       "18954   50          8           9       1       1\n",
       "15112   44         11           2       1       1\n",
       "5477    19         15           9       1       1\n",
       "1441    61         11          -1       1       1\n",
       "\n",
       "[1628 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult25[[\"Age\", \"Education\", \"Occupation\", \"Gender\", \"Income\"]] # Education, Occupation and Gender are discrete. \n",
    "adult23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(97)\n",
    "D2 = adult23.to_numpy()\n",
    "D2 = D2.astype(str)\n",
    "np.random.shuffle(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['50', '11', '3', '1', '0'],\n",
       "       ['29', '11', '3', '1', '1'],\n",
       "       ['24', '9', '9', '0', '1'],\n",
       "       ...,\n",
       "       ['47', '11', '0', '0', '1'],\n",
       "       ['46', '11', '11', '0', '1'],\n",
       "       ['55', '5', '13', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D4 = D2.astype(str)\n",
    "D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both test and training data have all classes in them\n",
    "# the code will not run if D88 and D99 do not print the same arrays\n",
    "D22 = D2[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D88 = D22[:len(D22)//2]\n",
    "np.sort(np.unique(D88[:,2]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D99 = D22[len(D22)//2:]\n",
    "np.sort(np.unique(D99[:,2]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D88 = D22[:len(D22)//2]\n",
    "np.sort(np.unique(D88[:,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D99 = D22[len(D22)//2:]\n",
    "np.sort(np.unique(D99[:,1]).astype(int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/loginaway/DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    '''\n",
    "    Basic element for decision tree.\n",
    "    Initialization: to initialize a tree node, at least feature_name is needed.\n",
    "    Connection: to connect different nodes, there are two situations.\n",
    "        1, if the parent node is discrete (i.e. node.discrete==True), \n",
    "            then \n",
    "            >> node[feature_value]=childnode \n",
    "            will do the connection.\n",
    "        2, if the parent node is continuous, then \n",
    "            >> node['<=']=childnode \n",
    "            or \n",
    "            >> node['>']=childnode \n",
    "            will assign its children. To be specific, \n",
    "            node['<=']=childnode means if the feature value is bigger than \n",
    "            node.threshold, then the workflow goes left to the childnode, and vice versa.\n",
    "    \n",
    "    Classification: if you have constructed a tree, you may want to classify a \n",
    "        certain group of data. Simply call the root node will do this task, i.e.\n",
    "        >> root(data)\n",
    "        where data should be a dictionary containing the corresponding key and value.\n",
    "        And this method will return the classification result.\n",
    "\n",
    "    Visualization: A naive visualization is implemented here. \n",
    "        For example, you may visualize a subtree rooted at 'node' by\n",
    "        >> print(node)\n",
    "        The tree structure will then be printed on the console.\n",
    "    '''\n",
    "    def __init__(self, feature_name='', discrete=True, threshold=0, depth=0, isLeaf=False, classification=None):\n",
    "        # for discrete node: the next node can be retrieved by self.map[feature_value]\n",
    "        # for continuous node: by self.map['<='] (when value <= self.threshold)\n",
    "        #                       by self.map['>'] (when value > self.threshold)\n",
    "\n",
    "        self.map=dict()\n",
    "\n",
    "        self.discrete=discrete\n",
    "        self.feature_name=feature_name\n",
    "        self.isLeaf=isLeaf\n",
    "        if isLeaf:\n",
    "            self.classification=classification\n",
    "        if not discrete:\n",
    "            self.threshold=threshold\n",
    "        self.depth=depth\n",
    "        self._class_counts = defaultdict(int)\n",
    "        self._noisy_class_counts = None\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.map[key]=value\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.map.get(key)\n",
    "\n",
    "    def __getdepth__(self):\n",
    "        return self.depth \n",
    "\n",
    "    def increment_class_count(self, class_value):\n",
    "        self._class_counts[str(class_value)] += 1\n",
    "\n",
    "    def __noisy_class_counts__(self, epsilon, class_values):\n",
    "        \n",
    "        ''' Add noise to the class counts of this leaf. Not performed on parents. '''\n",
    "        if not self._noisy_class_counts and not self._children: # to make sure this code is only run once per leaf\n",
    "            \n",
    "            counts = {}\n",
    "            for val in class_values:\n",
    "\n",
    "                if val in self._class_counts:\n",
    "                    # Laplace noise added to THE CLASS COUNTS\n",
    "                    counts[val] = max( 0, int(self._class_counts[val] + np.random.laplace(scale=float(1./epsilon))) )\n",
    "                else: # original count was 0\n",
    "                    counts[val] = max( 0, int(np.random.laplace(scale=float(1./epsilon))) )\n",
    "\n",
    "            self._noisy_class_counts = counts\n",
    "\n",
    "            # what I'm assuming is equation (10)\n",
    "            self._signal_to_noise = (epsilon * 1.0 * sum([v for k,v in counts.items()])) / (math.sqrt(2*1.0) * len(counts)) # enS / (sqrt(2n) * num_classes)\n",
    "         \n",
    "            #print(\"self._signal_to_noise = {}\".format(self._signal_to_noise))\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 # we're summing the redundant calls\n",
    "\n",
    "    def __call__(self, data): # returns the classification result! \n",
    "        \n",
    "        '''\n",
    "        This method should be used on the root to predict its classification.\n",
    "        data: a dict with its key being the features and value being \n",
    "        the corresponding value.\n",
    "        '''\n",
    "\n",
    "        # print(\"Inside __call__ method of Node\")\n",
    "\n",
    "        #print(\"data: \")\n",
    "        #print(data)\n",
    "\n",
    "        #print(\"feature name: \")\n",
    "        #print(self.feature_name)\n",
    "\n",
    "        #print(\"the map right now\")\n",
    "        #print(self.map)\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classification\n",
    "\n",
    "        # if the node has a discrete feature, use __getitem__ to find the next node\n",
    "        # then call the next node.\n",
    "        if self.discrete:\n",
    "            return self.map[data[self.feature_name]](data)\n",
    "\n",
    "        else:\n",
    "        # node is not discrete: print the self.feature_name\n",
    "            #print(\"continuous name\")\n",
    "            #print(self.feature_name)\n",
    "\n",
    "            if data[self.feature_name].astype(np.float32)>self.threshold:\n",
    "                return self.map['>'](data)\n",
    "            else:\n",
    "                return self.map['<='](data)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This method is designed for printing the subtree rooted at the current node.\n",
    "        To print the tree, try print(self).\n",
    "        '''\n",
    "        hierarchy={}\n",
    "        stack=[(0, 0, self)]\n",
    "        count=0\n",
    "        while stack:\n",
    "            # BFS\n",
    "            (layer_index, node_index, current)=stack.pop(0)\n",
    "            layer_index+=1\n",
    "\n",
    "            hierarchy[layer_index]=hierarchy.get(layer_index, [])\n",
    "            hierarchy[layer_index].append((node_index, current.feature_name,\\\n",
    "                    [(str(current.classification),)] if current.isLeaf else \\\n",
    "                     [(str(key), '' if current.discrete else str(current.threshold), item.feature_name) for key, item in current.map.items()]))\n",
    "            # print(hierarchy)\n",
    "\n",
    "            if current.isLeaf:\n",
    "                continue\n",
    "            for _, item in current.map.items():\n",
    "                count+=1\n",
    "                stack.append((layer_index, count, item))\n",
    "            # print(item_index_map)\n",
    "\n",
    "        totallist=[]\n",
    "        for layer, layer_content in hierarchy.items():\n",
    "            rowlist=[]\n",
    "            for i in layer_content:\n",
    "                rowlist.append('['+i[1]+']'+': {'+', '.join([' '.join(item) for item in i[2]])+'}')\n",
    "            rowstr=' +++ '.join(rowlist)\n",
    "            totallist.append(rowstr)\n",
    "\n",
    "        return '============Root===========\\n'+'\\n\\n'.join(totallist)+'\\n============Leaf==========='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '399']\n",
      " ['1' '1229']]\n"
     ]
    }
   ],
   "source": [
    "herlist = D2[:, 4]\n",
    "unique, counts = np.unique(herlist, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2333333333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = 5\n",
    "sum(2/(dm - i) for i in range(0, (dm - 2))) + (2 / (dm - (dm-1) + 2 )) + 1 # assuming we reach maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently includes methods as:\n",
    "        # _init_  : _trees\n",
    "        # get_domains\n",
    "        # reduce trees\n",
    "        # evaluate_accuracy_with_voting\n",
    "\n",
    "class DPRF_Forest:  \n",
    "    def __init__(self, \n",
    "                 training_nonprocessed,\n",
    "                 training_data, # 2D list of the training data where the columns are the attributes, and the first column is the class attribute\n",
    "                 # test_data, # T\n",
    "                 epsilon, # epsilon, Budget, B, the total privacy budget\n",
    "                 f, # n of attributes to be split used by each dctree f\n",
    "                 max_depth # max tree depth \n",
    "                 ):\n",
    "\n",
    "        # Forest = {}\n",
    "        self._trees = []\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "\n",
    "        ''' Some initialization '''\n",
    "        # Attribute set F\n",
    "        attribute_values = self.get_domains(training_data)\n",
    "\n",
    "        # Class values set\n",
    "        class_values = [str(y) for y in list(set([x[len(x)-1] for x in training_data]))]\n",
    "        attribute_indexes = [int(k) for k,v in attribute_values.items()]\n",
    "\n",
    "\n",
    "        #print(attribute_values)\n",
    "        #print(\"------------\")\n",
    "        #print(class_values)\n",
    "        #print(\"------------\")\n",
    "        #print(attribute_indexes)\n",
    "        #print(\"-----------\")\n",
    "\n",
    "        \n",
    "        # |D|\n",
    "        dataset_size = len(training_data)\n",
    "\n",
    "\n",
    "        valid_attribute_sizes = [[int(k),len(v)] for k,v in attribute_values.items()]\n",
    "\n",
    "        # Number of trees t\n",
    "        num_trees = len(attribute_indexes)\n",
    " \n",
    "\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] )\n",
    "\n",
    "        # e = B / t\n",
    "        epsilon_per_tree = epsilon / float(num_trees)\n",
    "\n",
    "        ''' Calculating minimum support threshold '''\n",
    "        # estimated_support_min_depth = dataset_size / (average_domain**2) # a large number\n",
    "        # estimated_support_max_depth = dataset_size / (average_domain ** (len(attribute_indexes)/2)) # max tree depth is k/2 # a small number\n",
    "        # epsilon_per_tree = epsilon / float(num_trees)\n",
    "        # min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER # the minimum support in order for S>N\n",
    "\n",
    "\n",
    "        # while estimated_support_max_depth > min_support: # then we can have more trees\n",
    "        #    num_trees += 1\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        # while min_support > estimated_support_min_depth and num_trees>1: # then we need to have less trees\n",
    "        #    num_trees, valid_attribute_sizes, estimated_support_min_depth = self.reduce_trees(num_trees, valid_attribute_sizes, dataset_size)\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        print(\"NUM TREES = {} & EPSILON PER TREE = {}\".format(num_trees, epsilon_per_tree))\n",
    "\n",
    "        # ? \n",
    "        root_attributes = []\n",
    "\n",
    "       \n",
    "        for a in attribute_indexes:\n",
    "            if a in [x[0] for x in valid_attribute_sizes]:\n",
    "                root_attributes.append(a) # OR: [index, support, gini]\n",
    "\n",
    "        print(\"root attributes\")\n",
    "        print(root_attributes)\n",
    "\n",
    "        # for treeId = 1,2 ... t do:\n",
    "        for t in range(num_trees):\n",
    "\n",
    "            if not root_attributes:\n",
    "                root_attributes = attribute_indexes[:]\n",
    "\n",
    "            # randomly extract |D| samples from D w/ a bagging algo?\n",
    "            root = random.choice(root_attributes)\n",
    "            root_attributes.remove(root)\n",
    "\n",
    "            print(\"what is root attributes: \")\n",
    "            print(root_attributes)\n",
    "\n",
    "\n",
    "            # TREE BUILDTREE(D, A, C, eps, f, Dm)\n",
    "            # D :\n",
    "\n",
    "            # attribute_indexes, - A_ind\n",
    "            # attribute_values, A - not exactly the same after all! \n",
    "            # root, \n",
    "            # class_values, \n",
    "            # feature_discrete\n",
    "            # treetype \n",
    "            # dataset_size, \n",
    "            # epsilon_per_tree\n",
    "\n",
    "            A={name: [i] for i, name in enumerate(adult23.columns[:-1])}\n",
    "\n",
    "            # {'Age': False, 'Education': True, 'Occupation': True, 'Gender': True, 'CapitalLoss': False}\n",
    "            feature_discrete={name: False if isinstance(training_nonprocessed[name].iloc[0], np.integer) else True for name in adult23.columns[:-1]}\n",
    "\n",
    "            #print(\"feature discrete\")\n",
    "            #print(feature_discrete)\n",
    "\n",
    "            #print(\"---------------\")\n",
    "            #print(\"A: \")\n",
    "            #print(A)\n",
    "            #print(\"---------------\")\n",
    "            #print(\"attribute values: \")\n",
    "            #print(attribute_values)\n",
    "            #print(\"---------------\")\n",
    "            #print(\"---------------\")\n",
    "\n",
    "            tree = Tree_DPDT(attribute_indexes, \n",
    "                            A, # original style A\n",
    "                            attribute_values, # new style attribute vals\n",
    "                            root, \n",
    "                            class_values, \n",
    "                            feature_discrete, \n",
    "                            'IGGR',  # treetype\n",
    "                            dataset_size, \n",
    "                            epsilon_per_tree,\n",
    "                            max_depth) \n",
    "\n",
    "            tree.train(training_data[:len(training_data)//2], 1)\n",
    "           \n",
    "            # TO EVOLVE HERE \n",
    "\n",
    "            # attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "            #num_unclassified = tree.filter_training_data_and_count(training_data, epsilon_per_tree, class_values)\n",
    "            \n",
    "            #if num_unclassified > 0:\n",
    "            #    print(\"number of unclassified records = {}\".format(num_unclassified))\n",
    "            \n",
    "\n",
    "            print(\"currently pruning! \")\n",
    "            # D2[:len(D2)//2], D2[len(D2)//2:]\n",
    "            tree.prune(training_data[:len(training_data)//2], training_data[len(training_data)//2:])\n",
    "\n",
    "            # FOREST = FOREST U TREE\n",
    "            self._trees.append(tree)\n",
    "\n",
    "        # print(self._trees)\n",
    "        # print(len(self._trees))\n",
    "        # print(\"succesfully ended building tree dictionary\")\n",
    "\n",
    "\n",
    "    def get_domains(self, data):\n",
    "        attr_domains = {}\n",
    "        transData = np.transpose(data)\n",
    "        for i in range(0,len(data[0]) - 1):\n",
    "            attr_domains[str(i)] = [str(x) for x in set(transData[i])]\n",
    "            print(\"original domain length of categorical attribute {}: {}\".format(i, len(attr_domains[str(i)])))\n",
    "        return attr_domains\n",
    "\n",
    "\n",
    "    def reduce_trees(self, num_trees, valid_attribute_sizes, dataset_size):\n",
    "        num_trees -= 1\n",
    "        largest_attribute = sorted(valid_attribute_sizes, key=lambda x:x[1], reverse=True)[0]\n",
    "        #print(\"Removing att{} with domain size {}\".format(largest_attribute[0], largest_attribute[1])) \n",
    "        new_valids = [ x for x in valid_attribute_sizes if x[0] != largest_attribute[0] ]\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] ) # average with all attributes\n",
    "        narrowed_average_domain = np.mean([x[1] for x in valid_attribute_sizes if x[0] in [y[0] for y in new_valids] ]) # average without the big attributes\n",
    "        estimated_support_squared = dataset_size / (average_domain * narrowed_average_domain)\n",
    "        return num_trees, new_valids, estimated_support_squared\n",
    "\n",
    "\n",
    "    # GET MAJORITY LABELS (Forest, T, C) equivalent from pseudocode\n",
    "    def evaluate_accuracy_with_voting(self, records, class_index=4):\n",
    "\n",
    "        ''' Calculate the Prediction Accuracy of the Forest. '''\n",
    "        actual_labels = [x[class_index] for x in records]\n",
    "\n",
    "        predicted_labels = []\n",
    "        leafs_not_used = 0\n",
    "        count_of_averages_used = 0\n",
    "\n",
    "        class_counts = defaultdict(list)\n",
    "\n",
    "        # FOR EACH RECORD X IN DATASET DO\n",
    "        # Strange for we need to access individual predictions here...\n",
    "        for rec in records:\n",
    "\n",
    "            h = []\n",
    "\n",
    "            class_value_fractions = defaultdict(list)\n",
    "\n",
    "            # FOR EACH TREE DO\n",
    "            for tree in self._trees:\n",
    "\n",
    "                # four trees\n",
    "                # <__main__.Tree_DPDT object at 0x7f299b73c580>, \n",
    "\n",
    "                # GET PREDICTED CLASSIFICATION RESULT for a single record\n",
    "                # node, leaf_not_used = tree._classify(tree._root_node, rec)\n",
    "\n",
    "                result = tree.pred(np.array([rec]))\n",
    "\n",
    "                h.append(*result)\n",
    "\n",
    "                #unique, counts = np.unique(h, return_counts=True)\n",
    "                #class_counts = dict(zip(unique, h))\n",
    "\n",
    "                #print(\"class counts\")\n",
    "                #print(class_counts)\n",
    "\n",
    "                #leafs_not_used += leaf_not_used\n",
    "\n",
    "                #support = float(sum([v for k,v in noisy_class_counts.items()]))\n",
    "\n",
    "                #for k,v in noisy_class_counts.items():\n",
    "                #    class_value_fractions[k].append(v/support)\n",
    "                    \n",
    "            #best_confidences = {}\n",
    "\n",
    "            #for k,lis in class_value_fractions.items():\n",
    "            #   best_confidences[k] = max(lis)\n",
    "            #best = [None, 0.0]\n",
    "\n",
    "            #for k,class_best in best_confidences.items():\n",
    "            #    if class_best > best[1]:\n",
    "            #        best = [k, class_best]\n",
    "            \n",
    "            # average_used = False\n",
    "\n",
    "            # majority vote\n",
    "            #for k, class_best in best_confidences.items():\n",
    "\n",
    "            #    if class_best == best[1] and k != best[0]:\n",
    "\n",
    "                    # average_used = True\n",
    "                   \n",
    "                    #orig_average = np.mean(class_value_fractions[best[0]])\n",
    "\n",
    "                    # contender_average = np.mean(class_value_fractions[k])\n",
    "            \n",
    "                    # if contender_average > orig_average:\n",
    "            #            best = [k, class_best]\n",
    "\n",
    "            # count_of_averages_used += 1 if average_used else 0\n",
    "\n",
    "            # majority vote\n",
    "\n",
    "            # IndexError: index 1 is out of bounds for axis 0 with size 1\n",
    "            hh = [int(*el) for el in h]\n",
    "\n",
    "            print(\"hh now contains: \")\n",
    "            print(hh)\n",
    "\n",
    "            c = Counter(hh)\n",
    "            ans = c.most_common(1)\n",
    "\n",
    "            predicted_labels.append(ans[0][0])\n",
    "\n",
    "        print(\"data and trees looped trhough!\")\n",
    "\n",
    "        # counts how many records we got right!\n",
    "\n",
    "        \n",
    "        print(type(predicted_labels[0]))\n",
    "        print(type(actual_labels[0]))\n",
    "\n",
    "        # print(zip(predicted_labels, actual_labels))\n",
    "\n",
    "        counts = Counter([x == y for x, y in zip(map(str, predicted_labels), actual_labels)])\n",
    "        \n",
    "        # acc = float(counts[True]) / len(records)\n",
    "        \n",
    "        return float(counts[True]) / len(records)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "#from Node import Node\n",
    "\n",
    "# set the maximal recursion limits here.\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# to be modified to single class\n",
    "\n",
    "# attribute_indexes, - A_ind\n",
    "# attribute_values, A\n",
    "# root, \n",
    "# class_values, \n",
    "# feature_discrete\n",
    "# treetype \n",
    "# dataset_size, \n",
    "# epsilon_per_tree\n",
    "\n",
    "\n",
    "class Tree_DPDT(DPRF_Forest):\n",
    "\n",
    "    '''\n",
    "    The main class of decision tree.\n",
    "    '''\n",
    "    def __init__(self, A_ind, A, attribute_values, root, class_values, feature_discrete, treetype, dataset_size, epsilon_per_tree, max_depth):\n",
    "        '''\n",
    "        attribute_values : attribute_values\n",
    "        A : orignal\n",
    "        feature_discrete: a dict with its each key-value pair being (feature_name: True/False),\n",
    "            where True means the feature is discrete and False means the feature is \n",
    "            continuous. \n",
    "        type: ID3/C4.5/CART\n",
    "        pruning: pre/post\n",
    "        '''\n",
    "\n",
    "        self.A=A\n",
    "        self.attribute_values=attribute_values\n",
    "        self.A_ind=A_ind\n",
    "        self.feature_discrete= feature_discrete\n",
    "        self.treeType=treetype\n",
    "        self.leaf_count=0\n",
    "        self.tmp_classification=''\n",
    "        self.class_values=class_values\n",
    "        self._root_node = root\n",
    "        self.tree=None\n",
    "        self.dataset_size=dataset_size\n",
    "        self.epsilon=epsilon_per_tree\n",
    "        self.dm = max_depth\n",
    "        self.current_IGGR = 1\n",
    "\n",
    "        self.eu = epsilon_per_tree / sum(2/(self.dm - i) for i in range(0, (self.dm - 2))) + (2 / (self.dm - (self.dm-1) + 2 )) + 1\n",
    "        self.ei = self.eu\n",
    "        self.ei1 = 0 \n",
    "        self.ei2 = self.ei\n",
    "\n",
    "    def Entropy(self, list_of_class):\n",
    "        '''\n",
    "        Compute the entropy for the given list of class.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        'duck': 2/3, 'dolphin': 1/3, so the entropy for this array is 0.918\n",
    "        '''\n",
    "        count={}\n",
    "        for key in list_of_class:\n",
    "            count[key]=count.get(key, 0)+1\n",
    "\n",
    "            \n",
    "        frequency=np.array(tuple(count.values()))/len(list_of_class)\n",
    "        return -1*np.vdot(frequency, np.log2(frequency))\n",
    "\n",
    "    def Information_Gain(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Gain.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Gain for this example is 0.2516.\n",
    "        '''\n",
    "        sec2=np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        return self.Entropy(list_of_class)-sec2\n",
    "\n",
    "\n",
    "    def Information_Ratio(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Ratio.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Ratio for this example is 0.2740.\n",
    "        '''\n",
    "        tmp=np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        # Here we assume instrinsic_value is SplitInformation! \n",
    "        intrinsic_value=-1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        return self.Information_Gain(list_of_class, grouped_list_of_class) / intrinsic_value\n",
    "\n",
    "    def IGGR(self, list_of_class, grouped_list_of_class, a1, a2, eps, curIGGR):\n",
    "\n",
    "        #print(\"eps at IGGR: \", eps)\n",
    "\n",
    "        def infoGainAndGainRatio(list_of_class, grouped_list_of_class):\n",
    "            sec2 = np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "            infoGain =  self.Entropy(list_of_class)-sec2\n",
    "            tmp = np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "            intrinsic_value = -1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "            GainRatio = self.Information_Gain(list_of_class, grouped_list_of_class)/ intrinsic_value\n",
    "\n",
    "            return infoGain, GainRatio, intrinsic_value \n",
    "\n",
    "        infoGainRes, GainRatioRes, intrinsic_value_res = infoGainAndGainRatio(list_of_class, grouped_list_of_class)\n",
    "\n",
    "        sol1 = a1 * infoGainRes + a2 * GainRatioRes\n",
    "        sol2 = (a1 + ( a2 / intrinsic_value_res) ) * infoGainRes \n",
    "\n",
    "        # print(\"relevant data for sensitivity\")\n",
    "        # print(list_of_class) classes \n",
    "        # print(grouped_list_of_class)\n",
    "        # print(curIGGR, \" and \", sol2)\n",
    "\n",
    "        sens_delta = 1 # ?\n",
    "\n",
    "        exp_noise = np.random.exponential( (eps * abs(sol2)) / 2 * sens_delta * abs(curIGGR) )\n",
    "\n",
    "        #exp_n = 6\n",
    "\n",
    "        # return sol2, exp_noise\n",
    "        return exp_noise\n",
    "\n",
    "\n",
    "    def orderByIGGR(self, D, A, a1, a2, eps):\n",
    "\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "\n",
    "        # print(\"epsilon at IGGR order: \", eps)\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        # target_function1 = self.Information_Gain\n",
    "        # target_function2 = self.Information_Ratio\n",
    "        # target_function3 = self.IGGR\n",
    "        \n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "                for val in possibleVal:\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "                IC_value = self.IGGR( D[:, -1], grouped_list_of_class, a1, a2, eps, self.current_IGGR )\n",
    "                self.current_IGGR = IC_value\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "\n",
    "                # continuous\n",
    "\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "                for point in split_points:\n",
    "\n",
    "                    # modified now to float comparisons\n",
    "                    # causes the code to crash to max recursion\n",
    "                    # smaller_set = D[np.argwhere(D[:, info[0]].astype(float) <= point ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    # bigger_set = D[np.argwhere(D[:, info[0]].astype(float) > point ), -1].flatten()\n",
    "                    \n",
    "                    smaller_set = D[np.argwhere(D[:, info[0]] <= str(point) ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    bigger_set = D[np.argwhere(D[:, info[0]] > str(point) ), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    # list_of_class, grouped_list_of_class, a1, a2\n",
    "                    IC_tmp = self.IGGR( D[:, -1], (smaller_set, bigger_set), a1, a2, eps, self.current_IGGR )\n",
    "                    self.current_IGGR = IC_tmp\n",
    "\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                    #print(\"attribute was: \", attr)\n",
    "                    #print(\"sets (small and big)\")\n",
    "                    #print(smaller_set)\n",
    "                    #print(bigger_set)\n",
    "\n",
    "                    # IC_tmp=0.0\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def chooseAttribute(self, D, A, eps):\n",
    "\n",
    "        '''\n",
    "        Choose an attribute from A according to the metrics above.\n",
    "        For the definition of D and A, see method 'fit'.\n",
    "        Different principal for different tree types:\n",
    "        ID3: choose the attribute that maximizes the Information Gain.\n",
    "        C4.5: \n",
    "            1, choose those attributes whose Information Gain are above average.\n",
    "            2, choose the one that maximizes the Gain Ratio from these attributes.\n",
    "        CART: choose the attribute that minimizes the Gini Index.\n",
    "        IG_GR: \n",
    "        '''\n",
    "\n",
    "        # print(\"epsilon passed: \", eps)\n",
    "\n",
    "        if self.treeType=='ID3':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='C4.5':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "\n",
    "            # for C4.5, we choose the attributes whose Gain are above average\n",
    "            # and then order them by Ratio.\n",
    "\n",
    "            sub_A={key: A[key] for key in attr_list}\n",
    "            attr_list=self.orderByGainOrRatio(D, sub_A, by='Ratio')\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='IGGR':\n",
    "\n",
    "            attr_list=self.orderByIGGR(D, A, 0.4, 0.6, eps)\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "    # tree.train(training_data[:len(training_data)//2], 1)\n",
    "    def train(self, D, depth):\n",
    "        self.tree=self.fit(D, self.A, depth)\n",
    "\n",
    "    def fit(self, D, A, depth):\n",
    "\n",
    "        '''\n",
    "        Train the tree.\n",
    "        To save the training result:\n",
    "        >> self.tree=self.fit(D, A)\n",
    "        D: the training set, a size [m, n+1] numpy array (with str type elements), \n",
    "            where m is the number of training data and n is the number of attributes.\n",
    "            The last column of D is the classifications (or labels).\n",
    "        A: the attributes set. It is a dict with its structure being like \n",
    "            {attr_name: [index_in_D_columns, possibleVal_or_threshold], ...}\n",
    "            attr_name: name of the attribute\n",
    "            index_in_D_columns: the corresponding index of the attribute in ndarray D (starting from 0)\n",
    "            possibleVal_or_threshold: \n",
    "                ###################################################\n",
    "                ## This value may not always be available in A   ##\n",
    "                ## it is added after 'chooseAttribute' is called ##\n",
    "                ## And it will be updated after each call        ##\n",
    "                ###################################################\n",
    "                1, if the attribute is discrete, then it is a ndarray containing all possible values \n",
    "                    of this attribute.\n",
    "                2, if the attribute is continuous, then possibleVal_or_threshold is the most recent \n",
    "                    threshold.\n",
    "        '''\n",
    "\n",
    "        # termination conditions \n",
    "\n",
    "        # the training set is empty\n",
    "        if len(D)==0:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=depth, isLeaf=True, \\\n",
    "                classification=self.tmp_classification)\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        # only one type of classification is left \n",
    "        if len(np.unique(D[:, -1]))<=1:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=depth, isLeaf=True, \\\n",
    "                classification=D[0, -1])\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(A)==0 or len(np.unique(D[:, :-1], axis=0))<=1:\n",
    "\n",
    "            count_dict={}\n",
    "\n",
    "            for key in D[:, -1]:\n",
    "\n",
    "                count_dict[key]=count_dict.get(key, 0)+1\n",
    "\n",
    "            most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), depth=depth, isLeaf=True, \\\n",
    "                classification=most_frequent)\n",
    "\n",
    "            self.leaf_count+=1\n",
    "            return node \n",
    "\n",
    "        count_dict={}\n",
    "\n",
    "        # the count query usese the Laplace mechanism to add noise to the class count\n",
    "        for key in D[:, -1]:\n",
    "            count_dict[key] = count_dict.get(key, 0)+1 + np.random.laplace(scale=1/self.eu)\n",
    "\n",
    "        print(\"count dictionary: \")\n",
    "        print(count_dict)\n",
    "        # {'1': 449, '0': 162}\n",
    "        # {'0': 4, '1': 5}\n",
    "        \n",
    "        most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "        print(\"most frequent : \")\n",
    "        print(most_frequent)\n",
    "        # count dictionary: \n",
    "        #{'0': 4, '1': 1}\n",
    "        # most frequent : \n",
    "        # 0\n",
    "\n",
    "        self.tmp_classification=most_frequent\n",
    "\n",
    "        # choose target attribute\n",
    "        target_attr=self.chooseAttribute(D, A, self.eu)\n",
    "        # print(target_attr)\n",
    "\n",
    "\n",
    "        # generate nodes for each possible values of the target attribute if it's discrete\n",
    "        # related information is stored in A[target_attr][1] now, \n",
    "        # since we have called chooseAttribute at least once.\n",
    "        info=A[target_attr]\n",
    "\n",
    "        if self.feature_discrete[target_attr]:\n",
    "\n",
    "            node=Node(feature_name=target_attr, discrete=True, depth=depth, isLeaf=False)\n",
    "            # generate nodes for each possible value\n",
    "            \n",
    "            for possibleVal in info[1]:\n",
    "\n",
    "                keys=set(A.keys()).difference({target_attr})\n",
    "                # connect node to its child\n",
    "                tmp_D=D[np.argwhere(D[:, info[0]]==possibleVal), :]\n",
    "                tmp_A={key: A[key] for key in keys}\n",
    "\n",
    "                node[possibleVal] = self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), tmp_A, depth+1)\n",
    "        \n",
    "        else:\n",
    "            # generate two nodes for the two classification if it's continuous\n",
    "            # continuous\n",
    "            threshold=info[1]\n",
    "            node=Node(feature_name=target_attr, discrete=False, threshold=threshold, depth=depth, isLeaf=False)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]<=str(threshold)), :]\n",
    "            node['<=']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A, depth+1)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]>str(threshold)), :]\n",
    "            node['>']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A, depth+1)\n",
    "            \n",
    "        \n",
    "        return node\n",
    "\n",
    "    def prune(self, training_D, testing_D):\n",
    "        self.post_prune(training_D, testing_D, self.A, current=self.tree)\n",
    "\n",
    "    def post_prune(self, training_D, testing_D, A, current=None, parent=None):\n",
    "    \n",
    "        '''\n",
    "        self.tree is required.\n",
    "        This method conducts the post-pruning to enhance the model performance.\n",
    "        To make sure this method will work, set \n",
    "        >> current=self.tree\n",
    "        when you call it.\n",
    "        '''\n",
    "\n",
    "        self.current_accuracy=self.evaluate(testing_D, A)\n",
    "\n",
    "        count_dict={}\n",
    "        if len(training_D)==0:\n",
    "            return \n",
    "\n",
    "        for key in training_D[:, -1]:\n",
    "            count_dict[key] = count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(training_D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "        leaf_parent=True\n",
    "        for key, node in current.map.items():\n",
    "            if not node.isLeaf:\n",
    "                leaf_parent=False\n",
    "\n",
    "                # Recursion, DFS\n",
    "                if node.discrete:\n",
    "                    tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]==key), :]\n",
    "                else:\n",
    "                    if key=='<=':\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]<=str(node.threshold)), :]\n",
    "                    else:\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]>str(node.threshold)), :]\n",
    "                self.post_prune(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), testing_D, A, parent=current, current=node)\n",
    "        \n",
    "        tmp_node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, classification=most_frequent)\n",
    "        \n",
    "        if parent:\n",
    "            # when current node is not the root\n",
    "            for key, node in parent.map.items():\n",
    "                if node==current:\n",
    "                    parent.map[key]=tmp_node\n",
    "                    saved_key=key\n",
    "                    break\n",
    "\n",
    "            # compare the evaluation, if it is enhanced then prune the tree.\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                parent.map[saved_key]=current\n",
    "                \n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "        else:\n",
    "\n",
    "            # when current node is the root\n",
    "            saved_tree=self.tree\n",
    "            self.tree=tmp_node\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                self.tree=saved_tree\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "\n",
    "    def pred(self, D):\n",
    "\n",
    "        return self.predict(D, self.A)\n",
    "    \n",
    "    def eval(self, D):\n",
    "        return self.evaluate(D, self.A)\n",
    "\n",
    "    def predict(self, D, A):\n",
    "        '''\n",
    "        Predict the classification for the data in D.\n",
    "        For the definition of A, see method 'fit'. \n",
    "        There is one critical difference between D and that defined in 'fit':\n",
    "            the last column may or may not be the labels. \n",
    "            This method works as long as the feature index in A matches the corresponding\n",
    "            column in D.\n",
    "        '''\n",
    "\n",
    "        # why the loop can not be dismissed?\n",
    "        row, _= D.shape # for the entire testing data \n",
    "        pred = np.empty((row, 1), dtype=str)\n",
    "        tmp_data={key: None for key in A.keys()}\n",
    "\n",
    "        # print(\"the tree is a: \", type(self.tree))\n",
    "        # the tree is a:  <class '__main__.Node'>\n",
    "\n",
    "        for i in range(len(D)): # only 1 row?\n",
    "            print(\"i: \", i)\n",
    "            for key, info in A.items():\n",
    "\n",
    "                tmp_data[key] = D[i, info[0]]\n",
    "\n",
    "            #print(\"data given to tree: \")\n",
    "            #print(tmp_data)\n",
    "            # {'Age': '20', 'Education': '8', 'Occupation': '4', 'Gender': '1'}\n",
    "\n",
    "            # but self.tree is evidently initialized as none?\n",
    "\n",
    "            pred[i] = self.tree(tmp_data)\n",
    "        \n",
    "      \n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, testing_D, A):\n",
    "        '''\n",
    "        Evaluate the performance of decision tree. (Accuracy)\n",
    "        For definition of testing_D and A, see 'predict'.\n",
    "        However, here the testing_D is required to be labelled, that is, its last column \n",
    "        should be labels of the data.\n",
    "        '''\n",
    "        true_label=testing_D[:, -1]\n",
    "        pred_label=self.predict(testing_D, A)\n",
    "        \n",
    "        success_count=0\n",
    "        for i in range(len(true_label)):\n",
    "            if true_label[i]==pred_label[i]:\n",
    "                success_count+=1\n",
    "\n",
    "        return success_count/len(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input:\n",
    "    # Training set D \n",
    "    # Privacy budget B (epsilon)\n",
    "\n",
    "    # Test Set T (?)  (Maybe?)\n",
    "\n",
    "    # Number of attributes to be split used by each dec tree f \n",
    "    # Max tree depth d_m\n",
    "\n",
    "B = 0.2 # epsilon\n",
    "f = 3\n",
    "dm = 5\n",
    "\n",
    "diff_priv_forest = DPRF_Forest(adult2, D2[400:], B, f, dm)\n",
    "\n",
    "#num_trees = len(our_diff_priv_forest._trees)\n",
    "#av_prunings = np.mean([x._prunings for x in our_diff_priv_forest._trees])\n",
    "#av_tree_size = np.mean([x._id-x._prunings+1 for x in our_diff_priv_forest._trees])\n",
    "#accuracy, leafs_not_most_confident, votes_requiring_average = diff_priv_forest.evaluate_accuracy_with_voting(adult23, class_index=0) # confidence count includes all trees (before voting)\n",
    "#print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '308']\n",
      " ['1' '920']]\n"
     ]
    }
   ],
   "source": [
    "g = D2[400:][:,-1]\n",
    "unique, counts = np.unique(g, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = diff_priv_forest.evaluate_accuracy_with_voting(D2[400:][:,0:5], class_index=4) # confidence count includes all trees (before voting)\n",
    "print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f hybridTree2019.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e9830449175e83b6f3fc7dabdb3df0657dd9f8c8ad4abdd216dc6acf66ebbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
