{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/step-by-step-decision-tree-id3-algorithm-from-scratch-in-python-no-fancy-library-4822bbfdd88f\n",
    "# ID3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute vs Measure\n",
    "https://analystanswers.com/what-is-a-data-attribute-definition-types-examples/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy quantifies how much information there is in a random variable, or more specifically its probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_m = pd.read_csv(\"../datafiles/PlayTennis.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play Tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook Temperature Humidity    Wind Play Tennis\n",
       "0     Sunny         Hot     High    Weak          No\n",
       "1     Sunny         Hot     High  Strong          No\n",
       "2  Overcast         Hot     High    Weak         Yes\n",
       "3      Rain        Mild     High    Weak         Yes\n",
       "4      Rain        Cool   Normal    Weak         Yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sunny', 'Overcast', 'Rain'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_m[\"Outlook\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play Tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Temperature Humidity    Wind Play Tennis\n",
       "2   Overcast         Hot     High    Weak         Yes\n",
       "6   Overcast        Cool   Normal  Strong         Yes\n",
       "11  Overcast        Mild     High  Strong         Yes\n",
       "12  Overcast         Hot   Normal    Weak         Yes"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_m[train_data_m[\"Outlook\"] == \"Overcast\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GainRatio is used to select the node splitting attribute and the splitting point of the continuous attribute value in the C4.5 algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A_i is an attribute in the attribute set A to be split, a_1 and a_2 are weight coefficients assigned to the information gain and the gainratio: 0 <= a_1, a_2 <= 1, and a_1 + a_2 = 1.\n",
    "\n",
    "IG_GR(Data, A_i) = (a_1 + a_2 / SplitInformation(D, A_i)) * InfoGain(D, A_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C4.5 \n",
    "https://github.com/barisesmer/C4.5/tree/master/c45\n",
    "\n",
    "https://github.com/Valdecy/C4.5/blob/master/Python-DM-Classification-04-C4.5.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/loginaway/DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    '''\n",
    "    Basic element for decision tree.\n",
    "    Initialization: to initialize a tree node, at least feature_name is needed.\n",
    "    Connection: to connect different nodes, there are two situations.\n",
    "        1, if the parent node is discrete (i.e. node.discrete==True), \n",
    "            then \n",
    "            >> node[feature_value]=childnode \n",
    "            will do the connection.\n",
    "        2, if the parent node is continuous, then \n",
    "            >> node['<=']=childnode \n",
    "            or \n",
    "            >> node['>']=childnode \n",
    "            will assign its children. To be specific, \n",
    "            node['<=']=childnode means if the feature value is bigger than \n",
    "            node.threshold, then the workflow goes left to the childnode, and vice versa.\n",
    "    \n",
    "    Classification: if you have constructed a tree, you may want to classify a \n",
    "        certain group of data. Simply call the root node will do this task, i.e.\n",
    "        >> root(data)\n",
    "        where data should be a dictionary containing the corresponding key and value.\n",
    "        And this method will return the classification result.\n",
    "    Visualization: A naive visualization is implemented here. \n",
    "        For example, you may visualize a subtree rooted at 'node' by\n",
    "        >> print(node)\n",
    "        The tree structure will then be printed on the console.\n",
    "    '''\n",
    "    def __init__(self, feature_name='', discrete=True, threshold=0, isLeaf=False, classification=None):\n",
    "        # for discrete node: the next node can be retrieved by self.map[feature_value]\n",
    "        # for continuous node: by self.map['<='] (when value <= self.threshold)\n",
    "        #                       by self.map['>'] (when value > self.threshold)\n",
    "        self.map=dict()\n",
    "\n",
    "        self.discrete=discrete\n",
    "        self.feature_name=feature_name\n",
    "        self.isLeaf=isLeaf\n",
    "        if isLeaf:\n",
    "            self.classification=classification\n",
    "        if not discrete:\n",
    "            self.threshold=threshold\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.map[key]=value\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.map.get(key)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        '''\n",
    "        This method should be used on the root to predict its classification.\n",
    "        data: a dict with its key being the features and value being \n",
    "        the corresponding value.\n",
    "        '''\n",
    "\n",
    "        #print(\"Inside __call__ method of Node\")\n",
    "\n",
    "        #print(\"data: \")\n",
    "        #print(data)\n",
    "\n",
    "        #print(\"feature name: \")\n",
    "        #print(self.feature_name)\n",
    "\n",
    "        #print(\"the map right now\")\n",
    "        #print(self.map)\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classification\n",
    "        # if the node has a discrete feature, use __getitem__ to find the next node\n",
    "        # then call the next node.\n",
    "        if self.discrete:\n",
    "            # print(\"feature was discrete!\")\n",
    "            \n",
    "            return self.map[data[self.feature_name]](data)\n",
    "        else:\n",
    "        # node is not discrete: print the self.feature_name\n",
    "            #print(\"continuous name\")\n",
    "            #print(self.feature_name)\n",
    "            if data[self.feature_name].astype(np.float32)>self.threshold:\n",
    "                return self.map['>'](data)\n",
    "            else:\n",
    "                return self.map['<='](data)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This method is designed for printing the subtree rooted at the current node.\n",
    "        To print the tree, try print(self).\n",
    "        '''\n",
    "        hierarchy={}\n",
    "        stack=[(0, 0, self)]\n",
    "        count=0\n",
    "        while stack:\n",
    "            # BFS\n",
    "            (layer_index, node_index, current)=stack.pop(0)\n",
    "            layer_index+=1\n",
    "\n",
    "            hierarchy[layer_index]=hierarchy.get(layer_index, [])\n",
    "            hierarchy[layer_index].append((node_index, current.feature_name,\\\n",
    "                    [(str(current.classification),)] if current.isLeaf else \\\n",
    "                     [(str(key), '' if current.discrete else str(current.threshold), item.feature_name) for key, item in current.map.items()]))\n",
    "            # print(hierarchy)\n",
    "\n",
    "            if current.isLeaf:\n",
    "                continue\n",
    "            for _, item in current.map.items():\n",
    "                count+=1\n",
    "                stack.append((layer_index, count, item))\n",
    "            # print(item_index_map)\n",
    "\n",
    "        totallist=[]\n",
    "        for layer, layer_content in hierarchy.items():\n",
    "            rowlist=[]\n",
    "            for i in layer_content:\n",
    "                rowlist.append('['+i[1]+']'+': {'+', '.join([' '.join(item) for item in i[2]])+'}')\n",
    "            rowstr=' +++ '.join(rowlist)\n",
    "            totallist.append(rowstr)\n",
    "\n",
    "        return '============Root===========\\n'+'\\n\\n'.join(totallist)+'\\n============Leaf==========='"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today: the tree \"works\", or some approximation of it? and With Adult Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': [0], 'Education': [1], 'Occupation': [2], 'Gender': [3]}\n",
      "============Root===========\n",
      "[Age]: {<= 23.5 leaf-0, > 23.5 Age}\n",
      "\n",
      "[leaf-0]: {1} +++ [Age]: {<= 71.0 Education, > 71.0 leaf-386}\n",
      "\n",
      "[Education]: {0  Age, 1  Occupation, 10  Age, 11  Age, 12  Age, 14  Occupation, 15  Gender, 2  Occupation, 3  leaf-243, 4  Age, 5  Age, 6  Occupation, 7  Gender, 8  Age, 9  Age} +++ [leaf-386]: {1}\n",
      "\n",
      "[Age]: {<= 52.0 leaf-1, > 52.0 Age} +++ [Occupation]: {-1  leaf-4, 0  leaf-5, 10  leaf-6, 11  leaf-7, 12  leaf-8, 13  leaf-9, 2  Age, 3  leaf-12, 4  leaf-13, 5  Age, 6  leaf-17, 7  leaf-18, 8  leaf-19, 9  leaf-20} +++ [Age]: {<= 36.5 leaf-21, > 36.5 Gender} +++ [Age]: {<= 25.5 leaf-25, > 25.5 Age} +++ [Age]: {<= 33.5 leaf-113, > 33.5 Age} +++ [Occupation]: {-1  leaf-154, 0  leaf-155, 10  leaf-156, 11  leaf-157, 12  leaf-158, 13  leaf-159, 2  leaf-160, 3  leaf-161, 4  leaf-162, 5  leaf-163, 6  leaf-164, 7  leaf-165, 8  leaf-166, 9  Age} +++ [Gender]: {0  Age, 1  Age} +++ [Occupation]: {-1  leaf-229, 0  leaf-230, 10  leaf-231, 11  leaf-232, 12  leaf-233, 13  leaf-234, 2  leaf-235, 3  leaf-236, 4  leaf-237, 5  leaf-238, 6  leaf-239, 7  leaf-240, 8  leaf-241, 9  leaf-242} +++ [leaf-243]: {1} +++ [Age]: {<= 49.5 leaf-244, > 49.5 Age} +++ [Age]: {<= 55.5 leaf-247, > 55.5 Age} +++ [Occupation]: {-1  leaf-250, 0  leaf-251, 10  leaf-252, 11  leaf-253, 12  leaf-254, 13  leaf-255, 2  leaf-256, 3  leaf-257, 4  leaf-258, 5  leaf-259, 6  leaf-260, 7  leaf-261, 8  leaf-262, 9  leaf-263} +++ [Gender]: {0  leaf-264, 1  Occupation} +++ [Age]: {<= 33.5 leaf-281, > 33.5 Age} +++ [Age]: {<= 28.5 Gender, > 28.5 Age}\n",
      "\n",
      "[leaf-1]: {1} +++ [Age]: {<= 53.5 leaf-2, > 53.5 leaf-3} +++ [leaf-4]: {1} +++ [leaf-5]: {1} +++ [leaf-6]: {1} +++ [leaf-7]: {1} +++ [leaf-8]: {1} +++ [leaf-9]: {1} +++ [Age]: {<= 32.5 leaf-10, > 32.5 leaf-11} +++ [leaf-12]: {0} +++ [leaf-13]: {1} +++ [Age]: {<= 35.5 leaf-14, > 35.5 Age} +++ [leaf-17]: {1} +++ [leaf-18]: {1} +++ [leaf-19]: {1} +++ [leaf-20]: {0} +++ [leaf-21]: {1} +++ [Gender]: {0  Age, 1  leaf-24} +++ [leaf-25]: {1} +++ [Age]: {<= 66.5 Age, > 66.5 leaf-112} +++ [leaf-113]: {1} +++ [Age]: {<= 35.5 leaf-114, > 35.5 Gender} +++ [leaf-154]: {0} +++ [leaf-155]: {0} +++ [leaf-156]: {0} +++ [leaf-157]: {1} +++ [leaf-158]: {0} +++ [leaf-159]: {0} +++ [leaf-160]: {0} +++ [leaf-161]: {0} +++ [leaf-162]: {0} +++ [leaf-163]: {0} +++ [leaf-164]: {0} +++ [leaf-165]: {0} +++ [leaf-166]: {0} +++ [Age]: {<= 46.5 leaf-167, > 46.5 Age} +++ [Age]: {<= 29.5 Occupation, > 29.5 Age} +++ [Age]: {<= 24.5 leaf-190, > 24.5 Age} +++ [leaf-229]: {1} +++ [leaf-230]: {1} +++ [leaf-231]: {1} +++ [leaf-232]: {1} +++ [leaf-233]: {1} +++ [leaf-234]: {1} +++ [leaf-235]: {0} +++ [leaf-236]: {1} +++ [leaf-237]: {1} +++ [leaf-238]: {1} +++ [leaf-239]: {1} +++ [leaf-240]: {1} +++ [leaf-241]: {1} +++ [leaf-242]: {1} +++ [leaf-244]: {1} +++ [Age]: {<= 53.5 leaf-245, > 53.5 leaf-246} +++ [leaf-247]: {1} +++ [Age]: {<= 57.0 leaf-248, > 57.0 leaf-249} +++ [leaf-250]: {1} +++ [leaf-251]: {0} +++ [leaf-252]: {1} +++ [leaf-253]: {1} +++ [leaf-254]: {1} +++ [leaf-255]: {1} +++ [leaf-256]: {1} +++ [leaf-257]: {1} +++ [leaf-258]: {1} +++ [leaf-259]: {1} +++ [leaf-260]: {1} +++ [leaf-261]: {1} +++ [leaf-262]: {1} +++ [leaf-263]: {1} +++ [leaf-264]: {1} +++ [Occupation]: {-1  leaf-265, 0  leaf-266, 10  leaf-267, 11  leaf-268, 12  leaf-269, 13  leaf-270, 2  Age, 3  leaf-274, 4  leaf-275, 5  leaf-276, 6  leaf-277, 7  leaf-278, 8  leaf-279, 9  leaf-280} +++ [leaf-281]: {1} +++ [Age]: {<= 34.5 leaf-282, > 34.5 Occupation} +++ [Gender]: {0  leaf-301, 1  Age} +++ [Age]: {<= 67.0 Age, > 67.0 leaf-385}\n",
      "\n",
      "[leaf-2]: {0} +++ [leaf-3]: {1} +++ [leaf-10]: {0} +++ [leaf-11]: {1} +++ [leaf-14]: {1} +++ [Age]: {<= 44.5 leaf-15, > 44.5 leaf-16} +++ [Age]: {<= 39.5 leaf-22, > 39.5 leaf-23} +++ [leaf-24]: {0} +++ [Age]: {<= 65.0 Age, > 65.0 leaf-111} +++ [leaf-112]: {1} +++ [leaf-114]: {0} +++ [Gender]: {0  Age, 1  Age} +++ [leaf-167]: {0} +++ [Age]: {<= 47.5 leaf-168, > 47.5 leaf-169} +++ [Occupation]: {-1  Age, 0  Age, 10  leaf-175, 11  leaf-176, 12  leaf-177, 13  leaf-178, 2  leaf-179, 3  leaf-180, 4  leaf-181, 5  leaf-182, 6  leaf-183, 7  leaf-184, 8  leaf-185, 9  leaf-186} +++ [Age]: {<= 48.5 leaf-187, > 48.5 Age} +++ [leaf-190]: {0} +++ [Age]: {<= 29.5 leaf-191, > 29.5 Occupation} +++ [leaf-245]: {0} +++ [leaf-246]: {1} +++ [leaf-248]: {0} +++ [leaf-249]: {1} +++ [leaf-265]: {0} +++ [leaf-266]: {1} +++ [leaf-267]: {0} +++ [leaf-268]: {1} +++ [leaf-269]: {0} +++ [leaf-270]: {0} +++ [Age]: {<= 38.0 leaf-271, > 38.0 Age} +++ [leaf-274]: {0} +++ [leaf-275]: {0} +++ [leaf-276]: {1} +++ [leaf-277]: {0} +++ [leaf-278]: {1} +++ [leaf-279]: {0} +++ [leaf-280]: {0} +++ [leaf-282]: {0} +++ [Occupation]: {-1  leaf-283, 0  Age, 10  leaf-286, 11  leaf-287, 12  leaf-288, 13  leaf-289, 2  Age, 3  Age, 4  leaf-294, 5  leaf-295, 6  leaf-296, 7  leaf-297, 8  leaf-298, 9  Age} +++ [leaf-301]: {1} +++ [Age]: {<= 24.5 leaf-302, > 24.5 Occupation} +++ [Age]: {<= 61.0 Age, > 61.0 Occupation} +++ [leaf-385]: {0}\n",
      "\n",
      "[leaf-15]: {0} +++ [leaf-16]: {1} +++ [leaf-22]: {0} +++ [leaf-23]: {1} +++ [Age]: {<= 63.5 Occupation, > 63.5 leaf-110} +++ [leaf-111]: {0} +++ [Age]: {<= 58.0 Age, > 58.0 leaf-122} +++ [Age]: {<= 57.0 Age, > 57.0 Occupation} +++ [leaf-168]: {1} +++ [leaf-169]: {0} +++ [Age]: {<= 27.0 leaf-170, > 27.0 leaf-171} +++ [Age]: {<= 25.5 leaf-172, > 25.5 Age} +++ [leaf-175]: {1} +++ [leaf-176]: {1} +++ [leaf-177]: {1} +++ [leaf-178]: {1} +++ [leaf-179]: {1} +++ [leaf-180]: {1} +++ [leaf-181]: {1} +++ [leaf-182]: {1} +++ [leaf-183]: {1} +++ [leaf-184]: {1} +++ [leaf-185]: {1} +++ [leaf-186]: {1} +++ [leaf-187]: {1} +++ [Age]: {<= 49.5 leaf-188, > 49.5 leaf-189} +++ [leaf-191]: {1} +++ [Occupation]: {-1  leaf-192, 0  Age, 10  Age, 11  Age, 12  Age, 13  Age, 2  Age, 3  Age, 4  Age, 5  leaf-222, 6  leaf-223, 7  leaf-224, 8  leaf-225, 9  Age} +++ [leaf-271]: {1} +++ [Age]: {<= 56.0 leaf-272, > 56.0 leaf-273} +++ [leaf-283]: {1} +++ [Age]: {<= 41.0 leaf-284, > 41.0 leaf-285} +++ [leaf-286]: {1} +++ [leaf-287]: {1} +++ [leaf-288]: {0} +++ [leaf-289]: {1} +++ [Age]: {<= 42.0 leaf-290, > 42.0 leaf-291} +++ [Age]: {<= 49.5 leaf-292, > 49.5 leaf-293} +++ [leaf-294]: {1} +++ [leaf-295]: {1} +++ [leaf-296]: {1} +++ [leaf-297]: {1} +++ [leaf-298]: {1} +++ [Age]: {<= 38.0 leaf-299, > 38.0 leaf-300} +++ [leaf-302]: {1} +++ [Occupation]: {-1  leaf-303, 0  leaf-304, 10  leaf-305, 11  Age, 12  leaf-308, 13  leaf-309, 2  leaf-310, 3  Age, 4  leaf-314, 5  leaf-315, 6  leaf-316, 7  leaf-317, 8  leaf-318, 9  leaf-319} +++ [Age]: {<= 58.5 Occupation, > 58.5 leaf-370} +++ [Occupation]: {-1  leaf-371, 0  leaf-372, 10  leaf-373, 11  leaf-374, 12  leaf-375, 13  leaf-376, 2  leaf-377, 3  leaf-378, 4  leaf-379, 5  leaf-380, 6  leaf-381, 7  leaf-382, 8  leaf-383, 9  leaf-384}\n",
      "\n",
      "[Occupation]: {-1  Age, 0  Age, 10  leaf-43, 11  Age, 12  Age, 13  Age, 2  Gender, 3  Age, 4  Age, 5  leaf-98, 6  Age, 7  Age, 8  leaf-108, 9  leaf-109} +++ [leaf-110]: {1} +++ [Age]: {<= 49.0 Age, > 49.0 leaf-121} +++ [leaf-122]: {0} +++ [Age]: {<= 46.0 Age, > 46.0 leaf-139} +++ [Occupation]: {-1  leaf-140, 0  leaf-141, 10  leaf-142, 11  leaf-143, 12  leaf-144, 13  leaf-145, 2  leaf-146, 3  leaf-147, 4  leaf-148, 5  leaf-149, 6  leaf-150, 7  leaf-151, 8  leaf-152, 9  leaf-153} +++ [leaf-170]: {1} +++ [leaf-171]: {0} +++ [leaf-172]: {0} +++ [Age]: {<= 28.5 leaf-173, > 28.5 leaf-174} +++ [leaf-188]: {0} +++ [leaf-189]: {1} +++ [leaf-192]: {1} +++ [Age]: {<= 50.5 Age, > 50.5 leaf-195} +++ [Age]: {<= 41.5 leaf-196, > 41.5 Age} +++ [Age]: {<= 34.0 leaf-199, > 34.0 Age} +++ [Age]: {<= 46.0 leaf-207, > 46.0 leaf-208} +++ [Age]: {<= 50.0 leaf-209, > 50.0 leaf-210} +++ [Age]: {<= 38.0 leaf-211, > 38.0 Age} +++ [Age]: {<= 33.0 leaf-214, > 33.0 Age} +++ [Age]: {<= 54.5 leaf-220, > 54.5 leaf-221} +++ [leaf-222]: {1} +++ [leaf-223]: {0} +++ [leaf-224]: {1} +++ [leaf-225]: {1} +++ [Age]: {<= 41.5 leaf-226, > 41.5 Age} +++ [leaf-272]: {0} +++ [leaf-273]: {1} +++ [leaf-284]: {1} +++ [leaf-285]: {0} +++ [leaf-290]: {1} +++ [leaf-291]: {0} +++ [leaf-292]: {0} +++ [leaf-293]: {1} +++ [leaf-299]: {0} +++ [leaf-300]: {1} +++ [leaf-303]: {1} +++ [leaf-304]: {1} +++ [leaf-305]: {1} +++ [Age]: {<= 25.5 leaf-306, > 25.5 leaf-307} +++ [leaf-308]: {1} +++ [leaf-309]: {1} +++ [leaf-310]: {0} +++ [Age]: {<= 26.0 leaf-311, > 26.0 Age} +++ [leaf-314]: {1} +++ [leaf-315]: {1} +++ [leaf-316]: {1} +++ [leaf-317]: {1} +++ [leaf-318]: {1} +++ [leaf-319]: {1} +++ [Occupation]: {-1  Age, 0  Age, 10  leaf-325, 11  Age, 12  Age, 13  leaf-338, 2  Gender, 3  Gender, 4  leaf-353, 5  Age, 6  leaf-356, 7  leaf-357, 8  leaf-358, 9  Age} +++ [leaf-370]: {0} +++ [leaf-371]: {1} +++ [leaf-372]: {1} +++ [leaf-373]: {1} +++ [leaf-374]: {1} +++ [leaf-375]: {1} +++ [leaf-376]: {1} +++ [leaf-377]: {1} +++ [leaf-378]: {0} +++ [leaf-379]: {1} +++ [leaf-380]: {1} +++ [leaf-381]: {1} +++ [leaf-382]: {1} +++ [leaf-383]: {1} +++ [leaf-384]: {1}\n",
      "\n",
      "[Age]: {<= 34.5 Age, > 34.5 leaf-28} +++ [Age]: {<= 56.0 Age, > 56.0 leaf-42} +++ [leaf-43]: {1} +++ [Age]: {<= 49.5 leaf-44, > 49.5 Gender} +++ [Age]: {<= 46.0 leaf-52, > 46.0 leaf-53} +++ [Age]: {<= 26.5 leaf-54, > 26.5 Age} +++ [Gender]: {0  leaf-63, 1  Age} +++ [Age]: {<= 61.0 Age, > 61.0 leaf-94} +++ [Age]: {<= 34.5 leaf-95, > 34.5 Age} +++ [leaf-98]: {1} +++ [Age]: {<= 50.5 Age, > 50.5 Age} +++ [Age]: {<= 31.5 Age, > 31.5 leaf-107} +++ [leaf-108]: {1} +++ [leaf-109]: {0} +++ [Age]: {<= 47.5 Age, > 47.5 leaf-120} +++ [leaf-121]: {1} +++ [Age]: {<= 37.5 leaf-123, > 37.5 Occupation} +++ [leaf-139]: {0} +++ [leaf-140]: {1} +++ [leaf-141]: {1} +++ [leaf-142]: {1} +++ [leaf-143]: {0} +++ [leaf-144]: {1} +++ [leaf-145]: {1} +++ [leaf-146]: {1} +++ [leaf-147]: {1} +++ [leaf-148]: {1} +++ [leaf-149]: {1} +++ [leaf-150]: {1} +++ [leaf-151]: {1} +++ [leaf-152]: {1} +++ [leaf-153]: {1} +++ [leaf-173]: {1} +++ [leaf-174]: {1} +++ [Age]: {<= 41.5 leaf-193, > 41.5 leaf-194} +++ [leaf-195]: {0} +++ [leaf-196]: {1} +++ [Age]: {<= 53.5 leaf-197, > 53.5 leaf-198} +++ [leaf-199]: {1} +++ [Age]: {<= 36.5 leaf-200, > 36.5 Age} +++ [leaf-207]: {1} +++ [leaf-208]: {0} +++ [leaf-209]: {1} +++ [leaf-210]: {0} +++ [leaf-211]: {1} +++ [Age]: {<= 42.5 leaf-212, > 42.5 leaf-213} +++ [leaf-214]: {0} +++ [Age]: {<= 42.0 leaf-215, > 42.0 Age} +++ [leaf-220]: {1} +++ [leaf-221]: {0} +++ [leaf-226]: {1} +++ [Age]: {<= 46.5 leaf-227, > 46.5 leaf-228} +++ [leaf-306]: {0} +++ [leaf-307]: {1} +++ [leaf-311]: {1} +++ [Age]: {<= 27.5 leaf-312, > 27.5 leaf-313} +++ [Age]: {<= 40.5 leaf-320, > 40.5 leaf-321} +++ [Age]: {<= 33.5 leaf-322, > 33.5 Age} +++ [leaf-325]: {0} +++ [Age]: {<= 30.0 leaf-326, > 30.0 Age} +++ [Age]: {<= 45.5 leaf-335, > 45.5 Age} +++ [leaf-338]: {1} +++ [Gender]: {0  leaf-339, 1  Age} +++ [Gender]: {0  leaf-342, 1  Age} +++ [leaf-353]: {0} +++ [Age]: {<= 38.0 leaf-354, > 38.0 leaf-355} +++ [leaf-356]: {1} +++ [leaf-357]: {1} +++ [leaf-358]: {1} +++ [Age]: {<= 29.5 leaf-359, > 29.5 Age}\n",
      "\n",
      "[Age]: {<= 30.5 leaf-26, > 30.5 leaf-27} +++ [leaf-28]: {1} +++ [Age]: {<= 53.5 Age, > 53.5 leaf-41} +++ [leaf-42]: {1} +++ [leaf-44]: {1} +++ [Gender]: {0  leaf-45, 1  Age} +++ [leaf-52]: {0} +++ [leaf-53]: {1} +++ [leaf-54]: {0} +++ [Age]: {<= 38.5 Age, > 38.5 Age} +++ [leaf-63]: {1} +++ [Age]: {<= 62.5 Age, > 62.5 leaf-82} +++ [Age]: {<= 59.5 Age, > 59.5 leaf-93} +++ [leaf-94]: {1} +++ [leaf-95]: {1} +++ [Age]: {<= 45.0 leaf-96, > 45.0 leaf-97} +++ [Age]: {<= 38.0 leaf-99, > 38.0 Age} +++ [Age]: {<= 56.0 leaf-102, > 56.0 leaf-103} +++ [Age]: {<= 30.0 leaf-104, > 30.0 Gender} +++ [leaf-107]: {1} +++ [Age]: {<= 46.5 Age, > 46.5 leaf-119} +++ [leaf-120]: {0} +++ [leaf-123]: {0} +++ [Occupation]: {-1  leaf-124, 0  leaf-125, 10  leaf-126, 11  leaf-127, 12  leaf-128, 13  leaf-129, 2  leaf-130, 3  Age, 4  leaf-133, 5  leaf-134, 6  leaf-135, 7  leaf-136, 8  leaf-137, 9  leaf-138} +++ [leaf-193]: {0} +++ [leaf-194]: {1} +++ [leaf-197]: {0} +++ [leaf-198]: {1} +++ [leaf-200]: {0} +++ [Age]: {<= 56.0 Age, > 56.0 leaf-206} +++ [leaf-212]: {0} +++ [leaf-213]: {1} +++ [leaf-215]: {1} +++ [Age]: {<= 50.5 Age, > 50.5 leaf-219} +++ [leaf-227]: {0} +++ [leaf-228]: {1} +++ [leaf-312]: {1} +++ [leaf-313]: {1} +++ [leaf-320]: {1} +++ [leaf-321]: {0} +++ [leaf-322]: {0} +++ [Age]: {<= 41.5 leaf-323, > 41.5 leaf-324} +++ [leaf-326]: {1} +++ [Age]: {<= 56.0 Age, > 56.0 leaf-334} +++ [leaf-335]: {1} +++ [Age]: {<= 48.5 leaf-336, > 48.5 leaf-337} +++ [leaf-339]: {1} +++ [Age]: {<= 51.5 leaf-340, > 51.5 leaf-341} +++ [leaf-342]: {1} +++ [Age]: {<= 44.5 Age, > 44.5 leaf-352} +++ [leaf-354]: {0} +++ [leaf-355]: {1} +++ [leaf-359]: {0} +++ [Age]: {<= 45.5 Gender, > 45.5 leaf-369}\n",
      "\n",
      "[leaf-26]: {1} +++ [leaf-27]: {0} +++ [Age]: {<= 50.5 Age, > 50.5 leaf-40} +++ [leaf-41]: {0} +++ [leaf-45]: {1} +++ [Age]: {<= 62.0 Age, > 62.0 leaf-51} +++ [Age]: {<= 36.5 Age, > 36.5 leaf-59} +++ [Age]: {<= 49.0 leaf-60, > 49.0 Age} +++ [Age]: {<= 29.5 leaf-64, > 29.5 Age} +++ [leaf-82]: {0} +++ [Age]: {<= 53.0 Age, > 53.0 leaf-92} +++ [leaf-93]: {0} +++ [leaf-96]: {0} +++ [leaf-97]: {1} +++ [leaf-99]: {1} +++ [Age]: {<= 40.5 leaf-100, > 40.5 leaf-101} +++ [leaf-102]: {0} +++ [leaf-103]: {1} +++ [leaf-104]: {1} +++ [Gender]: {0  leaf-105, 1  leaf-106} +++ [Age]: {<= 40.5 Age, > 40.5 leaf-118} +++ [leaf-119]: {0} +++ [leaf-124]: {1} +++ [leaf-125]: {1} +++ [leaf-126]: {1} +++ [leaf-127]: {1} +++ [leaf-128]: {1} +++ [leaf-129]: {1} +++ [leaf-130]: {1} +++ [Age]: {<= 40.0 leaf-131, > 40.0 leaf-132} +++ [leaf-133]: {0} +++ [leaf-134]: {0} +++ [leaf-135]: {0} +++ [leaf-136]: {0} +++ [leaf-137]: {0} +++ [leaf-138]: {1} +++ [Age]: {<= 50.0 Age, > 50.0 leaf-205} +++ [leaf-206]: {1} +++ [Age]: {<= 43.5 leaf-216, > 43.5 Age} +++ [leaf-219]: {1} +++ [leaf-323]: {1} +++ [leaf-324]: {0} +++ [Age]: {<= 34.0 Age, > 34.0 Age} +++ [leaf-334]: {1} +++ [leaf-336]: {0} +++ [leaf-337]: {1} +++ [leaf-340]: {0} +++ [leaf-341]: {1} +++ [Age]: {<= 43.5 Age, > 43.5 leaf-351} +++ [leaf-352]: {0} +++ [Gender]: {0  leaf-360, 1  Age} +++ [leaf-369]: {0}\n",
      "\n",
      "[Age]: {<= 48.5 Age, > 48.5 leaf-39} +++ [leaf-40]: {1} +++ [Age]: {<= 60.0 Age, > 60.0 leaf-50} +++ [leaf-51]: {1} +++ [Age]: {<= 32.0 leaf-55, > 32.0 Age} +++ [leaf-59]: {0} +++ [leaf-60]: {1} +++ [Age]: {<= 52.0 leaf-61, > 52.0 leaf-62} +++ [leaf-64]: {1} +++ [Age]: {<= 59.5 Age, > 59.5 leaf-81} +++ [Age]: {<= 46.5 Age, > 46.5 leaf-91} +++ [leaf-92]: {1} +++ [leaf-100]: {0} +++ [leaf-101]: {1} +++ [leaf-105]: {0} +++ [leaf-106]: {1} +++ [Age]: {<= 39.5 Age, > 39.5 leaf-117} +++ [leaf-118]: {1} +++ [leaf-131]: {1} +++ [leaf-132]: {0} +++ [Age]: {<= 44.5 Age, > 44.5 leaf-204} +++ [leaf-205]: {0} +++ [leaf-216]: {0} +++ [Age]: {<= 46.0 leaf-217, > 46.0 leaf-218} +++ [Age]: {<= 31.5 leaf-327, > 31.5 leaf-328} +++ [Age]: {<= 42.5 leaf-329, > 42.5 Age} +++ [Age]: {<= 40.0 Age, > 40.0 leaf-350} +++ [leaf-351]: {1} +++ [leaf-360]: {1} +++ [Age]: {<= 44.0 Age, > 44.0 leaf-368}\n",
      "\n",
      "[Age]: {<= 37.5 Age, > 37.5 Age} +++ [leaf-39]: {0} +++ [Age]: {<= 58.5 Age, > 58.5 leaf-49} +++ [leaf-50]: {0} +++ [leaf-55]: {1} +++ [Age]: {<= 33.5 leaf-56, > 33.5 Age} +++ [leaf-61]: {0} +++ [leaf-62]: {1} +++ [Age]: {<= 56.5 Age, > 56.5 leaf-80} +++ [leaf-81]: {1} +++ [Age]: {<= 44.0 Age, > 44.0 leaf-90} +++ [leaf-91]: {0} +++ [Age]: {<= 37.5 leaf-115, > 37.5 leaf-116} +++ [leaf-117]: {0} +++ [Age]: {<= 41.5 Age, > 41.5 leaf-203} +++ [leaf-204]: {1} +++ [leaf-217]: {1} +++ [leaf-218]: {0} +++ [leaf-327]: {1} +++ [leaf-328]: {1} +++ [leaf-329]: {0} +++ [Age]: {<= 44.5 leaf-330, > 44.5 Age} +++ [Age]: {<= 32.5 Age, > 32.5 Age} +++ [leaf-350]: {0} +++ [Age]: {<= 41.5 Age, > 41.5 leaf-367} +++ [leaf-368]: {1}\n",
      "\n",
      "[Age]: {<= 27.0 leaf-29, > 27.0 leaf-30} +++ [Age]: {<= 38.5 leaf-31, > 38.5 Age} +++ [Age]: {<= 54.5 Age, > 54.5 leaf-48} +++ [leaf-49]: {1} +++ [leaf-56]: {0} +++ [Age]: {<= 35.0 leaf-57, > 35.0 leaf-58} +++ [Age]: {<= 54.0 Age, > 54.0 leaf-79} +++ [leaf-80]: {0} +++ [Age]: {<= 30.0 leaf-83, > 30.0 Age} +++ [leaf-90]: {1} +++ [leaf-115]: {0} +++ [leaf-116]: {1} +++ [Age]: {<= 39.0 leaf-201, > 39.0 leaf-202} +++ [leaf-203]: {0} +++ [leaf-330]: {1} +++ [Age]: {<= 48.5 leaf-331, > 48.5 Age} +++ [Age]: {<= 29.5 leaf-343, > 29.5 leaf-344} +++ [Age]: {<= 33.5 leaf-345, > 33.5 Age} +++ [Age]: {<= 38.5 Age, > 38.5 leaf-366} +++ [leaf-367]: {0}\n",
      "\n",
      "[leaf-29]: {1} +++ [leaf-30]: {1} +++ [leaf-31]: {0} +++ [Age]: {<= 39.5 leaf-32, > 39.5 Age} +++ [Age]: {<= 50.5 leaf-46, > 50.5 leaf-47} +++ [leaf-48]: {0} +++ [leaf-57]: {1} +++ [leaf-58]: {1} +++ [Age]: {<= 35.5 Age, > 35.5 Age} +++ [leaf-79]: {1} +++ [leaf-83]: {1} +++ [Age]: {<= 33.5 leaf-84, > 33.5 Age} +++ [leaf-201]: {0} +++ [leaf-202]: {1} +++ [leaf-331]: {0} +++ [Age]: {<= 52.0 leaf-332, > 52.0 leaf-333} +++ [leaf-343]: {1} +++ [leaf-344]: {0} +++ [leaf-345]: {1} +++ [Age]: {<= 35.0 leaf-346, > 35.0 Age} +++ [Age]: {<= 35.5 Age, > 35.5 leaf-365} +++ [leaf-366]: {1}\n",
      "\n",
      "[leaf-32]: {1} +++ [Age]: {<= 40.5 leaf-33, > 40.5 Age} +++ [leaf-46]: {1} +++ [leaf-47]: {1} +++ [Age]: {<= 30.5 leaf-65, > 30.5 leaf-66} +++ [Age]: {<= 36.5 leaf-67, > 36.5 Age} +++ [leaf-84]: {0} +++ [Age]: {<= 36.5 leaf-85, > 36.5 Age} +++ [leaf-332]: {1} +++ [leaf-333]: {0} +++ [leaf-346]: {0} +++ [Age]: {<= 36.5 leaf-347, > 36.5 Age} +++ [Age]: {<= 34.0 Age, > 34.0 leaf-364} +++ [leaf-365]: {0}\n",
      "\n",
      "[leaf-33]: {0} +++ [Age]: {<= 42.5 leaf-34, > 42.5 Age} +++ [leaf-65]: {1} +++ [leaf-66]: {1} +++ [leaf-67]: {0} +++ [Age]: {<= 40.0 Age, > 40.0 Age} +++ [leaf-85]: {1} +++ [Age]: {<= 41.5 Age, > 41.5 leaf-89} +++ [leaf-347]: {1} +++ [Age]: {<= 37.5 leaf-348, > 37.5 leaf-349} +++ [Age]: {<= 31.0 leaf-361, > 31.0 Age} +++ [leaf-364]: {1}\n",
      "\n",
      "[leaf-34]: {1} +++ [Age]: {<= 44.0 leaf-35, > 44.0 Gender} +++ [Age]: {<= 37.5 leaf-68, > 37.5 leaf-69} +++ [Age]: {<= 43.5 Age, > 43.5 Age} +++ [Age]: {<= 38.5 Gender, > 38.5 leaf-88} +++ [leaf-89]: {0} +++ [leaf-348]: {1} +++ [leaf-349]: {0} +++ [leaf-361]: {1} +++ [Age]: {<= 32.5 leaf-362, > 32.5 leaf-363}\n",
      "\n",
      "[leaf-35]: {0} +++ [Gender]: {0  leaf-36, 1  Age} +++ [leaf-68]: {1} +++ [leaf-69]: {1} +++ [Age]: {<= 42.5 leaf-70, > 42.5 leaf-71} +++ [Age]: {<= 44.5 leaf-72, > 44.5 Age} +++ [Gender]: {0  leaf-86, 1  leaf-87} +++ [leaf-88]: {1} +++ [leaf-362]: {1} +++ [leaf-363]: {1}\n",
      "\n",
      "[leaf-36]: {1} +++ [Age]: {<= 45.5 leaf-37, > 45.5 leaf-38} +++ [leaf-70]: {1} +++ [leaf-71]: {0} +++ [leaf-72]: {1} +++ [Age]: {<= 46.5 Age, > 46.5 Age} +++ [leaf-86]: {0} +++ [leaf-87]: {0}\n",
      "\n",
      "[leaf-37]: {0} +++ [leaf-38]: {1} +++ [Age]: {<= 45.5 leaf-73, > 45.5 leaf-74} +++ [Age]: {<= 48.0 leaf-75, > 48.0 Age}\n",
      "\n",
      "[leaf-73]: {1} +++ [leaf-74]: {0} +++ [leaf-75]: {1} +++ [Age]: {<= 49.5 leaf-76, > 49.5 Age}\n",
      "\n",
      "[leaf-76]: {0} +++ [Age]: {<= 51.5 leaf-77, > 51.5 leaf-78}\n",
      "\n",
      "[leaf-77]: {1} +++ [leaf-78]: {1}\n",
      "============Leaf===========\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "0.7380073800738007\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "============Root===========\n",
      "[leaf-388]: {1}\n",
      "============Leaf===========\n",
      "inside D\n",
      "[['42' '11' '6' '0' '0']\n",
      " ['48' '14' '3' '1' '0']\n",
      " ['49' '11' '2' '1' '0']\n",
      " ...\n",
      " ['57' '12' '9' '1' '0']\n",
      " ['43' '8' '3' '1' '0']\n",
      " ['22' '11' '3' '0' '1']]\n",
      "D shape\n",
      "(813, 5)\n",
      "D type\n",
      "<class 'numpy.ndarray'>\n",
      "0.7490774907749077\n"
     ]
    }
   ],
   "source": [
    "#D6= np.genfromtxt('../datafiles/iris_train.txt', dtype=str)\n",
    "D6 = adult23.to_numpy()\n",
    "D6 = D6.astype(str)\n",
    "np.random.shuffle(D6)\n",
    "\n",
    "k=len(D6)\n",
    "\n",
    "#attribute_name=['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\"]\n",
    "\n",
    "A={name: [i] for i, name in enumerate(attribute_name)}\n",
    "\n",
    "print(A)\n",
    "\n",
    "# feature_discrete = {name: False for name in attribute_name}\n",
    "feature_discrete={name: False if isinstance(adult2[name].iloc[0], np.integer) else True for name in attribute_name}\n",
    "\n",
    "conf={'A': A, \n",
    "      'feature_discrete': feature_discrete, \n",
    "      'treeType':'IGGR'}\n",
    "\n",
    "\n",
    "dt=Tree_OLD(conf)\n",
    "\n",
    "dt.train(D6[:len(D6)//2])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D6[len(D6)//2:]))\n",
    "dt.prune(D6[:len(D6)//2], D6[len(D6)//2:])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D6[len(D6)//2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1628"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '1554']\n",
      " ['1504' '1']\n",
      " ['1590' '1']\n",
      " ['1594' '1']\n",
      " ['1602' '1']\n",
      " ['1617' '1']\n",
      " ['1628' '1']\n",
      " ['1651' '1']\n",
      " ['1672' '1']\n",
      " ['1719' '1']\n",
      " ['1721' '1']\n",
      " ['1741' '1']\n",
      " ['1762' '1']\n",
      " ['1848' '6']\n",
      " ['1876' '2']\n",
      " ['1887' '10']\n",
      " ['1902' '9']\n",
      " ['1974' '1']\n",
      " ['1977' '13']\n",
      " ['1980' '2']\n",
      " ['2001' '1']\n",
      " ['2002' '2']\n",
      " ['2051' '1']\n",
      " ['2057' '1']\n",
      " ['2080' '1']\n",
      " ['2179' '1']\n",
      " ['2206' '1']\n",
      " ['2246' '1']\n",
      " ['2258' '1']\n",
      " ['2267' '1']\n",
      " ['2339' '1']\n",
      " ['2377' '1']\n",
      " ['2415' '3']\n",
      " ['2444' '1']\n",
      " ['2559' '2']]\n"
     ]
    }
   ],
   "source": [
    "herlist = D2[:, 4]\n",
    "unique, counts = np.unique(herlist, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(float('2559') > float('915.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(D2[:, 4].astype(float)> float(str(951.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0',\n",
       "       '1', '1', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0',\n",
       "       '1', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1',\n",
       "       '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '1', '1', '1',\n",
       "       '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0',\n",
       "       '0', '1', '0', '0', '0', '0', '0', '0', '0'], dtype='<U21')"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2[np.argwhere(D2[:, 4].astype(float) > 951.0), -1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "#from Node import Node\n",
    "\n",
    "# set the maximal recursion limits here.\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# to be modified to single class\n",
    "\n",
    "# attribute_indexes, - A_ind\n",
    "# attribute_values, A\n",
    "# root, \n",
    "# class_values, \n",
    "# feature_discrete\n",
    "# treetype \n",
    "# dataset_size, \n",
    "# epsilon_per_tree\n",
    "\n",
    "\n",
    "class Tree_DPDT(DPRF_Forest):\n",
    "\n",
    "    '''\n",
    "    The main class of decision tree.\n",
    "    '''\n",
    "    def __init__(self, A_ind, A, attribute_values, root, class_values, feature_discrete, treetype, dataset_size, epsilon_per_tree):\n",
    "        '''\n",
    "        attribute_values : attribute_values\n",
    "        A : orignal\n",
    "        feature_discrete: a dict with its each key-value pair being (feature_name: True/False),\n",
    "            where True means the feature is discrete and False means the feature is \n",
    "            continuous. \n",
    "        type: ID3/C4.5/CART\n",
    "        pruning: pre/post\n",
    "        '''\n",
    "\n",
    "        self.A=A\n",
    "        self.attribute_values=attribute_values\n",
    "        self.A_ind=A_ind\n",
    "        self.feature_discrete= feature_discrete\n",
    "        self.treeType=treetype\n",
    "        self.leaf_count=0\n",
    "        self.tmp_classification=''\n",
    "        self.class_values=class_values\n",
    "        self._root_node = root\n",
    "        self.tree=None\n",
    "        self.dataset_size=dataset_size\n",
    "        self.epsilon=epsilon_per_tree\n",
    "\n",
    "    def Entropy(self, list_of_class):\n",
    "        '''\n",
    "        Compute the entropy for the given list of class.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        'duck': 2/3, 'dolphin': 1/3, so the entropy for this array is 0.918\n",
    "        '''\n",
    "        count={}\n",
    "        for key in list_of_class:\n",
    "            count[key]=count.get(key, 0)+1\n",
    "        frequency=np.array(tuple(count.values()))/len(list_of_class)\n",
    "        return -1*np.vdot(frequency, np.log2(frequency))\n",
    "\n",
    "    def Information_Gain(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Gain.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Gain for this example is 0.2516.\n",
    "        '''\n",
    "        sec2=np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        return self.Entropy(list_of_class)-sec2\n",
    "\n",
    "\n",
    "    def Information_Ratio(self, list_of_class, grouped_list_of_class):\n",
    "        '''\n",
    "        Compute the Information Ratio.\n",
    "        list_of_class: an array of classification labels, e.g. ['duck', 'duck', 'dolphin']\n",
    "        grouped_list_of_class: the list of class grouped by the values of \n",
    "            a certain attribute, e.g. [('duck'), ('duck', 'dolphin')].\n",
    "        The Information_Ratio for this example is 0.2740.\n",
    "        '''\n",
    "        tmp=np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        # Here we assume instrinsic_value is SplitInformation! \n",
    "        intrinsic_value=-1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        return self.Information_Gain(list_of_class, grouped_list_of_class) / intrinsic_value\n",
    "\n",
    "    def IGGR(self, list_of_class, grouped_list_of_class, a1, a2):\n",
    "\n",
    "        sec2 = np.sum([len(item)*self.Entropy(item) for item in grouped_list_of_class])/len(list_of_class)\n",
    "        \n",
    "        infoGain =  self.Entropy(list_of_class)-sec2\n",
    "\n",
    "        #print(\"IGGR grouped_list_of_class\")\n",
    "        #print(\"----------\")\n",
    "        #print(grouped_list_of_class)\n",
    "        #print(\"----------\")\n",
    "\n",
    "        tmp = np.array([len(item)/len(list_of_class) for item in grouped_list_of_class])\n",
    "        \n",
    "        intrinsic_value = -1*np.vdot(tmp, np.log2(tmp))\n",
    "\n",
    "        GainRatio = self.Information_Gain(list_of_class, grouped_list_of_class)/ intrinsic_value\n",
    "\n",
    "        sol1 = a1 * infoGain + a2 * GainRatio\n",
    "        sol2 = (a1 + ( a2 / intrinsic_value) ) * infoGain \n",
    "\n",
    "        return sol2\n",
    "\n",
    "    def orderByGainOrRatio(self, D, A, by='Gain'):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function = self.Information_Gain if by=='Gain' else self.Information_Ratio\n",
    "\n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "\n",
    "                for val in possibleVal:\n",
    "\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "\n",
    "                IC_value=target_function(D[:, -1], grouped_list_of_class)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "\n",
    "                # continuous\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "\n",
    "                for point in split_points:\n",
    "                    smaller_set=D[np.argwhere(D[:, info[0]]<=str(point)), -1].flatten()\n",
    "                    bigger_set=D[np.argwhere(D[:, info[0]]>str(point)), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    IC_tmp=target_function(D[:, -1], (smaller_set, bigger_set))\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def orderByIGGR(self, D, A, a1, a2):\n",
    "        '''\n",
    "        Return the order by Information Gain or Information Ratio.\n",
    "        by: 'Gain', 'Ratio'.\n",
    "        For the definition of D and A, see the remark in method 'fit'.\n",
    "        '''\n",
    "        tmp_value_dict=dict()\n",
    "\n",
    "        target_function1 = self.Information_Gain\n",
    "        target_function2 = self.Information_Ratio\n",
    "        target_function3 = self.IGGR\n",
    "        \n",
    "        for attr, info in A.items():\n",
    "            possibleVal=np.unique(D[:, info[0]])\n",
    "            # if the continuous attribute have only one possible value, then \n",
    "            # choosing it won't improve the model, so we abandon it.\n",
    "            if len(possibleVal)==1:\n",
    "                continue\n",
    "\n",
    "            if self.feature_discrete[attr] is True:\n",
    "                # discrete\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(possibleVal)\n",
    "                # retrieve the grouped list of class\n",
    "                grouped_list_of_class=[]\n",
    "                for val in possibleVal:\n",
    "                    indexes=np.argwhere(D[:, info[0]]==val)\n",
    "                    grouped_list_of_class.append(D[indexes, -1].flatten())\n",
    "                IC_value=target_function3(D[:, -1], grouped_list_of_class, a1, a2)\n",
    "                tmp_value_dict[attr]=IC_value\n",
    "\n",
    "            else:\n",
    "                # continuous\n",
    "\n",
    "\n",
    "                split_points=(possibleVal[: -1].astype(np.float32)+possibleVal[1:].astype(np.float32))/2\n",
    "                maxMetric=-1\n",
    "                for point in split_points:\n",
    "\n",
    "                    # modified now to float comparisons\n",
    "                    # causes the code to crash to max recursion\n",
    "                    # smaller_set = D[np.argwhere(D[:, info[0]].astype(float) <= point ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    # bigger_set = D[np.argwhere(D[:, info[0]].astype(float) > point ), -1].flatten()\n",
    "                    \n",
    "                    smaller_set = D[np.argwhere(D[:, info[0]] <= str(point) ), -1].flatten()\n",
    "                    # info[0] = 4, point = 951.0\n",
    "                    bigger_set = D[np.argwhere(D[:, info[0]] > str(point) ), -1].flatten()\n",
    "\n",
    "                    # compute the metric\n",
    "                    # list_of_class, grouped_list_of_class, a1, a2\n",
    "                    IC_tmp=target_function3(D[:, -1], (smaller_set, bigger_set), a1, a2)\n",
    "\n",
    "                    if IC_tmp>maxMetric:\n",
    "                        maxMetric=IC_tmp\n",
    "                        threshold=point\n",
    "\n",
    "                    # to stop the threshold from breaking the code\n",
    "                    \n",
    "                    #print(\"the inputs to bigger set producing empty list\")\n",
    "                    #print(info[0], point)\n",
    "\n",
    "                    #print(\"attribute was: \", attr)\n",
    "                    #print(\"sets (small and big)\")\n",
    "                    #print(smaller_set)\n",
    "                    #print(bigger_set)\n",
    "\n",
    "                    # IC_tmp=0.0\n",
    "\n",
    "                # set the threshold\n",
    "                if len(info)<2:\n",
    "                    A[attr].append(threshold)\n",
    "                else:\n",
    "                    A[attr][1]=threshold\n",
    "                tmp_value_dict[attr]=maxMetric\n",
    "\n",
    "        # find the attribute with the max tmp_value_dict value\n",
    "        attr_list=list(tmp_value_dict.keys())\n",
    "        attr_list.sort(key=lambda x: tmp_value_dict[x])\n",
    "\n",
    "        return attr_list\n",
    "\n",
    "    def chooseAttribute(self, D, A):\n",
    "\n",
    "        '''\n",
    "        Choose an attribute from A according to the metrics above.\n",
    "        For the definition of D and A, see method 'fit'.\n",
    "        Different principal for different tree types:\n",
    "        ID3: choose the attribute that maximizes the Information Gain.\n",
    "        C4.5: \n",
    "            1, choose those attributes whose Information Gain are above average.\n",
    "            2, choose the one that maximizes the Gain Ratio from these attributes.\n",
    "        CART: choose the attribute that minimizes the Gini Index.\n",
    "        IG_GR: \n",
    "        '''\n",
    "        if self.treeType=='ID3':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='C4.5':\n",
    "            attr_list=self.orderByGainOrRatio(D, A, by='Gain')\n",
    "\n",
    "            # for C4.5, we choose the attributes whose Gain are above average\n",
    "            # and then order them by Ratio.\n",
    "\n",
    "            sub_A={key: A[key] for key in attr_list}\n",
    "            attr_list=self.orderByGainOrRatio(D, sub_A, by='Ratio')\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "        if self.treeType=='IGGR':\n",
    "\n",
    "            attr_list=self.orderByIGGR(D, A, 0.4, 0.6)\n",
    "\n",
    "            return attr_list[-1]\n",
    "\n",
    "    def train(self, D):\n",
    "        self.tree=self.fit(D, self.A)\n",
    "\n",
    "    def fit(self, D, A):\n",
    "\n",
    "        '''\n",
    "        Train the tree.\n",
    "        To save the training result:\n",
    "        >> self.tree=self.fit(D, A)\n",
    "        D: the training set, a size [m, n+1] numpy array (with str type elements), \n",
    "            where m is the number of training data and n is the number of attributes.\n",
    "            The last column of D is the classifications (or labels).\n",
    "        A: the attributes set. It is a dict with its structure being like \n",
    "            {attr_name: [index_in_D_columns, possibleVal_or_threshold], ...}\n",
    "            attr_name: name of the attribute\n",
    "            index_in_D_columns: the corresponding index of the attribute in ndarray D (starting from 0)\n",
    "            possibleVal_or_threshold: \n",
    "                ###################################################\n",
    "                ## This value may not always be available in A   ##\n",
    "                ## it is added after 'chooseAttribute' is called ##\n",
    "                ## And it will be updated after each call        ##\n",
    "                ###################################################\n",
    "                1, if the attribute is discrete, then it is a ndarray containing all possible values \n",
    "                    of this attribute.\n",
    "                2, if the attribute is continuous, then possibleVal_or_threshold is the most recent \n",
    "                    threshold.\n",
    "        '''\n",
    "\n",
    "        # termination conditions \n",
    "\n",
    "        # the training set is empty\n",
    "        if len(D)==0:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=self.tmp_classification)\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        # only one type of classification is left \n",
    "        if len(np.unique(D[:, -1]))<=1:\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=D[0, -1])\n",
    "            self.leaf_count+=1\n",
    "            return node\n",
    "\n",
    "        if len(A)==0 or len(np.unique(D[:, :-1], axis=0))<=1:\n",
    "            count_dict={}\n",
    "            for key in D[:, -1]:\n",
    "                count_dict[key]=count_dict.get(key, 0)+1\n",
    "            most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "            node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, \\\n",
    "                classification=most_frequent)\n",
    "            self.leaf_count+=1\n",
    "            return node \n",
    "\n",
    "        count_dict={}\n",
    "        for key in D[:, -1]:\n",
    "            count_dict[key]=count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "        self.tmp_classification=most_frequent\n",
    "\n",
    "        # choose target attribute\n",
    "        target_attr=self.chooseAttribute(D, A)\n",
    "        # print(target_attr)\n",
    "\n",
    "        # generate nodes for each possible values of the target attribute if it's discrete\n",
    "        # generate two nodes for the two classification if it's continuous\n",
    "        # related information is stored in A[target_attr][1] now, \n",
    "        # since we have called chooseAttribute at least once.\n",
    "        info=A[target_attr]\n",
    "\n",
    "        if self.feature_discrete[target_attr]:\n",
    "\n",
    "            node=Node(feature_name=target_attr, discrete=True, isLeaf=False)\n",
    "            # generate nodes for each possible values\n",
    "            \n",
    "            for possibleVal in info[1]:\n",
    "                keys=set(A.keys()).difference({target_attr})\n",
    "                # connect node to its child\n",
    "                tmp_D=D[np.argwhere(D[:, info[0]]==possibleVal), :]\n",
    "                tmp_A={key: A[key] for key in keys}\n",
    "\n",
    "                node[possibleVal]=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), tmp_A)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # continuous\n",
    "            threshold=info[1]\n",
    "            node=Node(feature_name=target_attr, discrete=False, threshold=threshold, isLeaf=False)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]<=str(threshold)), :]\n",
    "            node['<=']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "\n",
    "            tmp_D=D[np.argwhere(D[:, info[0]]>str(threshold)), :]\n",
    "            node['>']=self.fit(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), A)\n",
    "            \n",
    "        \n",
    "        return node\n",
    "\n",
    "    def prune(self, training_D, testing_D):\n",
    "        print(\"We got to the pruning stage!\")\n",
    "        self.post_prune(training_D, testing_D, self.A, current=self.tree)\n",
    "\n",
    "    def post_prune(self, training_D, testing_D, A, current=None, parent=None):\n",
    "    \n",
    "        '''\n",
    "        self.tree is required.\n",
    "        This method conducts the post-pruning to enhance the model performance.\n",
    "        To make sure this method will work, set \n",
    "        >> current=self.tree\n",
    "        when you call it.\n",
    "        '''\n",
    "\n",
    "        # print(\"prune prints!\")\n",
    "\n",
    "        self.current_accuracy=self.evaluate(testing_D, A)\n",
    "\n",
    "        count_dict={}\n",
    "        if len(training_D)==0:\n",
    "            return \n",
    "\n",
    "        for key in training_D[:, -1]:\n",
    "            count_dict[key] = count_dict.get(key, 0)+1\n",
    "        most_frequent=sorted(training_D[:, -1], key=lambda x: count_dict[x])[-1]\n",
    "\n",
    "        leaf_parent=True\n",
    "        for key, node in current.map.items():\n",
    "            if not node.isLeaf:\n",
    "                leaf_parent=False\n",
    "                # Recursion, DFS\n",
    "                if node.discrete:\n",
    "                    tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]==key), :]\n",
    "                else:\n",
    "                    if key=='<=':\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]<=str(node.threshold)), :]\n",
    "                    else:\n",
    "                        tmp_D=training_D[np.argwhere(training_D[:, A[current.feature_name][0]]>str(node.threshold)), :]\n",
    "                self.post_prune(tmp_D.reshape((tmp_D.shape[0], tmp_D.shape[2])), testing_D, A, parent=current, current=node)\n",
    "        \n",
    "        tmp_node=Node(feature_name='leaf-'+str(self.leaf_count), isLeaf=True, classification=most_frequent)\n",
    "        \n",
    "        if parent:\n",
    "            # when current node is not the root\n",
    "            for key, node in parent.map.items():\n",
    "                if node==current:\n",
    "                    parent.map[key]=tmp_node\n",
    "                    saved_key=key\n",
    "                    break\n",
    "            # compare the evaluation, if it is enhanced then prune the tree.\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                parent.map[saved_key]=current\n",
    "                \n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "        else:\n",
    "            # when current node is the root\n",
    "            saved_tree=self.tree\n",
    "            self.tree=tmp_node\n",
    "            tmp_accuracy=self.evaluate(testing_D, A)\n",
    "            if tmp_accuracy<self.current_accuracy:\n",
    "                self.tree=saved_tree\n",
    "            else:\n",
    "                self.current_accuracy=tmp_accuracy\n",
    "                self.leaf_count+=1\n",
    "            return\n",
    "\n",
    "    def pred(self, D):\n",
    "        return self.predict(D, self.A)\n",
    "    \n",
    "    def eval(self, D):\n",
    "        return self.evaluate(D, self.A)\n",
    "\n",
    "    def predict(self, D, A):\n",
    "        '''\n",
    "        Predict the classification for the data in D.\n",
    "        For the definition of A, see method 'fit'. \n",
    "        There is one critical difference between D and that defined in 'fit':\n",
    "            the last column may or may not be the labels. \n",
    "            This method works as long as the feature index in A matches the corresponding\n",
    "            column in D.\n",
    "        '''\n",
    "        \n",
    "        print(\"tree\")\n",
    "        print(self.tree) \n",
    "\n",
    "        row, _= D.shape # for the entire testing data \n",
    "        pred=np.empty((row, 1), dtype=str)\n",
    "        tmp_data={key: None for key in A.keys()}\n",
    "\n",
    "        #print(\"tmp_data\")\n",
    "        #print(tmp_data)\n",
    "        #print(\"------\")\n",
    "        #print(\"inside of A \")\n",
    "        #print(A)\n",
    "\n",
    "        # IndexError: only integers, slices (`:`), ellipsis (`...`), \n",
    "        # numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
    "\n",
    "        # print(\"keys that do not agree with tmp_data: \")\n",
    "        for i in range(len(D)):\n",
    "            for key, info in A.items():\n",
    "\n",
    "                tmp_data[key] = D[i, info[0]]\n",
    "\n",
    "            pred[i]=self.tree(tmp_data)\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, testing_D, A):\n",
    "        '''\n",
    "        Evaluate the performance of decision tree. (Accuracy)\n",
    "        For definition of testing_D and A, see 'predict'.\n",
    "        However, here the testing_D is required to be labelled, that is, its last column \n",
    "        should be labels of the data.\n",
    "        '''\n",
    "        true_label=testing_D[:, -1]\n",
    "        pred_label=self.predict(testing_D, A)\n",
    "        \n",
    "        success_count=0\n",
    "        for i in range(len(true_label)):\n",
    "            if true_label[i]==pred_label[i]:\n",
    "                success_count+=1\n",
    "\n",
    "        return success_count/len(true_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age          Workclass  Fnlwgt   Education  EducationNum  \\\n",
       "0   39          State-gov   77516   Bachelors            13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
       "2   38            Private  215646     HS-grad             9   \n",
       "3   53            Private  234721        11th             7   \n",
       "4   28            Private  338409   Bachelors            13   \n",
       "\n",
       "         MaritalStatus          Occupation    Relationship    Race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   CapitalGain  CapitalLoss  HoursPerWeek         Country  Income  \n",
       "0         2174            0            40   United-States       1  \n",
       "1            0            0            13   United-States       1  \n",
       "2            0            0            40   United-States       1  \n",
       "3            0            0            40   United-States       1  \n",
       "4            0            0            40            Cuba       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "row_names = [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"EducationNum\", \"MaritalStatus\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Gender\", \"CapitalGain\", \"CapitalLoss\",\n",
    "        \"HoursPerWeek\", \"Country\", \"Income\"]\n",
    "us_adult_income = pd.read_csv(dataset, names=row_names,na_values=[' ?'])\n",
    "us_adult_income[\"Income\"] = pd.Categorical(us_adult_income[\"Income\"])\n",
    "us_adult_income[\"Income\"] = us_adult_income[\"Income\"].cat.codes\n",
    "us_adult_income[\"Income\"] = 1 - us_adult_income[\"Income\"]\n",
    "\n",
    "us_adult_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult2 = us_adult_income.sample(frac=0.05, random_state=42)\n",
    "adult2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult25 = adult2.copy()\n",
    "adult25[\"Workclass\"] = pd.Categorical(adult25[\"Workclass\"])\n",
    "adult25[\"Workclass\"] = adult25[\"Workclass\"].cat.codes\n",
    "\n",
    "adult25[\"Education\"] = pd.Categorical(adult25[\"Education\"])\n",
    "adult25[\"Education\"] = adult25[\"Education\"].cat.codes\n",
    "\n",
    "adult25[\"MaritalStatus\"] = pd.Categorical(adult25[\"MaritalStatus\"])\n",
    "adult25[\"MaritalStatus\"] = adult25[\"MaritalStatus\"].cat.codes\n",
    "\n",
    "adult25[\"Occupation\"] = pd.Categorical(adult25[\"Occupation\"])\n",
    "adult25[\"Occupation\"] = adult25[\"Occupation\"].cat.codes\n",
    "\n",
    "adult25[\"Relationship\"] = pd.Categorical(adult25[\"Relationship\"])\n",
    "adult25[\"Relationship\"] = adult25[\"Relationship\"].cat.codes\n",
    "\n",
    "adult25[\"Race\"] = pd.Categorical(adult25[\"Race\"])\n",
    "adult25[\"Race\"] = adult25[\"Race\"].cat.codes\n",
    "\n",
    "adult25[\"Gender\"] = pd.Categorical(adult25[\"Gender\"])\n",
    "adult25[\"Gender\"] = adult25[\"Gender\"].cat.codes\n",
    "\n",
    "adult25[\"Country\"] = pd.Categorical(adult25[\"Country\"])\n",
    "adult25[\"Country\"] = adult25[\"Country\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17491</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18954</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15112</th>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1628 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Education  Occupation  Gender  Income\n",
       "14160   27         15           0       0       1\n",
       "27048   45         11           3       0       1\n",
       "28868   29          9           3       1       0\n",
       "5667    30          9           6       0       1\n",
       "7827    29         15           2       1       1\n",
       "...    ...        ...         ...     ...     ...\n",
       "17491   19         11          11       0       1\n",
       "18954   50          8           9       1       1\n",
       "15112   44         11           2       1       1\n",
       "5477    19         15           9       1       1\n",
       "1441    61         11          -1       1       1\n",
       "\n",
       "[1628 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23 = adult25[[\"Age\", \"Education\", \"Occupation\", \"Gender\", \"Income\"]]\n",
    "adult23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult23 = adult23[adult23[\"Occupation\"] != 1]\n",
    "adult23 = adult23[adult23[\"Education\"] != 13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1625, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D= np.genfromtxt('../datafiles/iris_train.txt', dtype=str)\n",
    "np.random.shuffle(D)\n",
    "\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D4 = D2.astype(str)\n",
    "D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = [str(y) for y in list(set([x[len(x)-1] for x in D2]))]\n",
    "attribute_indexes = [k for k,v in A.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = adult23.to_numpy()\n",
    "D2 = D2.astype(str)\n",
    "np.random.shuffle(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D2= np.genfromtxt('../datafiles/iris_train.txt', dtype=str)\n",
    "\n",
    "k=len(D2)\n",
    "\n",
    "# attribute_name=['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "A={name: [i] for i, name in enumerate(attribute_name)}\n",
    "# feature_discrete = {name: False for name in attribute_name}\n",
    "feature_discrete={name: False if isinstance(adult2[name].iloc[0], np.integer) else True for name in attribute_name}\n",
    "\n",
    "#conf={'A': A, \n",
    "#      'feature_discrete': feature_discrete, \n",
    "#      'treeType':'IGGR'}\n",
    "\n",
    "# tree = Tree_DPDT(\n",
    "\n",
    "# attribute_indexes, \n",
    "# attribute_values, \n",
    "# root, \n",
    "# class_values, \n",
    "# IGGR\n",
    "# dataset_size, \n",
    "# epsilon_per_tree\n",
    "# \n",
    "# ) \n",
    "\n",
    "root=None\n",
    "\n",
    "dt=Tree_DPDT(\n",
    "    attribute_indexes, \n",
    "    A, \n",
    "    root, # ?,\n",
    "    class_values,\n",
    "    feature_discrete, \n",
    "    'IGGR', \n",
    "    len(D2), \n",
    "    0.2)\n",
    "\n",
    "dt.train(D2[:len(D2)//2])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D2[len(D2)//2:]))\n",
    "dt.prune(D2[:len(D2)//2], D2[len(D2)//2:])\n",
    "print(dt.tree)\n",
    "print(dt.eval(D2[len(D2)//2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_node:\n",
    "    _parent_node = None\n",
    "    _split_value_from_parent = None\n",
    "    _splitting_attribute = None\n",
    "    _level = None\n",
    "    _id = None\n",
    "    _children = None\n",
    "    _class_counts = None\n",
    "    _noisy_class_counts = None\n",
    "    _signal_to_noise = 0.0\n",
    "\n",
    "    def __init__(self, parent_node, split_value_from_parent, splitting_attribute, tree_level, id, children):\n",
    "        self._parent_node = parent_node\n",
    "        self._split_value_from_parent = split_value_from_parent\n",
    "        self._splitting_attribute = splitting_attribute\n",
    "        self._level = tree_level\n",
    "        self._id = id\n",
    "        self._children = children\n",
    "        self._class_counts = defaultdict(int)\n",
    "        self._noisy_class_counts = None\n",
    "        self._signal_to_noise = 0.0\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self._children.append(child_node)\n",
    "\n",
    "    def remove_child(self, child_node):\n",
    "        value = child_node._split_value_from_parent\n",
    "        if value not in [x._split_value_from_parent for x in self._children]:\n",
    "            return 0\n",
    "        else:\n",
    "            new_children = [x for x in self._children if x._split_value_from_parent != value]\n",
    "            self._children = new_children\n",
    "            return 1\n",
    "\n",
    "\n",
    "    def increment_class_count(self, class_value):\n",
    "        self._class_counts[str(class_value)] += 1\n",
    "\n",
    "\n",
    "    def make_noisy_class_counts(self, epsilon, class_values):\n",
    "        ''' Add noise to the class counts of this leaf. Not performed on parents. '''\n",
    "        if not self._noisy_class_counts and not self._children: # to make sure this code is only run once per leaf\n",
    "            counts = {}\n",
    "            for val in class_values:\n",
    "                if val in self._class_counts:\n",
    "                    # Laplace noise added to THE CLASS COUNTS\n",
    "                    counts[val] = max( 0, int(self._class_counts[val] + np.random.laplace(scale=float(1./epsilon))) )\n",
    "                else: # original count was 0\n",
    "                    counts[val] = max( 0, int(np.random.laplace(scale=float(1./epsilon))) )\n",
    "            self._noisy_class_counts = counts\n",
    "\n",
    "            # what I'm assuming is equation (10)\n",
    "            self._signal_to_noise = (epsilon * 1.0 * sum([v for k,v in counts.items()])) / (math.sqrt(2*1.0) * len(counts)) # enS / (sqrt(2n) * num_classes)\n",
    "            #print(\"self._signal_to_noise = {}\".format(self._signal_to_noise))\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 # we're summing the redundant calls\n",
    "\n",
    "\n",
    "    def add_childrens_class_counts(self, epsilon):\n",
    "        ''' Add up the class counts of all leaf nodes that trace back to this parent node. '''\n",
    "        if not self._noisy_class_counts and self._children:\n",
    "            counts, num_leafs = self._find_leafs_and_count(self, defaultdict(int), 0)\n",
    "            self._noisy_class_counts = counts\n",
    "            self._signal_to_noise =  (epsilon * num_leafs * sum([v for k,v in counts.items()])) / (math.sqrt(2*num_leafs) * len(counts)) # enS / (sqrt(2n) * num_classes)\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 # we're summing the redundant calls\n",
    "                    \n",
    "    def _find_leafs_and_count(self, node, counts, num_leafs):\n",
    "        if node._children:\n",
    "            for child in node._children:\n",
    "                counts, num_leafs = self._find_leafs_and_count(child, counts, num_leafs)\n",
    "        else:\n",
    "            num_leafs += 1\n",
    "            for k,v in node._noisy_class_counts.items():\n",
    "                counts[k] += v\n",
    "        return counts, num_leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently includes methods as:\n",
    "        # _init_  : _trees\n",
    "        # get_domains\n",
    "        # reduce trees\n",
    "        # evaluate_accuracy_with_voting\n",
    "\n",
    "class DPRF_Forest:  \n",
    "    def __init__(self, \n",
    "                 training_nonprocessed,\n",
    "                 training_data, # 2D list of the training data where the columns are the attributes, and the first column is the class attribute\n",
    "                 # test_data, # T\n",
    "                 epsilon, # epsilon, Budget, B, the total privacy budget\n",
    "                 f, # n of attributes to be split used by each dctree f\n",
    "                 max_depth # max tree depth \n",
    "                 ):\n",
    "\n",
    "        self._trees = []\n",
    "\n",
    "\n",
    "        ''' Some initialization '''\n",
    "        # Attribute set F\n",
    "        attribute_values = self.get_domains(training_data)\n",
    "\n",
    "        # Class values set\n",
    "        class_values = [str(y) for y in list(set([x[len(x)-1] for x in training_data]))]\n",
    "        attribute_indexes = [int(k) for k,v in attribute_values.items()]\n",
    "\n",
    "\n",
    "        #print(attribute_values)\n",
    "        #print(\"------------\")\n",
    "        #print(class_values)\n",
    "        #print(\"------------\")\n",
    "        #print(attribute_indexes)\n",
    "        #print(\"-----------\")\n",
    "\n",
    "        \n",
    "        # |D|\n",
    "        dataset_size = len(training_data)\n",
    "\n",
    "\n",
    "        valid_attribute_sizes = [[int(k),len(v)] for k,v in attribute_values.items()]\n",
    "\n",
    "        print(\"valid attribute sizes\")\n",
    "        print(valid_attribute_sizes)\n",
    "\n",
    "        # Number of trees t\n",
    "        num_trees = len(attribute_indexes)\n",
    "\n",
    "        print(\"Number of trees: \", num_trees)\n",
    "\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] )\n",
    "\n",
    "        # e = B / t\n",
    "        epsilon_per_tree = epsilon / float(num_trees)\n",
    "\n",
    "        ''' Calculating minimum support threshold '''\n",
    "        # estimated_support_min_depth = dataset_size / (average_domain**2) # a large number\n",
    "        # estimated_support_max_depth = dataset_size / (average_domain ** (len(attribute_indexes)/2)) # max tree depth is k/2 # a small number\n",
    "        # epsilon_per_tree = epsilon / float(num_trees)\n",
    "        # min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER # the minimum support in order for S>N\n",
    "\n",
    "\n",
    "        # while estimated_support_max_depth > min_support: # then we can have more trees\n",
    "        #    num_trees += 1\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        # while min_support > estimated_support_min_depth and num_trees>1: # then we need to have less trees\n",
    "        #    num_trees, valid_attribute_sizes, estimated_support_min_depth = self.reduce_trees(num_trees, valid_attribute_sizes, dataset_size)\n",
    "        #    epsilon_per_tree = epsilon / float(num_trees)\n",
    "        #    min_support = len(class_values) * math.sqrt(2) * (1/epsilon_per_tree) * SNR_MULTIPLIER\n",
    "\n",
    "\n",
    "        print(\"NUM TREES = {} & EPSILON PER TREE = {}\".format(num_trees, epsilon_per_tree))\n",
    "\n",
    "        # ? \n",
    "        root_attributes = []\n",
    "\n",
    "       \n",
    "        for a in attribute_indexes:\n",
    "            if a in [x[0] for x in valid_attribute_sizes]:\n",
    "                root_attributes.append(a) # OR: [index, support, gini]\n",
    "\n",
    "        print(\"root attributes\")\n",
    "        print(root_attributes)\n",
    "\n",
    "        # for treeId = 1,2 ... t do:\n",
    "        for t in range(num_trees):\n",
    "\n",
    "            if not root_attributes:\n",
    "                root_attributes = attribute_indexes[:]\n",
    "\n",
    "            # randomly extract |D| samples from D w/ a bagging algo?\n",
    "            root = random.choice(root_attributes)\n",
    "            root_attributes.remove(root)\n",
    "\n",
    "\n",
    "\n",
    "            # TREE BUILDTREE(D, A, C, eps, f, Dm)\n",
    "            # D :\n",
    "\n",
    "            # attribute_indexes, - A_ind\n",
    "            # attribute_values, A - not exactly the same after all! \n",
    "            # root, \n",
    "            # class_values, \n",
    "            # feature_discrete\n",
    "            # treetype \n",
    "            # dataset_size, \n",
    "            # epsilon_per_tree\n",
    "\n",
    "            A={name: [i] for i, name in enumerate(adult23.columns[:-1])}\n",
    "\n",
    "            # {'Age': False, 'Education': True, 'Occupation': True, 'Gender': True, 'CapitalLoss': False}\n",
    "            feature_discrete={name: False if isinstance(training_nonprocessed[name].iloc[0], np.integer) else True for name in adult23.columns[:-1]}\n",
    "\n",
    "            print(\"feature discrete\")\n",
    "            print(feature_discrete)\n",
    "\n",
    "            print(\"---------------\")\n",
    "            print(\"A: \")\n",
    "            print(A)\n",
    "            print(\"---------------\")\n",
    "            print(\"attribute values: \")\n",
    "            print(attribute_values)\n",
    "            print(\"---------------\")\n",
    "            print(\"---------------\")\n",
    "\n",
    "            tree = Tree_DPDT(attribute_indexes, \n",
    "                            A, # original style A\n",
    "                            attribute_values, # new style attribute vals\n",
    "                            root, \n",
    "                            class_values, \n",
    "                            feature_discrete, \n",
    "                            'IGGR',  # treetype\n",
    "                            dataset_size, \n",
    "                            epsilon_per_tree) \n",
    "\n",
    "            tree.train(training_data[:len(training_data)//2])\n",
    "           \n",
    "            # TO EVOLVE HERE \n",
    "\n",
    "            # attribute_name = [\"Age\", \"Education\", \"Occupation\", \"Gender\", \"CapitalLoss\"]\n",
    "\n",
    "            #num_unclassified = tree.filter_training_data_and_count(training_data, epsilon_per_tree, class_values)\n",
    "            \n",
    "            #if num_unclassified > 0:\n",
    "            #    print(\"number of unclassified records = {}\".format(num_unclassified))\n",
    "            \n",
    "\n",
    "            print(\"currently pruning! \")\n",
    "            # D2[:len(D2)//2], D2[len(D2)//2:]\n",
    "            tree.prune(training_data[:len(training_data)//2], training_data[len(training_data)//2:])\n",
    "\n",
    "            # FOREST = FOREST U TREE\n",
    "            self._trees.append(tree)\n",
    "\n",
    "            print(\"succesfully ended building tree dictionary\")\n",
    "\n",
    "\n",
    "    def get_domains(self, data):\n",
    "        attr_domains = {}\n",
    "        transData = np.transpose(data)\n",
    "        for i in range(0,len(data[0]) - 1):\n",
    "            attr_domains[str(i)] = [str(x) for x in set(transData[i])]\n",
    "            print(\"original domain length of categorical attribute {}: {}\".format(i, len(attr_domains[str(i)])))\n",
    "        return attr_domains\n",
    "\n",
    "\n",
    "    def reduce_trees(self, num_trees, valid_attribute_sizes, dataset_size):\n",
    "        num_trees -= 1\n",
    "        largest_attribute = sorted(valid_attribute_sizes, key=lambda x:x[1], reverse=True)[0]\n",
    "        #print(\"Removing att{} with domain size {}\".format(largest_attribute[0], largest_attribute[1])) \n",
    "        new_valids = [ x for x in valid_attribute_sizes if x[0] != largest_attribute[0] ]\n",
    "        average_domain = np.mean( [x[1] for x in valid_attribute_sizes] ) # average with all attributes\n",
    "        narrowed_average_domain = np.mean([x[1] for x in valid_attribute_sizes if x[0] in [y[0] for y in new_valids] ]) # average without the big attributes\n",
    "        estimated_support_squared = dataset_size / (average_domain * narrowed_average_domain)\n",
    "        return num_trees, new_valids, estimated_support_squared\n",
    "\n",
    "\n",
    "    # GET MAJORITY LABELS (Forest, T, C) equivalent from pseudocode\n",
    "    def evaluate_accuracy_with_voting(self, records, class_index=4):\n",
    "\n",
    "        ''' Calculate the Prediction Accuracy of the Forest. '''\n",
    "        actual_labels = [x[class_index] for x in records]\n",
    "\n",
    "        predicted_labels = []\n",
    "        leafs_not_used = 0\n",
    "        count_of_averages_used = 0\n",
    "\n",
    "        h = []\n",
    "        class_counts = defaultdict(list)\n",
    "\n",
    "        # FOR EACH RECORD X IN DATASET DO\n",
    "        # Strange for we need to access individual predictions here...\n",
    "        for rec in records:\n",
    "\n",
    "            class_value_fractions = defaultdict(list)\n",
    "\n",
    "            # FOR EACH TREE DO\n",
    "            for tree in self._trees:\n",
    "\n",
    "                # GET PREDICTED CLASSIFICATION RESULT for a single record\n",
    "                # node, leaf_not_used = tree._classify(tree._root_node, rec)\n",
    "\n",
    "                print(\"tree\")\n",
    "                print(tree)\n",
    "\n",
    "                node = tree.pred(tree, np.array([rec]))\n",
    "\n",
    "\n",
    "                print(\"precited \", node , \" for record \", rec)\n",
    "\n",
    "                h.append(*node)\n",
    "\n",
    "                print(\"result list\")\n",
    "                print(h)\n",
    "\n",
    "                # noisy_class_counts = node._noisy_class_counts\n",
    "                # defaultdict(<class 'int'>, {'0': 2, '1': 33})\n",
    "                unique, counts = np.unique(h, return_counts=True)\n",
    "                class_counts = dict(zip(unique, h))\n",
    "\n",
    "                print(\"class counts\")\n",
    "                print(class_counts)\n",
    "\n",
    "                leafs_not_used += leaf_not_used\n",
    "\n",
    "                support = float(sum([v for k,v in noisy_class_counts.items()]))\n",
    "\n",
    "                for k,v in noisy_class_counts.items():\n",
    "                    class_value_fractions[k].append(v/support)\n",
    "                    \n",
    "            best_confidences = {}\n",
    "\n",
    "            for k,lis in class_value_fractions.items():\n",
    "               best_confidences[k] = max(lis)\n",
    "            best = [None, 0.0]\n",
    "\n",
    "            for k,class_best in best_confidences.items():\n",
    "                if class_best > best[1]:\n",
    "                    best = [k, class_best]\n",
    "            average_used = False\n",
    "\n",
    "\n",
    "            for k, class_best in best_confidences.items():\n",
    "\n",
    "                if class_best == best[1] and k != best[0]:\n",
    "\n",
    "                    average_used = True\n",
    "                    #print(\"original best: {} vs. contender: {}\".format(best, (k,class_best)))\n",
    "\n",
    "                    orig_average = np.mean(class_value_fractions[best[0]])\n",
    "\n",
    "                    contender_average = np.mean(class_value_fractions[k])\n",
    "                    #print(\"original average: {} vs. contender: {}\".format(orig_average, contender_average))\n",
    "\n",
    "                    if contender_average > orig_average:\n",
    "                        best = [k, class_best]\n",
    "\n",
    "            count_of_averages_used += 1 if average_used else 0\n",
    "            predicted_labels.append(int(best[0]))\n",
    "\n",
    "        counts = Counter([x == y for x, y in zip(predicted_labels, actual_labels)])\n",
    "        \n",
    "        # acc = float(counts[True]) / len(records)\n",
    "        \n",
    "        return float(counts[True]) / len(records)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1625"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original domain length of categorical attribute 0: 63\n",
      "original domain length of categorical attribute 1: 15\n",
      "original domain length of categorical attribute 2: 14\n",
      "original domain length of categorical attribute 3: 2\n",
      "valid attribute sizes\n",
      "[[0, 63], [1, 15], [2, 14], [3, 2]]\n",
      "Number of trees:  4\n",
      "NUM TREES = 4 & EPSILON PER TREE = 0.05\n",
      "root attributes\n",
      "[0, 1, 2, 3]\n",
      "feature discrete\n",
      "{'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}\n",
      "---------------\n",
      "A: \n",
      "{'Age': [0], 'Education': [1], 'Occupation': [2], 'Gender': [3]}\n",
      "---------------\n",
      "attribute values: \n",
      "{'0': ['41', '31', '29', '47', '51', '30', '70', '79', '66', '19', '48', '73', '64', '52', '60', '83', '90', '67', '39', '42', '33', '43', '46', '20', '58', '22', '49', '81', '59', '38', '23', '62', '55', '40', '36', '71', '53', '57', '72', '45', '65', '28', '37', '44', '35', '21', '69', '50', '56', '34', '24', '63', '32', '54', '27', '68', '26', '61', '74', '80', '17', '25', '18'], '1': ['5', '8', '11', '0', '6', '2', '10', '1', '7', '14', '12', '3', '15', '9', '4'], '2': ['-1', '5', '8', '11', '13', '0', '6', '2', '10', '7', '12', '3', '4', '9'], '3': ['1', '0']}\n",
      "---------------\n",
      "---------------\n",
      "currently pruning! \n",
      "We got to the pruning stage!\n",
      "succesfully ended building tree dictionary\n",
      "feature discrete\n",
      "{'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}\n",
      "---------------\n",
      "A: \n",
      "{'Age': [0], 'Education': [1], 'Occupation': [2], 'Gender': [3]}\n",
      "---------------\n",
      "attribute values: \n",
      "{'0': ['41', '31', '29', '47', '51', '30', '70', '79', '66', '19', '48', '73', '64', '52', '60', '83', '90', '67', '39', '42', '33', '43', '46', '20', '58', '22', '49', '81', '59', '38', '23', '62', '55', '40', '36', '71', '53', '57', '72', '45', '65', '28', '37', '44', '35', '21', '69', '50', '56', '34', '24', '63', '32', '54', '27', '68', '26', '61', '74', '80', '17', '25', '18'], '1': ['5', '8', '11', '0', '6', '2', '10', '1', '7', '14', '12', '3', '15', '9', '4'], '2': ['-1', '5', '8', '11', '13', '0', '6', '2', '10', '7', '12', '3', '4', '9'], '3': ['1', '0']}\n",
      "---------------\n",
      "---------------\n",
      "currently pruning! \n",
      "We got to the pruning stage!\n",
      "succesfully ended building tree dictionary\n",
      "feature discrete\n",
      "{'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}\n",
      "---------------\n",
      "A: \n",
      "{'Age': [0], 'Education': [1], 'Occupation': [2], 'Gender': [3]}\n",
      "---------------\n",
      "attribute values: \n",
      "{'0': ['41', '31', '29', '47', '51', '30', '70', '79', '66', '19', '48', '73', '64', '52', '60', '83', '90', '67', '39', '42', '33', '43', '46', '20', '58', '22', '49', '81', '59', '38', '23', '62', '55', '40', '36', '71', '53', '57', '72', '45', '65', '28', '37', '44', '35', '21', '69', '50', '56', '34', '24', '63', '32', '54', '27', '68', '26', '61', '74', '80', '17', '25', '18'], '1': ['5', '8', '11', '0', '6', '2', '10', '1', '7', '14', '12', '3', '15', '9', '4'], '2': ['-1', '5', '8', '11', '13', '0', '6', '2', '10', '7', '12', '3', '4', '9'], '3': ['1', '0']}\n",
      "---------------\n",
      "---------------\n",
      "currently pruning! \n",
      "We got to the pruning stage!\n",
      "succesfully ended building tree dictionary\n",
      "feature discrete\n",
      "{'Age': False, 'Education': True, 'Occupation': True, 'Gender': True}\n",
      "---------------\n",
      "A: \n",
      "{'Age': [0], 'Education': [1], 'Occupation': [2], 'Gender': [3]}\n",
      "---------------\n",
      "attribute values: \n",
      "{'0': ['41', '31', '29', '47', '51', '30', '70', '79', '66', '19', '48', '73', '64', '52', '60', '83', '90', '67', '39', '42', '33', '43', '46', '20', '58', '22', '49', '81', '59', '38', '23', '62', '55', '40', '36', '71', '53', '57', '72', '45', '65', '28', '37', '44', '35', '21', '69', '50', '56', '34', '24', '63', '32', '54', '27', '68', '26', '61', '74', '80', '17', '25', '18'], '1': ['5', '8', '11', '0', '6', '2', '10', '1', '7', '14', '12', '3', '15', '9', '4'], '2': ['-1', '5', '8', '11', '13', '0', '6', '2', '10', '7', '12', '3', '4', '9'], '3': ['1', '0']}\n",
      "---------------\n",
      "---------------\n",
      "currently pruning! \n",
      "We got to the pruning stage!\n",
      "succesfully ended building tree dictionary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input:\n",
    "    # Training set D \n",
    "    # Privacy budget B (epsilon)\n",
    "\n",
    "    # Test Set T (?)  (Maybe?)\n",
    "\n",
    "    # Number of attributes to be split used by each dec tree f \n",
    "    # Max tree depth d_m\n",
    "\n",
    "B = 0.2 # epsilon\n",
    "f = 3\n",
    "dm = 5\n",
    "\n",
    "# predict print\n",
    "\n",
    "diff_priv_forest = DPRF_Forest(adult2, D2[400:], B, f, dm)\n",
    "#num_trees = len(our_diff_priv_forest._trees)\n",
    "#av_prunings = np.mean([x._prunings for x in our_diff_priv_forest._trees])\n",
    "#av_tree_size = np.mean([x._id-x._prunings+1 for x in our_diff_priv_forest._trees])\n",
    "#accuracy, leafs_not_most_confident, votes_requiring_average = diff_priv_forest.evaluate_accuracy_with_voting(adult23, class_index=0) # confidence count includes all trees (before voting)\n",
    "#print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '295']\n",
      " ['1' '930']]\n"
     ]
    }
   ],
   "source": [
    "g = D2[400:][:,-1]\n",
    "unique, counts = np.unique(g, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['29', '11', '7', '0', '1'],\n",
       "       ['19', '2', '5', '1', '1'],\n",
       "       ['59', '15', '0', '0', '1'],\n",
       "       ...,\n",
       "       ['29', '4', '7', '1', '1'],\n",
       "       ['45', '14', '9', '1', '0'],\n",
       "       ['21', '15', '-1', '1', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2[400:][:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "<__main__.Tree_DPDT object at 0x7f8a9c9ea2e0>\n",
      "precited  [['1']]  for record  ['29' '11' '7' '0' '1']\n",
      "result list\n",
      "[array(['1'], dtype='<U1')]\n",
      "class counts\n",
      "{'1': array(['1'], dtype='<U1')}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'leaf_not_used' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m diff_priv_forest\u001b[39m.\u001b[39;49mevaluate_accuracy_with_voting(D2[\u001b[39m400\u001b[39;49m:][:,\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m], class_index\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m) \u001b[39m# confidence count includes all trees (before voting)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maccuracy = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(accuracy))\n",
      "Cell \u001b[0;32mIn[164], line 227\u001b[0m, in \u001b[0;36mDPRF_Forest.evaluate_accuracy_with_voting\u001b[0;34m(self, records, class_index)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclass counts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    225\u001b[0m \u001b[39mprint\u001b[39m(class_counts)\n\u001b[0;32m--> 227\u001b[0m leafs_not_used \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m leaf_not_used\n\u001b[1;32m    229\u001b[0m support \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m([v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m noisy_class_counts\u001b[39m.\u001b[39mitems()]))\n\u001b[1;32m    231\u001b[0m \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m noisy_class_counts\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'leaf_not_used' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = diff_priv_forest.evaluate_accuracy_with_voting(D2[400:][:,0:5], class_index=4) # confidence count includes all trees (before voting)\n",
    "print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([['1']], dtype='<U1')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the classification result set is the left column - \n",
    "D2[100:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e9830449175e83b6f3fc7dabdb3df0657dd9f8c8ad4abdd216dc6acf66ebbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
